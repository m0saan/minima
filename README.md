Welcome to minima
================

<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

# minima: A Mini Deep Learning Framework

> minima is a lightweight deep learning framewor, lean yet effective
> tailor-made for educational exploration.  
> Just like a delicate sapling inspired by the towering strength of an
> oak, Minima draws its inspiration from PyTorch.  
> Yet, it carves its own identity with a straightforward interface and a
> curated set of features.  
> This makes learning and using it a breeze, allowing you to
> effortlessly build and train neural networks.  
> Indeed, Minima is your friendly companion on the journey to
> understanding deep learning, where less is often more.

## Installing

You can install minima on your own machines with conda

If you’re using
[miniconda](https://docs.conda.io/en/latest/miniconda.html)
(recommended) then run:

``` bash
conda install minima
```

…or if you’re using
[Anaconda](https://www.anaconda.com/products/individual) then run:

``` bash
conda install minima anaconda
```

To install with pip, use: `pip install minima`.

If you plan to develop Minima yourself, or want to be on the cutting
edge, you can use an editable install.

``` bash
git clone https://github.com/m0saan/minima
pip install .
```

## Features

- Easy to install and use
- Simple and intuitive API for defining and training neural networks
- Built-in support for common layers and activation functions
- Supports both CPU and GPU acceleration
- Compatible with NumPy arrays for easy data manipulation

## Usage

Here’s a simple example of how to define and train a neural network
using Minima:

``` python
import minima as mi

# Define the neural network architecture
model = mi.Sequential(
    mi.Linear(784, 128),
    mi.ReLU(),
    mi.Linear(128, 10),
    mi.Softmax()
)

# Load the dataset
x_train, y_train, x_test, y_test = load_data()

# Train the model
loss_fn = mi.CrossEntropyLoss()
optimizer = mi.SGD(model.parameters(), lr=0.01)
for epoch in range(10):
    for x_batch, y_batch in minibatch(x_train, y_train, batch_size=32):
        y_pred = model(x_batch)
        loss = loss_fn(y_pred, y_batch)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

# Evaluate the model
y_pred = model(x_test)
accuracy = compute_accuracy(y_pred, y_test)
print(f"Accuracy: {accuracy:.2f}")
```

This example defines a simple neural network with two linear layers and
two activation functions, trains it on a dataset using stochastic
gradient descent, and evaluates its accuracy on a test set.

## Documentation

For more information on how to use minima, please refer to the
documentation, which can be found in the website above.

## Contributing

comming soon!

## License

minima is released under the Apache License 2.0. See `LICENSE` for more
information.
