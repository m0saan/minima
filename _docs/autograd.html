<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="Fill in a module description here">

<title>minima - autograd</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="styles.css">
<meta property="og:title" content="minima - autograd">
<meta property="og:description" content="Fill in a module description here">
<meta property="og:site-name" content="minima">
<meta name="twitter:title" content="minima - autograd">
<meta name="twitter:description" content="Fill in a module description here">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">minima</span>
    </a>
  </div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">autograd</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Welcome to minima</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./autograd.html" class="sidebar-item-text sidebar-link active">autograd</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./operators.html" class="sidebar-item-text sidebar-link">operators</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#derivatives" id="toc-derivatives" class="nav-link active" data-scroll-target="#derivatives">Derivatives</a></li>
  <li><a href="#derivatives-in-the-context-of-neural-nets-autograd" id="toc-derivatives-in-the-context-of-neural-nets-autograd" class="nav-link" data-scroll-target="#derivatives-in-the-context-of-neural-nets-autograd">Derivatives in the context of neural nets (Autograd)</a>
  <ul class="collapse">
  <li><a href="#value" id="toc-value" class="nav-link" data-scroll-target="#value">Value</a></li>
  </ul></li>
  <li><a href="#manual-gradient" id="toc-manual-gradient" class="nav-link" data-scroll-target="#manual-gradient">Manual gradient</a>
  <ul class="collapse">
  <li><a href="#base-case-l-grad" id="toc-base-case-l-grad" class="nav-link" data-scroll-target="#base-case-l-grad">base case (<code>L grad</code>)</a></li>
  <li><a href="#value-1" id="toc-value-1" class="nav-link" data-scroll-target="#value-1">Value</a></li>
  <li><a href="#value-2" id="toc-value-2" class="nav-link" data-scroll-target="#value-2">Value</a></li>
  <li><a href="#value-3" id="toc-value-3" class="nav-link" data-scroll-target="#value-3">Value</a></li>
  <li><a href="#all_devices" id="toc-all_devices" class="nav-link" data-scroll-target="#all_devices">all_devices</a></li>
  <li><a href="#cpu" id="toc-cpu" class="nav-link" data-scroll-target="#cpu">cpu</a></li>
  <li><a href="#cpudevice" id="toc-cpudevice" class="nav-link" data-scroll-target="#cpudevice">CPUDevice</a></li>
  <li><a href="#device" id="toc-device" class="nav-link" data-scroll-target="#device">Device</a></li>
  <li><a href="#operator" id="toc-operator" class="nav-link" data-scroll-target="#operator">Operator</a></li>
  <li><a href="#tensorop" id="toc-tensorop" class="nav-link" data-scroll-target="#tensorop">TensorOp</a></li>
  <li><a href="#tensor" id="toc-tensor" class="nav-link" data-scroll-target="#tensor">Tensor</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/m0saan/minima/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block">autograd</h1>
</div>

<div>
  <div class="description">
    Fill in a module description here
  </div>
</div>


<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->
<p>Today, I’d like to introduce you to the automatic differentiation component of my deep learning framework, Minima. I am going to explain a specific module, called Micrograd, in detail. I had released the Micrograd module on Github a couple of years ago, but never provided an in-depth explanation of its functioning. So here we go.</p>
<p>Micrograd, at its core, is an autograd (short for automatic gradient) engine. It implements the backpropagation algorithm, a crucial aspect of deep learning. Backpropagation enables efficient computation of the gradient (rate of change) of a loss function (something we aim to minimize) with respect to the weights of a neural network. This enables us to fine-tune the weights of the neural network iteratively, reducing the loss function, and in turn, increasing the accuracy of the network. Backpropagation is central to most modern deep neural network libraries, such as PyTorch or Jax.</p>
<p>To help illustrate what Micrograd does, consider a simple example. We start with two inputs, a and b, encapsulated in special objects we call “Value” objects. We then construct a mathematical expression involving a and b. The Value objects help Micrograd to keep track of how these inputs are being transformed, and the operations performed on them, resulting in a complete mathematical “expression graph”.</p>
<p>Micrograd then performs two key tasks. First, it carries out a ‘forward pass’ - it evaluates the final value of the expression we’ve created. But more importantly, it then performs a ‘backward pass’ - essentially a run of the backpropagation algorithm. It starts from the final value and traces back through the expression graph, calculating the derivative (rate of change) of the final value with respect to each of the original inputs and intermediate nodes, using the chain rule of calculus.</p>
<p>This is crucial as these derivatives tell us how much our final value is affected by small changes in the inputs. In essence, the derivative of the final value with respect to an input is a measure of the “sensitivity” of the final value to that input.</p>
<p>Now, the example above might seem abstract - the expression we constructed didn’t have any particular meaning, it was just a demonstration of the capabilities of Micrograd. But the reason it’s useful is that this kind of mathematical expression is exactly what neural networks are - they take input data and network weights as inputs, and transform them through a series of mathematical operations into a final output, usually a prediction or a loss value.</p>
<p>One thing to note here is that Micrograd operates at the level of individual scalar values, not n-dimensional tensors as you’d typically find in full-scale deep learning libraries. This is for simplicity and instructional clarity. In real-world, high-performance neural network libraries, tensors are used to bundle up large arrays of scalar values, enabling efficient, parallel computation. But fundamentally, the math stays the same.</p>
<p>So, what’s the magic behind Micrograd? Surprisingly, the core autograd engine, the part that handles backpropagation and makes neural network training possible, is a mere 100 lines of simple Python code. On top of that, the neural network library, constructed based on this autograd engine, is only an additional 50 lines of code. It’s pretty impressive how much power you can get from just a handful of well-written Python code lines.</p>
<p>All of this is to say, understanding automatic differentiation and neural network training doesn’t require an enormous, complicated codebase. It’s essentially about understanding a relatively small number of key concepts and how they work together. Of course, making these things run fast and efficiently in practice does require additional complexity, but at a fundamental level, what’s happening isn’t that complicated. And Micrograd serves as an excellent tool for understanding these fundamentals. Now,</p>
<section id="derivatives" class="level2">
<h2 class="anchored" data-anchor-id="derivatives">Derivatives</h2>
<p>In calculus, the derivative of a function at a certain point is a measure of how the function changes at that point. It is defined as the limit of the ratio of the change in the function value (<code>f(x)</code>) to the change in the <code>x</code> value (<code>Δx</code>) as <code>Δx</code> approaches zero. This can be written as:</p>
<p><span class="math display">\[f'(x) = \lim_{{Δx \to 0}} \frac{{f(x + Δx) - f(x)}}{{Δx}}\]</span></p>
<p>This equation represents the slope of the tangent line to the function at a specific point <code>x</code>, which can also be interpreted as the instantaneous rate of change of the function at that point.</p>
<p>If you have a function <code>y = f(x) = x^n</code>, where <code>n</code> is a constant, the power rule of differentiation tells us that the derivative of <code>f(x)</code> with respect to <code>x</code> is:</p>
<p><span class="math display">\[f'(x) = n * x^{n-1}\]</span></p>
<p>In the context of the function <code>d = a*b + c</code> which we’re going to use below, since <code>a</code> is the variable and <code>b</code> and <code>c</code> are constants, the derivative of <code>d</code> with respect to <code>a</code> is just <code>b</code>. This can be written in LaTeX as:</p>
<p><span class="math display">\[ \frac{{dd}}{{da}} = b \]</span></p>
<p>we begin by assigning values to three variables <code>a</code>, <code>b</code>, and <code>c</code>. We then create a fourth variable, <code>d</code>, which is equal to the product of <code>a</code> and <code>b</code>, added to <code>c</code>. When you execute this cell, it should display the value of <code>d</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> <span class="op">-</span><span class="dv">2</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> <span class="dv">11</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> a<span class="op">*</span>b <span class="op">+</span> c</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>d</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>3</code></pre>
</div>
</div>
<p>we define a function <code>f_a(a,b,c)</code>, which helps us estimate the slope of the function at the point <code>a</code>. The function first calculates <code>d1</code> using the given inputs, <code>a</code>, <code>b</code>, and <code>c</code>. Then it increments <code>a</code> by a small value <code>h</code> and recalculates the value <code>d2</code>. The function then prints the original <code>d1</code>, the new <code>d2</code>, and the estimated slope which is <code>(d2 - d1) / h</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f_a(a,b,c):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    h <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    d1 <span class="op">=</span> a<span class="op">*</span>b <span class="op">+</span> c</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    a <span class="op">+=</span> h</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    d2 <span class="op">=</span> a<span class="op">*</span>b <span class="op">+</span> c</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'd1: </span><span class="sc">{</span>d1<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'd2: </span><span class="sc">{</span>d2<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'slope: </span><span class="sc">{</span>(d2 <span class="op">-</span> d1) <span class="op">/</span> h<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>f_a(a,b,c)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>d1: 3
d2: 2.9800000000000004
slope: -1.9999999999999574</code></pre>
</div>
</div>
<p>that states that the derivative of <code>d</code> with respect to <code>a</code>, denoted as <code>(db/da)</code>, is analytically equal to <code>b</code>. This is because in the expression <code>d = a*b + c</code>, the coefficient of <code>a</code> is <code>b</code>, so by the power rule of differentiation, the derivative is <code>b</code>. In this case, <code>b</code> equals <code>-2</code>.</p>
<p>Now if we do this with <code>b</code></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f_b(a,b,c):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    h <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    d1 <span class="op">=</span> a<span class="op">*</span>b <span class="op">+</span> c</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    b <span class="op">+=</span> h</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    d2 <span class="op">=</span> a<span class="op">*</span>b <span class="op">+</span> c</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'd1: </span><span class="sc">{</span>d1<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'd2: </span><span class="sc">{</span>d2<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'slope: </span><span class="sc">{</span>(d2 <span class="op">-</span> d1) <span class="op">/</span> h<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>f_b(a,b,c)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>d1: 3
d2: 3.04
slope: 4.0000000000000036</code></pre>
</div>
</div>
<p>Here’s what happens in the function: 1. It begins by defining a small change <code>h</code> which is set to <code>0.01</code>. 2. Then, the function calculates <code>d1</code>, which is the result of <code>a*b + c</code> with the original inputs <code>a</code>, <code>b</code>, and <code>c</code>. 3. It increments <code>b</code> by the small value <code>h</code>. 4. Next, the function calculates a new <code>d2</code>, which is the result of <code>a*b + c</code> after the increment to <code>b</code>. 5. Finally, the function prints out the original <code>d1</code>, the new <code>d2</code>, and the estimated slope calculated as <code>(d2 - d1) / h</code>.</p>
<p>When you call <code>f_b(a,b,c)</code>, the function performs all these operations using the values of <code>a</code>, <code>b</code>, and <code>c</code> from the previous context.</p>
<p>The output will give you an approximate value of the derivative of <code>d</code> with respect to <code>b</code> (noted as <code>dd/db</code> in mathematical notation), assuming that the function <code>d(a, b, c) = a*b + c</code> is relatively smooth and continuous near the point <code>b</code>.</p>
</section>
<section id="derivatives-in-the-context-of-neural-nets-autograd" class="level2">
<h2 class="anchored" data-anchor-id="derivatives-in-the-context-of-neural-nets-autograd">Derivatives in the context of neural nets (Autograd)</h2>
<p>Automatic differentiation, or auto grad as it’s often referred to in the context of deep learning, is a powerful tool that greatly simplifies the process of working with derivatives. It does this by automatically computing the derivatives (or gradients) of functions, thus relieving the need to manually calculate these derivatives as we have done above.</p>
<p>The use of auto grad is fundamental to the training process of deep learning models. Deep learning models, such as neural networks, are essentially complex mathematical functions with many parameters (weights and biases). Training these models involves adjusting these parameters to minimize a loss function, which quantifies how well the model is performing on a given task. The most common method for doing this is gradient descent, which uses the gradients of the loss function with respect to the parameters to update the parameters in a way that decreases the loss.</p>
<p>However, the manual calculation of these gradients, especially for complex models, is not only tedious but also prone to errors. Here’s where auto grad comes in. By using automatic differentiation, we can compute these gradients automatically and accurately, no matter how complex the model is.</p>
<p>In a deep learning framework, when we define our model and loss function, the framework uses auto grad to build a computational graph under the hood. This graph captures all the computations that are done in the forward pass (i.e., when we pass our inputs through the model to get the output). Then, when we need to compute the gradients during the backward pass, the framework uses this computational graph and the chain rule from calculus to compute the gradients automatically. This process is often referred to as backpropagation.</p>
<p>The main advantage of using auto grad in deep learning is that it allows us to focus on designing our models and defining our loss functions without worrying about the details of computing the gradients. This simplifies our code, reduces the chance of errors, and allows for greater flexibility in designing complex models. In fact, with auto grad, we can easily experiment with new types of models and loss functions, as we can rely on the framework to correctly compute the gradients no matter how complex our design is.</p>
<p>Let’s start building a mini autograd engine</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> trace(root):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    nodes, edges <span class="op">=</span> <span class="bu">set</span>(), <span class="bu">set</span>()</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> build(v):</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> v <span class="kw">not</span> <span class="kw">in</span> nodes:</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>            nodes.add(v)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> child <span class="kw">in</span> v._prev:</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>                edges.add((child, v))</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>                build(child)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    build(root)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> nodes, edges</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> draw_dot(root, <span class="bu">format</span><span class="op">=</span><span class="st">'svg'</span>, rankdir<span class="op">=</span><span class="st">'LR'</span>):</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="co">    format: png | svg | ...</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="co">    rankdir: TB (top to bottom graph) | LR (left to right)</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> rankdir <span class="kw">in</span> [<span class="st">'LR'</span>, <span class="st">'TB'</span>]</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>    nodes, edges <span class="op">=</span> trace(root)</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>    dot <span class="op">=</span> Digraph(<span class="bu">format</span><span class="op">=</span><span class="bu">format</span>, graph_attr<span class="op">=</span>{<span class="st">'rankdir'</span>: rankdir}) <span class="co">#, node_attr={'rankdir': 'TB'})</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> n <span class="kw">in</span> nodes:</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>        dot.node(name<span class="op">=</span><span class="bu">str</span>(<span class="bu">id</span>(n)), label <span class="op">=</span> <span class="st">"{</span><span class="sc">%s</span><span class="st"> data </span><span class="sc">%.4f</span><span class="st"> | grad </span><span class="sc">%.4f</span><span class="st"> }"</span> <span class="op">%</span> (n.label, n._data, n.grad), shape<span class="op">=</span><span class="st">'record'</span>)</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> n._op:</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>            dot.node(name<span class="op">=</span><span class="bu">str</span>(<span class="bu">id</span>(n)) <span class="op">+</span> n._op, label<span class="op">=</span>n._op)</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>            dot.edge(<span class="bu">str</span>(<span class="bu">id</span>(n)) <span class="op">+</span> n._op, <span class="bu">str</span>(<span class="bu">id</span>(n)))</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> n1, n2 <span class="kw">in</span> edges:</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>        dot.edge(<span class="bu">str</span>(<span class="bu">id</span>(n1)), <span class="bu">str</span>(<span class="bu">id</span>(n2)) <span class="op">+</span> n2._op)</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> dot</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In the context of deep learning and automatic differentiation, the Value class is designed to encapsulate a scalar value and its relationships within a computational graph. This abstraction is essential for constructing mathematical expressions from basic operations and for performing the forward pass, which evaluates the expression.</p>
<p>The Value class is initialized with data and optional parameters specifying its children (or dependencies) and the operation that produced it. Each instance of the Value class can have a gradient, which is initialized as zero and can be updated during backpropagation.</p>
<p>Two fundamental operations are implemented for instances of the Value class: addition (<strong>add</strong>) and multiplication (<strong>mul</strong>). These methods allow two Value instances (or a Value and a scalar) to be added or multiplied, respectively. The results of these operations are also Value instances, maintaining the relationships in the computational graph.</p>
<p>This ability to build out mathematical expressions using only addition and multiplication allows for the construction of a broad variety of functions. For example, given multiple inputs (a, b, c, f), we can formulate a mathematical expression that generates a single output (l). After the forward pass, the output value is calculated and can be visualized, as demonstrated in the example where the forward pass output is -8.</p>
<p>In summary, the Value class is a fundamental building block for creating and navigating a computational graph in the context of automatic differentiation, making it an invaluable tool in any deep learning framework.</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/autograd.py#L170" target="_blank" style="float:right; font-size:smaller">source</a></p>
<section id="value" class="level3">
<h3 class="anchored" data-anchor-id="value">Value</h3>
<blockquote class="blockquote">
<pre><code> Value (data, _children=(), _op='', label='')</code></pre>
</blockquote>
<p>Represents a node within a computational graph.</p>
<p>This class encapsulates a single value and its relationships in the graph, making it easy to track and manage the value’s dependencies, the operation that produced it, and whether it requires a gradient for backpropagation. It’s central to the functioning of automatic differentiation within deep learning frameworks.</p>
<p>Attributes: op (Operator) _prev (Set[‘Value’]) cached_data (NDArray) requires_grad (bool)</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> Value(<span class="fl">2.0</span>, label<span class="op">=</span><span class="st">'a'</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> Value(<span class="op">-</span><span class="fl">3.0</span>, label<span class="op">=</span><span class="st">'b'</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> Value(<span class="fl">10.0</span>, label<span class="op">=</span><span class="st">'c'</span>)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>e <span class="op">=</span> a<span class="op">*</span>b<span class="op">;</span> e.label<span class="op">=</span><span class="st">'e'</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> e <span class="op">+</span> c<span class="op">;</span> d.label<span class="op">=</span><span class="st">'d'</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> Value(<span class="op">-</span><span class="fl">2.0</span>, label<span class="op">=</span><span class="st">'f'</span>)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> d<span class="op">*</span>f<span class="op">;</span> L.label<span class="op">=</span><span class="st">'L'</span> </span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>draw_dot(L)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="00_autograd_files/figure-html/cell-7-output-1.svg" class="img-fluid"></p>
</div>
</div>
<p>The code provided builds upon the previously discussed <a href="https://m0saan.github.io/minima/autograd.html#value"><code>Value</code></a> class, which acts as a node within a computational graph in the context of automatic differentiation. It demonstrates how to define scalar values <code>a</code>, <code>b</code>, <code>c</code>, and <code>f</code> and use them to build a computational graph. The graph computes the expression <code>L = (a * b + c) * f</code>, represented in nodes labeled ‘e’, ‘d’, and ‘L’.</p>
<p>The focus of this explanation is the process of backpropagation and the computation of gradients for every node in the graph, which is crucial for training neural networks. In a neural network setting, the loss function <code>L</code> would typically be calculated with respect to the network’s weights. Here, these weights are abstractly represented by the scalar variables <code>a</code>, <code>b</code>, <code>c</code>, and <code>f</code>.</p>
<p>The fundamental idea behind backpropagation is to compute the derivative of the output value <code>L</code> with respect to every node in the graph. These derivatives represent the impact each node has on the final output. They are stored in the <code>grad</code> attribute of the <a href="https://m0saan.github.io/minima/autograd.html#value"><code>Value</code></a> class, which is initialized to zero, signifying that there is initially no effect on the output.</p>
<p>In this context, a gradient of zero means changing the value of a node has no effect on the final output, or loss function. After performing backpropagation, the <code>grad</code> attribute will store the actual derivative of <code>L</code> with respect to that node. This is essential information when training a neural network because it dictates how to adjust the weights (in this example, <code>a</code>, <code>b</code>, <code>c</code>, and <code>f</code>) to minimize the loss function <code>L</code>.</p>
<p>The function <code>draw_dot(L)</code> is presumably used to visualize this computational graph, including both the <code>data</code> and the <code>grad</code> of each node. This visualization aids in understanding the forward and backward passes of computation within the graph.</p>
<p>In conclusion, this code snippet creates a simple computational graph using the <a href="https://m0saan.github.io/minima/autograd.html#value"><code>Value</code></a> class, computes a mathematical expression, and prepares for backpropagation. The next steps would involve the actual calculation of the gradients, enabling the iterative optimization of weights based on their influence on the final output.</p>
</section>
</section>
<section id="manual-gradient" class="level2">
<h2 class="anchored" data-anchor-id="manual-gradient">Manual gradient</h2>
<section id="base-case-l-grad" class="level3">
<h3 class="anchored" data-anchor-id="base-case-l-grad">base case (<code>L grad</code>)</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> lol():</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    h <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    a <span class="op">=</span> Value(<span class="fl">2.0</span>, label<span class="op">=</span><span class="st">'a'</span>)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> Value(<span class="op">-</span><span class="fl">3.0</span>, label<span class="op">=</span><span class="st">'b'</span>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    c <span class="op">=</span> Value(<span class="fl">10.0</span>, label<span class="op">=</span><span class="st">'c'</span>)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    e <span class="op">=</span> a<span class="op">*</span>b<span class="op">;</span> e.label<span class="op">=</span><span class="st">'e'</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    d <span class="op">=</span> e <span class="op">+</span> c<span class="op">;</span> d.label<span class="op">=</span><span class="st">'d'</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    f <span class="op">=</span> Value(<span class="op">-</span><span class="fl">2.0</span>, label<span class="op">=</span><span class="st">'f'</span>)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>    L <span class="op">=</span> d<span class="op">*</span>f<span class="op">;</span> L.label<span class="op">=</span><span class="st">'L'</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    L1 <span class="op">=</span> L.data</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>    a <span class="op">=</span> Value(<span class="fl">2.0</span>, label<span class="op">=</span><span class="st">'a'</span>)</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> Value(<span class="op">-</span><span class="fl">3.0</span>, label<span class="op">=</span><span class="st">'b'</span>)</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>    c <span class="op">=</span> Value(<span class="fl">10.0</span>, label<span class="op">=</span><span class="st">'c'</span>)</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>    e <span class="op">=</span> a<span class="op">*</span>b<span class="op">;</span> e.label<span class="op">=</span><span class="st">'e'</span></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>    d <span class="op">=</span> e <span class="op">+</span> c<span class="op">;</span> d.label<span class="op">=</span><span class="st">'d'</span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>    f <span class="op">=</span> Value(<span class="op">-</span><span class="fl">2.0</span>, label<span class="op">=</span><span class="st">'f'</span>)</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>    L <span class="op">=</span> d<span class="op">*</span>f<span class="op">;</span> L.label<span class="op">=</span><span class="st">'L'</span> </span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>    L2 <span class="op">=</span> L.data <span class="op">+</span> h</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'grad: </span><span class="sc">{</span>(L2 <span class="op">-</span> L1) <span class="op">/</span> h<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>lol()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>grad: 1.000000000000334</code></pre>
</div>
</div>
<p>sure enough it’s 1</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>L.grad <span class="op">=</span> <span class="dv">1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="f" class="level4">
<h4 class="anchored" data-anchor-id="f">f</h4>
<p>Here is a generic version of <code>lol</code></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> lol(label):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> foo(v, label):</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> v.label <span class="op">==</span> label: v.data <span class="op">+=</span> h</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    h <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    a <span class="op">=</span> Value(<span class="fl">2.0</span>, label<span class="op">=</span><span class="st">'a'</span>)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> Value(<span class="op">-</span><span class="fl">3.0</span>, label<span class="op">=</span><span class="st">'b'</span>)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    c <span class="op">=</span> Value(<span class="fl">10.0</span>, label<span class="op">=</span><span class="st">'c'</span>)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>    e <span class="op">=</span> a<span class="op">*</span>b<span class="op">;</span> e.label<span class="op">=</span><span class="st">'e'</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>    d <span class="op">=</span> e <span class="op">+</span> c<span class="op">;</span> d.label<span class="op">=</span><span class="st">'d'</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>    f <span class="op">=</span> Value(<span class="op">-</span><span class="fl">2.0</span>, label<span class="op">=</span><span class="st">'f'</span>)</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>    L <span class="op">=</span> d<span class="op">*</span>f<span class="op">;</span> L.label<span class="op">=</span><span class="st">'L'</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>    L1 <span class="op">=</span> L.data</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>    a <span class="op">=</span> Value(<span class="fl">2.0</span>, label<span class="op">=</span><span class="st">'a'</span>)<span class="op">;</span> foo(a, label)</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> Value(<span class="op">-</span><span class="fl">3.0</span>, label<span class="op">=</span><span class="st">'b'</span>)<span class="op">;</span> foo(b, label)</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>    c <span class="op">=</span> Value(<span class="fl">10.0</span>, label<span class="op">=</span><span class="st">'c'</span>)<span class="op">;</span> foo(c, label)</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>    e <span class="op">=</span> a<span class="op">*</span>b<span class="op">;</span> e.label<span class="op">=</span><span class="st">'e'</span><span class="op">;</span> foo(e, label)</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>    d <span class="op">=</span> e <span class="op">+</span> c<span class="op">;</span> d.label<span class="op">=</span><span class="st">'d'</span><span class="op">;</span> foo(d, label)</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>    f <span class="op">=</span> Value(<span class="op">-</span><span class="fl">2.0</span>, label<span class="op">=</span><span class="st">'f'</span>)<span class="op">;</span> foo(f, label)</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>    L <span class="op">=</span> d<span class="op">*</span>f<span class="op">;</span> L.label<span class="op">=</span><span class="st">'L'</span><span class="op">;</span> foo(L, label) </span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>    L2 <span class="op">=</span> L.data</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'grad: </span><span class="sc">{</span>(L2 <span class="op">-</span> L1) <span class="op">/</span> h<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>lol(<span class="st">'f'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>grad: 3.9999999999995595</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>f.grad <span class="op">=</span> <span class="dv">4</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>lol(<span class="st">'d'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>grad: -2.000000000000668</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>d.grad <span class="op">=</span> <span class="op">-</span><span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s draw what we have up to this point</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>draw_dot(L)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="00_autograd_files/figure-html/cell-14-output-1.svg" class="img-fluid"></p>
</div>
</div>
<p>Sure, here’s the step by step derivation for each of the variables:</p>
<ol type="1">
<li>With respect to <code>a</code>:</li>
</ol>
<p>Given that <code>L = (a*b + c) * f</code>, we will apply the product rule for differentiation.</p>
<p>The derivative of <code>a*b</code> with respect to <code>a</code> is <code>b</code>, and the derivative of <code>c</code> with respect to <code>a</code> is <code>0</code>. Therefore:</p>
<p><span class="math display">\[
\frac{dL}{da} = f \cdot \frac{d(a*b + c)}{da} = f \cdot (b + 0) = b \cdot f
\]</span></p>
<ol start="2" type="1">
<li>With respect to <code>b</code>:</li>
</ol>
<p>The derivative of <code>a*b</code> with respect to <code>b</code> is <code>a</code>, and the derivative of <code>c</code> with respect to <code>b</code> is <code>0</code>. Therefore:</p>
<p><span class="math display">\[
\frac{dL}{db} = f \cdot \frac{d(a*b + c)}{db} = f \cdot (a + 0) = a \cdot f
\]</span></p>
<ol start="3" type="1">
<li>With respect to <code>c</code>:</li>
</ol>
<p>The derivative of <code>a*b</code> with respect to <code>c</code> is <code>0</code>, and the derivative of <code>c</code> with respect to <code>c</code> is <code>1</code>. Therefore:</p>
<p><span class="math display">\[
\frac{dL}{dc} = f \cdot \frac{d(a*b + c)}{dc} = f \cdot (0 + 1) = f
\]</span></p>
<ol start="4" type="1">
<li>With respect to <code>f</code>:</li>
</ol>
<p>The derivative of <code>(a*b + c)</code> with respect to <code>f</code> is <code>0</code>, and <code>f</code> is just <code>f</code>, therefore:</p>
<p><span class="math display">\[
\frac{dL}{df} = (a*b + c) \cdot \frac{df}{df} = a*b + c
\]</span></p>
<ol start="5" type="1">
<li>With respect to <code>e</code> (where <code>e = a*b</code>):</li>
</ol>
<p>The derivative of <code>e + c</code> with respect to <code>e</code> is <code>1</code>. Therefore:</p>
<p><span class="math display">\[
\frac{dL}{de} = f \cdot \frac{d(e + c)}{de} = f \cdot 1 = f
\]</span></p>
<ol start="6" type="1">
<li>With respect to <code>d</code> (where <code>d = e + c</code>):</li>
</ol>
<p>The derivative of <code>d</code> with respect to <code>d</code> is <code>1</code>. Therefore:</p>
<p><span class="math display">\[
\frac{dL}{dd} = f \cdot \frac{df}{df} = f
\]</span></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>lol(<span class="st">'e'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>grad: -2.000000000000668</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>e.grad <span class="op">=</span> <span class="op">-</span><span class="dv">2</span> <span class="co"># 1 * d.grad</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>lol(<span class="st">'c'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>grad: -1.9999999999988916</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>c.grad <span class="op">=</span> <span class="op">-</span><span class="dv">2</span> <span class="co"># 1 * d.grad</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>draw_dot(L)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="00_autograd_files/figure-html/cell-19-output-1.svg" class="img-fluid"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>lol(<span class="st">'a'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>grad: 6.000000000000227</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>a.grad <span class="op">=</span> <span class="dv">6</span>  <span class="co"># b * e.grad</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>lol(<span class="st">'b'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>grad: -3.9999999999995595</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>b.grad <span class="op">=</span> <span class="op">-</span><span class="dv">4</span> <span class="co"># a * e.grad</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>draw_dot(L)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="00_autograd_files/figure-html/cell-24-output-1.svg" class="img-fluid"></p>
</div>
</div>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/autograd.py#L170" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
</section>
<section id="value-1" class="level3">
<h3 class="anchored" data-anchor-id="value-1">Value</h3>
<blockquote class="blockquote">
<pre><code> Value (data, _children=(), _op='', label='')</code></pre>
</blockquote>
<p>Represents a node within a computational graph.</p>
<p>This class encapsulates a single value and its relationships in the graph, making it easy to track and manage the value’s dependencies, the operation that produced it, and whether it requires a gradient for backpropagation. It’s central to the functioning of automatic differentiation within deep learning frameworks.</p>
<p>Attributes: op (Operator) _prev (Set[‘Value’]) cached_data (NDArray) requires_grad (bool)</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> Value(<span class="fl">2.0</span>, label<span class="op">=</span><span class="st">'a'</span>)</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> Value(<span class="op">-</span><span class="fl">3.0</span>, label<span class="op">=</span><span class="st">'b'</span>)</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> Value(<span class="fl">10.0</span>, label<span class="op">=</span><span class="st">'c'</span>)</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>e <span class="op">=</span> a<span class="op">*</span>b<span class="op">;</span> e.label<span class="op">=</span><span class="st">'e'</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> e <span class="op">+</span> c<span class="op">;</span> d.label<span class="op">=</span><span class="st">'d'</span></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> Value(<span class="op">-</span><span class="fl">2.0</span>, label<span class="op">=</span><span class="st">'f'</span>)</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> d<span class="op">*</span>f<span class="op">;</span> L.label<span class="op">=</span><span class="st">'L'</span> </span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>draw_dot(L)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="00_autograd_files/figure-html/cell-26-output-1.svg" class="img-fluid"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>L.grad <span class="op">=</span> <span class="dv">1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>L._backward()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>draw_dot(L)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="00_autograd_files/figure-html/cell-29-output-1.svg" class="img-fluid"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>d._backward()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>draw_dot(L)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="00_autograd_files/figure-html/cell-31-output-1.svg" class="img-fluid"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>c._backward()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We expect that nothing will happen</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>draw_dot(L)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="00_autograd_files/figure-html/cell-33-output-1.svg" class="img-fluid"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>e._backward()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>draw_dot(L)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="00_autograd_files/figure-html/cell-35-output-1.svg" class="img-fluid"></p>
</div>
</div>
<p>sure enough, exactly as we did before</p>
<p>We can do thid process automatically using topo sort algorithms, which’s will give us the correct order on which to call _backward on</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> Value(<span class="fl">2.0</span>, label<span class="op">=</span><span class="st">'a'</span>)</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> Value(<span class="op">-</span><span class="fl">3.0</span>, label<span class="op">=</span><span class="st">'b'</span>)</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> Value(<span class="fl">10.0</span>, label<span class="op">=</span><span class="st">'c'</span>)</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>e <span class="op">=</span> a<span class="op">*</span>b<span class="op">;</span> e.label<span class="op">=</span><span class="st">'e'</span></span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> e <span class="op">+</span> c<span class="op">;</span> d.label<span class="op">=</span><span class="st">'d'</span></span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> Value(<span class="op">-</span><span class="fl">2.0</span>, label<span class="op">=</span><span class="st">'f'</span>)</span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> d<span class="op">*</span>f<span class="op">;</span> L.label<span class="op">=</span><span class="st">'L'</span> </span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>draw_dot(L)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="00_autograd_files/figure-html/cell-36-output-1.svg" class="img-fluid"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co"># topological order all of the children in the graph</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>topo <span class="op">=</span> []</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>visited <span class="op">=</span> <span class="bu">set</span>()</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_topo(v):</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> v <span class="kw">not</span> <span class="kw">in</span> visited:</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>        visited.add(v)</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> child <span class="kw">in</span> v._prev:</span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>            build_topo(child)</span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a>        topo.append(v)</span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a>build_topo(L)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>topo</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>[Value(data=10.0, grad=0),
 Value(data=-3.0, grad=0),
 Value(data=2.0, grad=0),
 Value(data=-6.0, grad=0),
 Value(data=4.0, grad=0),
 Value(data=-2.0, grad=0),
 Value(data=-8.0, grad=0)]</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="co"># go one variable at a time and apply the chain rule to get its gradient</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>L.grad <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> v <span class="kw">in</span> <span class="bu">reversed</span>(topo):</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>    v._backward()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>draw_dot(L)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="00_autograd_files/figure-html/cell-40-output-1.svg" class="img-fluid"></p>
</div>
</div>
<p>So let’s now update the Value class with this logic</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/autograd.py#L170" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="value-2" class="level3">
<h3 class="anchored" data-anchor-id="value-2">Value</h3>
<blockquote class="blockquote">
<pre><code> Value (data, _children=(), _op='', label='')</code></pre>
</blockquote>
<p>Represents a node within a computational graph.</p>
<p>This class encapsulates a single value and its relationships in the graph, making it easy to track and manage the value’s dependencies, the operation that produced it, and whether it requires a gradient for backpropagation. It’s central to the functioning of automatic differentiation within deep learning frameworks.</p>
<p>Attributes: op (Operator) _prev (Set[‘Value’]) cached_data (NDArray) requires_grad (bool)</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> Value(<span class="fl">2.0</span>, label<span class="op">=</span><span class="st">'a'</span>)</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> Value(<span class="op">-</span><span class="fl">3.0</span>, label<span class="op">=</span><span class="st">'b'</span>)</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> Value(<span class="fl">10.0</span>, label<span class="op">=</span><span class="st">'c'</span>)</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>e <span class="op">=</span> a<span class="op">*</span>b<span class="op">;</span> e.label<span class="op">=</span><span class="st">'e'</span></span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> e <span class="op">+</span> c<span class="op">;</span> d.label<span class="op">=</span><span class="st">'d'</span></span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> Value(<span class="op">-</span><span class="fl">2.0</span>, label<span class="op">=</span><span class="st">'f'</span>)</span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> d<span class="op">*</span>f<span class="op">;</span> L.label<span class="op">=</span><span class="st">'L'</span> </span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a>draw_dot(L)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="00_autograd_files/figure-html/cell-42-output-1.svg" class="img-fluid"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>L.backward()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>draw_dot(L)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="00_autograd_files/figure-html/cell-44-output-1.svg" class="img-fluid"></p>
</div>
</div>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/autograd.py#L170" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="value-3" class="level3">
<h3 class="anchored" data-anchor-id="value-3">Value</h3>
<blockquote class="blockquote">
<pre><code> Value (data, children=(), op='', label='')</code></pre>
</blockquote>
<p>A class representing a scalar value and its gradient in a computational graph.</p>
<p>Attributes: - data (float): the scalar value associated with this node - grad (float): the gradient of the output of the computational graph w.r.t. this node’s value - label (str): a label for this node, used for debugging and visualization purposes - _op (str): a string representation of the operation that produced this node in the computational graph - _prev (set of Value objects): the set of nodes that contributed to the computation of this node - _backward (function): a function that computes the gradients of this node w.r.t. its inputs</p>
<p>Methods: - <strong>init</strong>(self, data, children=(), op=’‘, label=’’): Initializes a Value object with the given data, children, op, and label - <strong>repr</strong>(self): Returns a string representation of this Value object - <strong>add</strong>(self, other): Implements the addition operation between two Value objects - <strong>mul</strong>(self, other): Implements the multiplication operation between two Value objects - item(self): Returns the scalar value associated with this Value object - tanh(self): Applies the hyperbolic tangent function to this Value object and returns a new Value object</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/autograd.py#L508" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="all_devices" class="level3">
<h3 class="anchored" data-anchor-id="all_devices">all_devices</h3>
<blockquote class="blockquote">
<pre><code> all_devices ()</code></pre>
</blockquote>
<p>return a list of all available devices</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/autograd.py#L504" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="cpu" class="level3">
<h3 class="anchored" data-anchor-id="cpu">cpu</h3>
<blockquote class="blockquote">
<pre><code> cpu ()</code></pre>
</blockquote>
<p>Return cpu device</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/autograd.py#L489" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="cpudevice" class="level3">
<h3 class="anchored" data-anchor-id="cpudevice">CPUDevice</h3>
<blockquote class="blockquote">
<pre><code> CPUDevice ()</code></pre>
</blockquote>
<p>Represents data that sits in CPU</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/autograd.py#L485" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="device" class="level3">
<h3 class="anchored" data-anchor-id="device">Device</h3>
<blockquote class="blockquote">
<pre><code> Device ()</code></pre>
</blockquote>
<p>Indicates the device supporting an NDArray.</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/autograd.py#L513" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="operator" class="level3">
<h3 class="anchored" data-anchor-id="operator">Operator</h3>
<blockquote class="blockquote">
<pre><code> Operator ()</code></pre>
</blockquote>
<p>Initialize self. See help(type(self)) for accurate signature.</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/autograd.py#L525" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="tensorop" class="level3">
<h3 class="anchored" data-anchor-id="tensorop">TensorOp</h3>
<blockquote class="blockquote">
<pre><code> TensorOp ()</code></pre>
</blockquote>
<p>Op class specialized to output tensors, will be alternate subclasses for other structures</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="co">#| export</span></span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Value:</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Represents a node within a computational graph.</span></span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a><span class="co">    This class encapsulates a single value and its relationships in the graph, making it easy to track and manage the value's dependencies, </span></span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a><span class="co">    the operation that produced it, and whether it requires a gradient for backpropagation. It's central to the functioning of automatic </span></span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a><span class="co">    differentiation within deep learning frameworks.</span></span>
<span id="cb62-9"><a href="#cb62-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-10"><a href="#cb62-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Attributes:</span></span>
<span id="cb62-11"><a href="#cb62-11" aria-hidden="true" tabindex="-1"></a><span class="co">        op (Operator)</span></span>
<span id="cb62-12"><a href="#cb62-12" aria-hidden="true" tabindex="-1"></a><span class="co">        _prev (Set['Value']) </span></span>
<span id="cb62-13"><a href="#cb62-13" aria-hidden="true" tabindex="-1"></a><span class="co">        cached_data (NDArray)</span></span>
<span id="cb62-14"><a href="#cb62-14" aria-hidden="true" tabindex="-1"></a><span class="co">        requires_grad (bool)</span></span>
<span id="cb62-15"><a href="#cb62-15" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb62-16"><a href="#cb62-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,</span>
<span id="cb62-17"><a href="#cb62-17" aria-hidden="true" tabindex="-1"></a>                 op: Operator, <span class="co"># The operator that produced this node. If the node was initialized from actual data, this is 'None'.</span></span>
<span id="cb62-18"><a href="#cb62-18" aria-hidden="true" tabindex="-1"></a>                 prev: Set[<span class="st">'Value'</span>], <span class="co"># The set of values that this value directly depends on. It's the union of the `_next` sets of all the values in `args`.</span></span>
<span id="cb62-19"><a href="#cb62-19" aria-hidden="true" tabindex="-1"></a>                 cached_data: NDArray, <span class="co"># The actual data for this value. It's `None` for values that aren't yet computed.</span></span>
<span id="cb62-20"><a href="#cb62-20" aria-hidden="true" tabindex="-1"></a>                 requires_grad: <span class="bu">bool</span>): <span class="co"># Specifies whether this node requires a gradient. This is `False` for nodes that don't need gradients.</span></span>
<span id="cb62-21"><a href="#cb62-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb62-22"><a href="#cb62-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._op <span class="op">=</span> op</span>
<span id="cb62-23"><a href="#cb62-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._prev <span class="op">=</span> op</span>
<span id="cb62-24"><a href="#cb62-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cached_data <span class="op">=</span> cached_data</span>
<span id="cb62-25"><a href="#cb62-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.requires_grad <span class="op">=</span> requires_grad</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/autograd.py#L532" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="tensor" class="level3">
<h3 class="anchored" data-anchor-id="tensor">Tensor</h3>
<blockquote class="blockquote">
<pre><code> Tensor (array, device:Optional[__main__.Device]=None, dtype=None,
         requires_grad=True, **kwargs)</code></pre>
</blockquote>
<p>A Tensor represents a multidimensional array of values in a computational graph.</p>
<p>Attributes: - data: The actual data of the tensor. It is computed lazily. - children: Other tensors that this tensor depends on for computing its value. - requires_grad: Whether this tensor needs to compute gradients.</p>
<p>Methods: - realize_data: Computes and returns the actual data for this tensor. - shape: Returns the shape of this tensor. - dtype: Returns the data type of this tensor.</p>
<p>Example: &gt;&gt;&gt; t1 = Tensor([[1.0, 2.0], [3.0, 4.0]]) &gt;&gt;&gt; print(t1.shape) (2, 2) &gt;&gt;&gt; print(t1.dtype) float64</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> unittest</span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> minima.autograd <span class="im">import</span> Tensor</span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TestTensor(unittest.TestCase):</span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> test_create_tensor(<span class="va">self</span>):</span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a>        t1 <span class="op">=</span> Tensor([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>])</span>
<span id="cb64-9"><a href="#cb64-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertTrue(np.array_equal(t1.realize_data(), np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>])))</span>
<span id="cb64-10"><a href="#cb64-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t1.shape, (<span class="dv">3</span>,))</span>
<span id="cb64-11"><a href="#cb64-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t1.dtype, np.float64)</span>
<span id="cb64-12"><a href="#cb64-12" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-13"><a href="#cb64-13" aria-hidden="true" tabindex="-1"></a>        t2 <span class="op">=</span> Tensor([[<span class="dv">1</span>, <span class="dv">2</span>], [<span class="dv">3</span>, <span class="dv">4</span>]])</span>
<span id="cb64-14"><a href="#cb64-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertTrue(np.array_equal(t2.realize_data(), np.array([[<span class="dv">1</span>, <span class="dv">2</span>], [<span class="dv">3</span>, <span class="dv">4</span>]])))</span>
<span id="cb64-15"><a href="#cb64-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t2.shape, (<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb64-16"><a href="#cb64-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t2.dtype, np.float64)</span>
<span id="cb64-17"><a href="#cb64-17" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-18"><a href="#cb64-18" aria-hidden="true" tabindex="-1"></a>        t3 <span class="op">=</span> Tensor(np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>]), dtype<span class="op">=</span>np.int32)</span>
<span id="cb64-19"><a href="#cb64-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertTrue(np.array_equal(t3.realize_data(), np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], dtype<span class="op">=</span>np.int32)))</span>
<span id="cb64-20"><a href="#cb64-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t3.shape, (<span class="dv">3</span>,))</span>
<span id="cb64-21"><a href="#cb64-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t3.dtype, np.int32)</span>
<span id="cb64-22"><a href="#cb64-22" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-23"><a href="#cb64-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> test_create_tensor_from_tensor(<span class="va">self</span>):</span>
<span id="cb64-24"><a href="#cb64-24" aria-hidden="true" tabindex="-1"></a>        t1 <span class="op">=</span> Tensor([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>])</span>
<span id="cb64-25"><a href="#cb64-25" aria-hidden="true" tabindex="-1"></a>        t2 <span class="op">=</span> Tensor(t1)</span>
<span id="cb64-26"><a href="#cb64-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertTrue(np.array_equal(t2.realize_data(), np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>])))</span>
<span id="cb64-27"><a href="#cb64-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t2.shape, (<span class="dv">3</span>,))</span>
<span id="cb64-28"><a href="#cb64-28" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t2.dtype, np.float64)</span>
<span id="cb64-29"><a href="#cb64-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-30"><a href="#cb64-30" aria-hidden="true" tabindex="-1"></a>        t3 <span class="op">=</span> Tensor(np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>]), dtype<span class="op">=</span>np.int32)</span>
<span id="cb64-31"><a href="#cb64-31" aria-hidden="true" tabindex="-1"></a>        t4 <span class="op">=</span> Tensor(t3)</span>
<span id="cb64-32"><a href="#cb64-32" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertTrue(np.array_equal(t4.realize_data(), np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], dtype<span class="op">=</span>np.int32)))</span>
<span id="cb64-33"><a href="#cb64-33" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t4.shape, (<span class="dv">3</span>,))</span>
<span id="cb64-34"><a href="#cb64-34" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t4.dtype, np.int32)</span>
<span id="cb64-35"><a href="#cb64-35" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-36"><a href="#cb64-36" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> test_create_tensor_with_device(<span class="va">self</span>):</span>
<span id="cb64-37"><a href="#cb64-37" aria-hidden="true" tabindex="-1"></a>        t1 <span class="op">=</span> Tensor([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], device<span class="op">=</span><span class="st">'cpu'</span>)</span>
<span id="cb64-38"><a href="#cb64-38" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t1.device, <span class="st">'cpu'</span>)</span>
<span id="cb64-39"><a href="#cb64-39" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-40"><a href="#cb64-40" aria-hidden="true" tabindex="-1"></a>        t2 <span class="op">=</span> Tensor([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], device<span class="op">=</span><span class="st">'cuda'</span>)</span>
<span id="cb64-41"><a href="#cb64-41" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t2.device, <span class="st">'cuda'</span>)</span>
<span id="cb64-42"><a href="#cb64-42" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-43"><a href="#cb64-43" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> test_create_tensor_with_requires_grad(<span class="va">self</span>):</span>
<span id="cb64-44"><a href="#cb64-44" aria-hidden="true" tabindex="-1"></a>        t1 <span class="op">=</span> Tensor([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb64-45"><a href="#cb64-45" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertTrue(t1.requires_grad)</span>
<span id="cb64-46"><a href="#cb64-46" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-47"><a href="#cb64-47" aria-hidden="true" tabindex="-1"></a>        t2 <span class="op">=</span> Tensor([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb64-48"><a href="#cb64-48" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertFalse(t2.requires_grad)</span>
<span id="cb64-49"><a href="#cb64-49" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-50"><a href="#cb64-50" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> test_create_tensor_with_kwargs(<span class="va">self</span>):</span>
<span id="cb64-51"><a href="#cb64-51" aria-hidden="true" tabindex="-1"></a>        t1 <span class="op">=</span> Tensor([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], device<span class="op">=</span><span class="st">'cuda'</span>, dtype<span class="op">=</span>np.float32, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb64-52"><a href="#cb64-52" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t1.device, <span class="st">'cuda'</span>)</span>
<span id="cb64-53"><a href="#cb64-53" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t1.dtype, np.float32)</span>
<span id="cb64-54"><a href="#cb64-54" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertTrue(t1.requires_grad)</span>
<span id="cb64-55"><a href="#cb64-55" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-56"><a href="#cb64-56" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> test_create_tensor_from_numpy(<span class="va">self</span>):</span>
<span id="cb64-57"><a href="#cb64-57" aria-hidden="true" tabindex="-1"></a>        np_array <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>])</span>
<span id="cb64-58"><a href="#cb64-58" aria-hidden="true" tabindex="-1"></a>        t1 <span class="op">=</span> Tensor(np_array)</span>
<span id="cb64-59"><a href="#cb64-59" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertTrue(np.array_equal(t1.realize_data(), np_array))</span>
<span id="cb64-60"><a href="#cb64-60" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t1.shape, (<span class="dv">3</span>,))</span>
<span id="cb64-61"><a href="#cb64-61" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t1.dtype, np.float64)</span>
<span id="cb64-62"><a href="#cb64-62" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-63"><a href="#cb64-63" aria-hidden="true" tabindex="-1"></a>        np_array <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], dtype<span class="op">=</span>np.int32)</span>
<span id="cb64-64"><a href="#cb64-64" aria-hidden="true" tabindex="-1"></a>        t2 <span class="op">=</span> Tensor(np_array)</span>
<span id="cb64-65"><a href="#cb64-65" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertTrue(np.array_equal(t2.realize_data(), np_array))</span>
<span id="cb64-66"><a href="#cb64-66" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t2.shape, (<span class="dv">3</span>,))</span>
<span id="cb64-67"><a href="#cb64-67" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t2.dtype, np.int32)</span>
<span id="cb64-68"><a href="#cb64-68" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-69"><a href="#cb64-69" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> test_create_tensor_from_numpy_with_device(<span class="va">self</span>):</span>
<span id="cb64-70"><a href="#cb64-70" aria-hidden="true" tabindex="-1"></a>        np_array <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>])</span>
<span id="cb64-71"><a href="#cb64-71" aria-hidden="true" tabindex="-1"></a>        t1 <span class="op">=</span> Tensor(np_array, device<span class="op">=</span><span class="st">'cuda'</span>)</span>
<span id="cb64-72"><a href="#cb64-72" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t1.device, <span class="st">'cuda'</span>)</span>
<span id="cb64-73"><a href="#cb64-73" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-74"><a href="#cb64-74" aria-hidden="true" tabindex="-1"></a>        np_array <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], dtype<span class="op">=</span>np.int32)</span>
<span id="cb64-75"><a href="#cb64-75" aria-hidden="true" tabindex="-1"></a>        t2 <span class="op">=</span> Tensor(np_array, device<span class="op">=</span><span class="st">'cuda'</span>)</span>
<span id="cb64-76"><a href="#cb64-76" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t2.device, <span class="st">'cuda'</span>)</span>
<span id="cb64-77"><a href="#cb64-77" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-78"><a href="#cb64-78" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> test_create_tensor_from_numpy_with_requires_grad(<span class="va">self</span>):</span>
<span id="cb64-79"><a href="#cb64-79" aria-hidden="true" tabindex="-1"></a>        np_array <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>])</span>
<span id="cb64-80"><a href="#cb64-80" aria-hidden="true" tabindex="-1"></a>        t1 <span class="op">=</span> Tensor(np_array, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb64-81"><a href="#cb64-81" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertTrue(t1.requires_grad)</span>
<span id="cb64-82"><a href="#cb64-82" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-83"><a href="#cb64-83" aria-hidden="true" tabindex="-1"></a>        np_array <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], dtype<span class="op">=</span>np.int32)</span>
<span id="cb64-84"><a href="#cb64-84" aria-hidden="true" tabindex="-1"></a>        t2 <span class="op">=</span> Tensor(np_array, requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb64-85"><a href="#cb64-85" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertFalse(t2.requires_grad)</span>
<span id="cb64-86"><a href="#cb64-86" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-87"><a href="#cb64-87" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> test_create_tensor_from_numpy_with_kwargs(<span class="va">self</span>):</span>
<span id="cb64-88"><a href="#cb64-88" aria-hidden="true" tabindex="-1"></a>        np_array <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>])</span>
<span id="cb64-89"><a href="#cb64-89" aria-hidden="true" tabindex="-1"></a>        t1 <span class="op">=</span> Tensor(np_array, device<span class="op">=</span><span class="st">'cuda'</span>, dtype<span class="op">=</span>np.float32, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb64-90"><a href="#cb64-90" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t1.device, <span class="st">'cuda'</span>)</span>
<span id="cb64-91"><a href="#cb64-91" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t1.dtype, np.float32)</span>
<span id="cb64-92"><a href="#cb64-92" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertTrue(t1.requires_grad)</span>
<span id="cb64-93"><a href="#cb64-93" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-94"><a href="#cb64-94" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> test_create_tensor_from_tensor_with_device(<span class="va">self</span>):</span>
<span id="cb64-95"><a href="#cb64-95" aria-hidden="true" tabindex="-1"></a>        t1 <span class="op">=</span> Tensor([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], device<span class="op">=</span><span class="st">'cpu'</span>)</span>
<span id="cb64-96"><a href="#cb64-96" aria-hidden="true" tabindex="-1"></a>        t2 <span class="op">=</span> Tensor(t1, device<span class="op">=</span><span class="st">'cuda'</span>)</span>
<span id="cb64-97"><a href="#cb64-97" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t2.device, <span class="st">'cuda'</span>)</span>
<span id="cb64-98"><a href="#cb64-98" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-99"><a href="#cb64-99" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> test_create_tensor_from_tensor_with_requires_grad(<span class="va">self</span>):</span>
<span id="cb64-100"><a href="#cb64-100" aria-hidden="true" tabindex="-1"></a>        t1 <span class="op">=</span> Tensor([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb64-101"><a href="#cb64-101" aria-hidden="true" tabindex="-1"></a>        t2 <span class="op">=</span> Tensor(t1, requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb64-102"><a href="#cb64-102" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertFalse(t2.requires_grad)</span>
<span id="cb64-103"><a href="#cb64-103" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-104"><a href="#cb64-104" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> test_create_tensor_from_tensor_with_kwargs(<span class="va">self</span>):</span>
<span id="cb64-105"><a href="#cb64-105" aria-hidden="true" tabindex="-1"></a>        t1 <span class="op">=</span> Tensor([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], device<span class="op">=</span><span class="st">'cpu'</span>, dtype<span class="op">=</span>np.float32, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb64-106"><a href="#cb64-106" aria-hidden="true" tabindex="-1"></a>        t2 <span class="op">=</span> Tensor(t1, device<span class="op">=</span><span class="st">'cuda'</span>, dtype<span class="op">=</span>np.float64, requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb64-107"><a href="#cb64-107" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t2.device, <span class="st">'cuda'</span>)</span>
<span id="cb64-108"><a href="#cb64-108" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t2.dtype, np.float64)</span>
<span id="cb64-109"><a href="#cb64-109" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertFalse(t2.requires_grad)</span>
<span id="cb64-110"><a href="#cb64-110" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-111"><a href="#cb64-111" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> test_create_tensor_from_tensor_with_different_device_and_dtype(<span class="va">self</span>):</span>
<span id="cb64-112"><a href="#cb64-112" aria-hidden="true" tabindex="-1"></a>        t1 <span class="op">=</span> Tensor([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], device<span class="op">=</span><span class="st">'cpu'</span>, dtype<span class="op">=</span>np.float32)</span>
<span id="cb64-113"><a href="#cb64-113" aria-hidden="true" tabindex="-1"></a>        t2 <span class="op">=</span> Tensor(t1, device<span class="op">=</span><span class="st">'cuda'</span>, dtype<span class="op">=</span>np.float64)</span>
<span id="cb64-114"><a href="#cb64-114" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t2.device, <span class="st">'cuda'</span>)</span>
<span id="cb64-115"><a href="#cb64-115" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t2.dtype, np.float64)</span>
<span id="cb64-116"><a href="#cb64-116" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertTrue(np.array_equal(t2.realize_data(), np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], dtype<span class="op">=</span>np.float64)))</span>
<span id="cb64-117"><a href="#cb64-117" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-118"><a href="#cb64-118" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> test_create_tensor_from_tensor_with_same_device_and_dtype(<span class="va">self</span>):</span>
<span id="cb64-119"><a href="#cb64-119" aria-hidden="true" tabindex="-1"></a>        t1 <span class="op">=</span> Tensor([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], device<span class="op">=</span><span class="st">'cpu'</span>, dtype<span class="op">=</span>np.float32)</span>
<span id="cb64-120"><a href="#cb64-120" aria-hidden="true" tabindex="-1"></a>        t2 <span class="op">=</span> Tensor(t1, device<span class="op">=</span><span class="st">'cpu'</span>, dtype<span class="op">=</span>np.float32)</span>
<span id="cb64-121"><a href="#cb64-121" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t2.device, <span class="st">'cpu'</span>)</span>
<span id="cb64-122"><a href="#cb64-122" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t2.dtype, np.float32)</span>
<span id="cb64-123"><a href="#cb64-123" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertTrue(np.array_equal(t2.realize_data(), np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], dtype<span class="op">=</span>np.float32)))</span>
<span id="cb64-124"><a href="#cb64-124" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-125"><a href="#cb64-125" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> test_create_tensor_from_tensor_with_same_device_and_dtype_and_requires_grad(<span class="va">self</span>):</span>
<span id="cb64-126"><a href="#cb64-126" aria-hidden="true" tabindex="-1"></a>        t1 <span class="op">=</span> Tensor([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], device<span class="op">=</span><span class="st">'cpu'</span>, dtype<span class="op">=</span>np.float32, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb64-127"><a href="#cb64-127" aria-hidden="true" tabindex="-1"></a>        t2 <span class="op">=</span> Tensor(t1, device<span class="op">=</span><span class="st">'cpu'</span>, dtype<span class="op">=</span>np.float32, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb64-128"><a href="#cb64-128" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t2.device, <span class="st">'cpu'</span>)</span>
<span id="cb64-129"><a href="#cb64-129" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t2.dtype, np.float32)</span>
<span id="cb64-130"><a href="#cb64-130" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertTrue(np.array_equal(t2.realize_data(), np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], dtype<span class="op">=</span>np.float32)))</span>
<span id="cb64-131"><a href="#cb64-131" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertTrue(t2.requires_grad)</span>
<span id="cb64-132"><a href="#cb64-132" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-133"><a href="#cb64-133" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> test_create_tensor_from_tensor_with_same_device_and_dtype_and_requires_grad_false(<span class="va">self</span>):</span>
<span id="cb64-134"><a href="#cb64-134" aria-hidden="true" tabindex="-1"></a>        t1 <span class="op">=</span> Tensor([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], device<span class="op">=</span><span class="st">'cpu'</span>, dtype<span class="op">=</span>np.float32, requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb64-135"><a href="#cb64-135" aria-hidden="true" tabindex="-1"></a>        t2 <span class="op">=</span> Tensor(t1, device<span class="op">=</span><span class="st">'cpu'</span>, dtype<span class="op">=</span>np.float32, requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb64-136"><a href="#cb64-136" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t2.device, <span class="st">'cpu'</span>)</span>
<span id="cb64-137"><a href="#cb64-137" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t2.dtype, np.float32)</span>
<span id="cb64-138"><a href="#cb64-138" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertTrue(np.array_equal(t2.realize_data(), np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], dtype<span class="op">=</span>np.float32)))</span>
<span id="cb64-139"><a href="#cb64-139" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertFalse(t2.requires_grad)</span>
<span id="cb64-140"><a href="#cb64-140" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-141"><a href="#cb64-141" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> test_create_tensor_from_tensor_with_same_device_and_dtype_and_requires_grad_true_false(<span class="va">self</span>):</span>
<span id="cb64-142"><a href="#cb64-142" aria-hidden="true" tabindex="-1"></a>        t1 <span class="op">=</span> Tensor([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], device<span class="op">=</span><span class="st">'cpu'</span>, dtype<span class="op">=</span>np.float32, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb64-143"><a href="#cb64-143" aria-hidden="true" tabindex="-1"></a>        t2 <span class="op">=</span> Tensor(t1, device<span class="op">=</span><span class="st">'cpu'</span>, dtype<span class="op">=</span>np.float32, requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb64-144"><a href="#cb64-144" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t2.device, <span class="st">'cpu'</span>)</span>
<span id="cb64-145"><a href="#cb64-145" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t2.dtype, np.float32)</span>
<span id="cb64-146"><a href="#cb64-146" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertTrue(np.array_equal(t2.realize_data(), np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], dtype<span class="op">=</span>np.float32)))</span>
<span id="cb64-147"><a href="#cb64-147" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertFalse(t2.requires_grad)</span>
<span id="cb64-148"><a href="#cb64-148" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-149"><a href="#cb64-149" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> test_create_tensor_from_tensor_with_same_device_and_dtype_and_requires_grad_false_true(<span class="va">self</span>):</span>
<span id="cb64-150"><a href="#cb64-150" aria-hidden="true" tabindex="-1"></a>        t1 <span class="op">=</span> Tensor([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], device<span class="op">=</span><span class="st">'cpu'</span>, dtype<span class="op">=</span>np.float32, requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb64-151"><a href="#cb64-151" aria-hidden="true" tabindex="-1"></a>        t2 <span class="op">=</span> Tensor(t1, device<span class="op">=</span><span class="st">'cpu'</span>, dtype<span class="op">=</span>np.float32, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb64-152"><a href="#cb64-152" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t2.device, <span class="st">'cpu'</span>)</span>
<span id="cb64-153"><a href="#cb64-153" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t2.dtype, np.float32)</span>
<span id="cb64-154"><a href="#cb64-154" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertTrue(np.array_equal(t2.realize_data(), np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], dtype<span class="op">=</span>np.float32)))</span>
<span id="cb64-155"><a href="#cb64-155" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertTrue(t2.requires_grad)</span>
<span id="cb64-156"><a href="#cb64-156" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-157"><a href="#cb64-157" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> test_create_tensor_from_tensor_with_different_device_and_dtype_and_requires_grad(<span class="va">self</span>):</span>
<span id="cb64-158"><a href="#cb64-158" aria-hidden="true" tabindex="-1"></a>        t1 <span class="op">=</span> Tensor([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], device<span class="op">=</span><span class="st">'cpu'</span>, dtype<span class="op">=</span>np.float32, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb64-159"><a href="#cb64-159" aria-hidden="true" tabindex="-1"></a>        t2 <span class="op">=</span> Tensor(t1, device<span class="op">=</span><span class="st">'cuda'</span>, dtype<span class="op">=</span>np.float64, requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb64-160"><a href="#cb64-160" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t2.device, <span class="st">'cuda'</span>)</span>
<span id="cb64-161"><a href="#cb64-161" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t2.dtype, np.float64)</span>
<span id="cb64-162"><a href="#cb64-162" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertTrue(np.array_equal(t2.realize_data(), np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], dtype<span class="op">=</span>np.float64)))</span>
<span id="cb64-163"><a href="#cb64-163" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertFalse(t2.requires_grad)</span>
<span id="cb64-164"><a href="#cb64-164" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-165"><a href="#cb64-165" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> test_create_tensor_from_tensor_with_different_device_and_dtype_and_requires_grad_false(<span class="va">self</span>):</span>
<span id="cb64-166"><a href="#cb64-166" aria-hidden="true" tabindex="-1"></a>        t1 <span class="op">=</span> Tensor([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], device<span class="op">=</span><span class="st">'cpu'</span>, dtype<span class="op">=</span>np.float32, requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb64-167"><a href="#cb64-167" aria-hidden="true" tabindex="-1"></a>        t2 <span class="op">=</span> Tensor(t1, device<span class="op">=</span><span class="st">'cuda'</span>, dtype<span class="op">=</span>np.float64, requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb64-168"><a href="#cb64-168" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t2.device, <span class="st">'cuda'</span>)</span>
<span id="cb64-169"><a href="#cb64-169" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t2.dtype, np.float64)</span>
<span id="cb64-170"><a href="#cb64-170" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertTrue(np.array_equal(t2.realize_data(), np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], dtype<span class="op">=</span>np.float64)))</span>
<span id="cb64-171"><a href="#cb64-171" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertFalse(t2.requires_grad)</span>
<span id="cb64-172"><a href="#cb64-172" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-173"><a href="#cb64-173" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> test_create_tensor_from_tensor_with_different_device_and_dtype_and_requires_grad_true_false(<span class="va">self</span>):</span>
<span id="cb64-174"><a href="#cb64-174" aria-hidden="true" tabindex="-1"></a>        t1 <span class="op">=</span> Tensor([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], device<span class="op">=</span><span class="st">'cpu'</span>, dtype<span class="op">=</span>np.float32, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb64-175"><a href="#cb64-175" aria-hidden="true" tabindex="-1"></a>        t2 <span class="op">=</span> Tensor(t1, device<span class="op">=</span><span class="st">'cuda'</span>, dtype<span class="op">=</span>np.float64, requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb64-176"><a href="#cb64-176" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t2.device, <span class="st">'cuda'</span>)</span>
<span id="cb64-177"><a href="#cb64-177" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t2.dtype, np.float64)</span>
<span id="cb64-178"><a href="#cb64-178" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertTrue(np.array_equal(t2.realize_data(), np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], dtype<span class="op">=</span>np.float64)))</span>
<span id="cb64-179"><a href="#cb64-179" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertFalse(t2.requires_grad)</span>
<span id="cb64-180"><a href="#cb64-180" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-181"><a href="#cb64-181" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> test_create_tensor_from_tensor_with_different_device_and_dtype_and_requires_grad_false_true(<span class="va">self</span>):</span>
<span id="cb64-182"><a href="#cb64-182" aria-hidden="true" tabindex="-1"></a>        t1 <span class="op">=</span> Tensor([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], device<span class="op">=</span><span class="st">'cpu'</span>, dtype<span class="op">=</span>np.float32, requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb64-183"><a href="#cb64-183" aria-hidden="true" tabindex="-1"></a>        t2 <span class="op">=</span> Tensor(t1, device<span class="op">=</span><span class="st">'cuda'</span>, dtype<span class="op">=</span>np.float64, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb64-184"><a href="#cb64-184" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t2.device, <span class="st">'cuda'</span>)</span>
<span id="cb64-185"><a href="#cb64-185" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t2.dtype, np.float64)</span>
<span id="cb64-186"><a href="#cb64-186" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertTrue(np.array_equal(t2.realize_data(), np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], dtype<span class="op">=</span>np.float64)))</span>
<span id="cb64-187"><a href="#cb64-187" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertTrue(t2.requires_grad)</span>
<span id="cb64-188"><a href="#cb64-188" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-189"><a href="#cb64-189" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> test_create_tensor_from_tensor_with_same_device_and_dtype_and_requires_grad_true(<span class="va">self</span>):</span>
<span id="cb64-190"><a href="#cb64-190" aria-hidden="true" tabindex="-1"></a>        t1 <span class="op">=</span> Tensor([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], device<span class="op">=</span><span class="st">'cpu'</span>, dtype<span class="op">=</span>np.float32, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb64-191"><a href="#cb64-191" aria-hidden="true" tabindex="-1"></a>        t2 <span class="op">=</span> Tensor(t1, device<span class="op">=</span><span class="st">'cpu'</span>, dtype<span class="op">=</span>np.float32, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb64-192"><a href="#cb64-192" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t2.device, <span class="st">'cpu'</span>)</span>
<span id="cb64-193"><a href="#cb64-193" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t2.dtype, np.float32)</span>
<span id="cb64-194"><a href="#cb64-194" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertTrue(np.array_equal(t2.realize_data(), np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], dtype<span class="op">=</span>np.float32)))</span>
<span id="cb64-195"><a href="#cb64-195" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertTrue(t2.requires_grad)</span>
<span id="cb64-196"><a href="#cb64-196" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-197"><a href="#cb64-197" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> test_create_tensor_from_tensor_with_same_device_and_dtype_and_requires_grad_false(<span class="va">self</span>):</span>
<span id="cb64-198"><a href="#cb64-198" aria-hidden="true" tabindex="-1"></a>        t1 <span class="op">=</span> Tensor([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], device<span class="op">=</span><span class="st">'cpu'</span>, dtype<span class="op">=</span>np.float32, requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb64-199"><a href="#cb64-199" aria-hidden="true" tabindex="-1"></a>        t2 <span class="op">=</span> Tensor(t1, device<span class="op">=</span><span class="st">'cpu'</span>, dtype<span class="op">=</span>np.float32, requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb64-200"><a href="#cb64-200" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t2.device, <span class="st">'cpu'</span>)</span>
<span id="cb64-201"><a href="#cb64-201" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t2.dtype, np.float32)</span>
<span id="cb64-202"><a href="#cb64-202" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertTrue(np.array_equal(t2.realize_data(), np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], dtype<span class="op">=</span>np.float32)))</span>
<span id="cb64-203"><a href="#cb64-203" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertFalse(t2.requires_grad)</span>
<span id="cb64-204"><a href="#cb64-204" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-205"><a href="#cb64-205" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> test_create_tensor_from_tensor_with_same_device_and_dtype_and_requires_grad_true_false(<span class="va">self</span>):</span>
<span id="cb64-206"><a href="#cb64-206" aria-hidden="true" tabindex="-1"></a>        t1 <span class="op">=</span> Tensor([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], device<span class="op">=</span><span class="st">'cpu'</span>, dtype<span class="op">=</span>np.float32, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb64-207"><a href="#cb64-207" aria-hidden="true" tabindex="-1"></a>        t2 <span class="op">=</span> Tensor(t1, device<span class="op">=</span><span class="st">'cpu'</span>, dtype<span class="op">=</span>np.float32, requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb64-208"><a href="#cb64-208" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t2.device, <span class="st">'cpu'</span>)</span>
<span id="cb64-209"><a href="#cb64-209" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t2.dtype, np.float32)</span>
<span id="cb64-210"><a href="#cb64-210" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertTrue(np.array_equal(t2.realize_data(), np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], dtype<span class="op">=</span>np.float32)))</span>
<span id="cb64-211"><a href="#cb64-211" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertFalse(t2.requires_grad)</span>
<span id="cb64-212"><a href="#cb64-212" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-213"><a href="#cb64-213" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> test_create_tensor_from_tensor_with_same_device_and_dtype_and_requires_grad_false_true(<span class="va">self</span>):</span>
<span id="cb64-214"><a href="#cb64-214" aria-hidden="true" tabindex="-1"></a>        t1 <span class="op">=</span> Tensor([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], device<span class="op">=</span><span class="st">'cpu'</span>, dtype<span class="op">=</span>np.float32, requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb64-215"><a href="#cb64-215" aria-hidden="true" tabindex="-1"></a>        t2 <span class="op">=</span> Tensor(t1, device<span class="op">=</span><span class="st">'cpu'</span>, dtype<span class="op">=</span>np.float32, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb64-216"><a href="#cb64-216" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t2.device, <span class="st">'cpu'</span>)</span>
<span id="cb64-217"><a href="#cb64-217" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t2.dtype, np.float32)</span>
<span id="cb64-218"><a href="#cb64-218" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertTrue(np.array_equal(t2.realize_data(), np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], dtype<span class="op">=</span>np.float32)))</span>
<span id="cb64-219"><a href="#cb64-219" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertTrue(t2.requires_grad)</span>
<span id="cb64-220"><a href="#cb64-220" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-221"><a href="#cb64-221" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> test_create_tensor_from_tensor_with_different_device_and_dtype_and_requires_grad_true(<span class="va">self</span>):</span>
<span id="cb64-222"><a href="#cb64-222" aria-hidden="true" tabindex="-1"></a>        t1 <span class="op">=</span> Tensor([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], device<span class="op">=</span><span class="st">'cpu'</span>, dtype<span class="op">=</span>np.float32, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb64-223"><a href="#cb64-223" aria-hidden="true" tabindex="-1"></a>        t2 <span class="op">=</span> Tensor(t1, device<span class="op">=</span><span class="st">'cuda'</span>, dtype<span class="op">=</span>np.float64, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb64-224"><a href="#cb64-224" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t2.device, <span class="st">'cuda'</span>)</span>
<span id="cb64-225"><a href="#cb64-225" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t2.dtype, np.float64)</span>
<span id="cb64-226"><a href="#cb64-226" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertTrue(np.array_equal(t2.realize_data(), np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], dtype<span class="op">=</span>np.float64)))</span>
<span id="cb64-227"><a href="#cb64-227" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertTrue(t2.requires_grad)</span>
<span id="cb64-228"><a href="#cb64-228" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-229"><a href="#cb64-229" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> test_create_tensor_from_tensor_with_different_device_and_dtype_and_requires_grad_false(<span class="va">self</span>):</span>
<span id="cb64-230"><a href="#cb64-230" aria-hidden="true" tabindex="-1"></a>        t1 <span class="op">=</span> Tensor([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], device<span class="op">=</span><span class="st">'cpu'</span>, dtype<span class="op">=</span>np.float32, requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb64-231"><a href="#cb64-231" aria-hidden="true" tabindex="-1"></a>        t2 <span class="op">=</span> Tensor(t1, device<span class="op">=</span><span class="st">'cuda'</span>, dtype<span class="op">=</span>np.float64, requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb64-232"><a href="#cb64-232" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t2.device, <span class="st">'cuda'</span>)</span>
<span id="cb64-233"><a href="#cb64-233" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t2.dtype, np.float64)</span>
<span id="cb64-234"><a href="#cb64-234" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertTrue(np.array_equal(t2.realize_data(), np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], dtype<span class="op">=</span>np.float64)))</span>
<span id="cb64-235"><a href="#cb64-235" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertFalse(t2.requires_grad)</span>
<span id="cb64-236"><a href="#cb64-236" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-237"><a href="#cb64-237" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> test_create_tensor_from_tensor_with_different_device_and_dtype_and_requires_grad_true_false(<span class="va">self</span>):</span>
<span id="cb64-238"><a href="#cb64-238" aria-hidden="true" tabindex="-1"></a>        t1 <span class="op">=</span> Tensor([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], device<span class="op">=</span><span class="st">'cpu'</span>, dtype<span class="op">=</span>np.float32, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb64-239"><a href="#cb64-239" aria-hidden="true" tabindex="-1"></a>        t2 <span class="op">=</span> Tensor(t1, device<span class="op">=</span><span class="st">'cuda'</span>, dtype<span class="op">=</span>np.float64, requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb64-240"><a href="#cb64-240" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t2.device, <span class="st">'cuda'</span>)</span>
<span id="cb64-241"><a href="#cb64-241" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t2.dtype, np.float64)</span>
<span id="cb64-242"><a href="#cb64-242" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertTrue(np.array_equal(t2.realize_data(), np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], dtype<span class="op">=</span>np.float64)))</span>
<span id="cb64-243"><a href="#cb64-243" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertFalse(t2.requires_grad)</span>
<span id="cb64-244"><a href="#cb64-244" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-245"><a href="#cb64-245" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> test_create_tensor_from_tensor_with_different_device_and_dtype_and_requires_grad_false_true(<span class="va">self</span>):</span>
<span id="cb64-246"><a href="#cb64-246" aria-hidden="true" tabindex="-1"></a>        t1 <span class="op">=</span> Tensor([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], device<span class="op">=</span><span class="st">'cpu'</span>, dtype<span class="op">=</span>np.float32, requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb64-247"><a href="#cb64-247" aria-hidden="true" tabindex="-1"></a>        t2 <span class="op">=</span> Tensor(t1, device<span class="op">=</span><span class="st">'cuda'</span>, dtype<span class="op">=</span>np.float64, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb64-248"><a href="#cb64-248" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t2.device, <span class="st">'cuda'</span>)</span>
<span id="cb64-249"><a href="#cb64-249" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(t2.dtype, np.float64)</span>
<span id="cb64-250"><a href="#cb64-250" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertTrue(np.array_equal(t2.realize_data(), np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], dtype<span class="op">=</span>np.float64)))</span>
<span id="cb64-251"><a href="#cb64-251" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertTrue(t2.requires_grad)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nbdev<span class="op">;</span> nbdev.nbdev_export()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>