<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="Fill in a module description here">

<title>minima - operators</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="styles.css">
<meta property="og:title" content="minima - operators">
<meta property="og:description" content="Fill in a module description here">
<meta property="og:site-name" content="minima">
<meta name="twitter:title" content="minima - operators">
<meta name="twitter:description" content="Fill in a module description here">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">minima</span>
    </a>
  </div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">operators</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Welcome to minima</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./autograd.html" class="sidebar-item-text sidebar-link">autograd</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./operators.html" class="sidebar-item-text sidebar-link active">operators</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#element-wise-addition" id="toc-element-wise-addition" class="nav-link active" data-scroll-target="#element-wise-addition">Element Wise Addition</a>
  <ul class="collapse">
  <li><a href="#add" id="toc-add" class="nav-link" data-scroll-target="#add">add</a></li>
  <li><a href="#ewiseadd" id="toc-ewiseadd" class="nav-link" data-scroll-target="#ewiseadd">EWiseAdd</a></li>
  </ul></li>
  <li><a href="#scalar-addition" id="toc-scalar-addition" class="nav-link" data-scroll-target="#scalar-addition">Scalar Addition</a>
  <ul class="collapse">
  <li><a href="#add_scalar" id="toc-add_scalar" class="nav-link" data-scroll-target="#add_scalar">add_scalar</a></li>
  <li><a href="#addscalar" id="toc-addscalar" class="nav-link" data-scroll-target="#addscalar">AddScalar</a></li>
  </ul></li>
  <li><a href="#element-wise-multiplication" id="toc-element-wise-multiplication" class="nav-link" data-scroll-target="#element-wise-multiplication">Element Wise Multiplication</a>
  <ul class="collapse">
  <li><a href="#multiply" id="toc-multiply" class="nav-link" data-scroll-target="#multiply">multiply</a></li>
  <li><a href="#ewisemul" id="toc-ewisemul" class="nav-link" data-scroll-target="#ewisemul">EWiseMul</a></li>
  </ul></li>
  <li><a href="#scalar-multiplication" id="toc-scalar-multiplication" class="nav-link" data-scroll-target="#scalar-multiplication">Scalar Multiplication</a>
  <ul class="collapse">
  <li><a href="#mul_scalar" id="toc-mul_scalar" class="nav-link" data-scroll-target="#mul_scalar">mul_scalar</a></li>
  <li><a href="#mulscalar" id="toc-mulscalar" class="nav-link" data-scroll-target="#mulscalar">MulScalar</a></li>
  </ul></li>
  <li><a href="#negation" id="toc-negation" class="nav-link" data-scroll-target="#negation">Negation</a></li>
  <li><a href="#exp" id="toc-exp" class="nav-link" data-scroll-target="#exp">Exp</a></li>
  <li><a href="#relu" id="toc-relu" class="nav-link" data-scroll-target="#relu">ReLU</a></li>
  <li><a href="#power-scalar" id="toc-power-scalar" class="nav-link" data-scroll-target="#power-scalar">Power Scalar</a></li>
  <li><a href="#element-wise-divide" id="toc-element-wise-divide" class="nav-link" data-scroll-target="#element-wise-divide">Element Wise Divide</a></li>
  <li><a href="#divide-scalar" id="toc-divide-scalar" class="nav-link" data-scroll-target="#divide-scalar">Divide Scalar</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/m0saan/minima/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block">operators</h1>
</div>

<div>
  <div class="description">
    Fill in a module description here
  </div>
</div>


<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->
<p>The <code>out_grad</code> parameter refers to the gradient of the loss function with respect to the output of the node. Multiplying this with the local gradient gives the gradient of the loss with respect to the input to the node, according to the chain rule of calculus, which is the basis for backpropagation in neural networks.</p>
<p>The chain rule is a fundamental concept in calculus that provides a method to compute the derivative of composite functions. In simple terms, the chain rule states that the derivative of a composite function is the derivative of the outer function multiplied by the derivative of the inner function.</p>
<p>Given a composite function that is the composition of two functions, say, <span class="math inline">\(f(g(x))\)</span>, the chain rule can be stated as follows:</p>
<p><span class="math display">\[\frac{df}{dx} = \frac{df}{dg} \cdot \frac{dg}{dx}\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(\frac{df}{dx}\)</span> is the derivative of the composite function <span class="math inline">\(f(g(x))\)</span> with respect to <span class="math inline">\(x\)</span>,</li>
<li><span class="math inline">\(\frac{df}{dg}\)</span> is the derivative of the outer function <span class="math inline">\(f\)</span> with respect to its argument <span class="math inline">\(g(x)\)</span>, and</li>
<li><span class="math inline">\(\frac{dg}{dx}\)</span> is the derivative of the inner function <span class="math inline">\(g(x)\)</span> with respect to <span class="math inline">\(x\)</span>.</li>
</ul>
<p>The chain rule can be extended to the case where we have more than two composite functions.</p>
<section id="element-wise-addition" class="level2">
<h2 class="anchored" data-anchor-id="element-wise-addition">Element Wise Addition</h2>
<p>Let’s walk through the step-by-step derivative calculation for the <a href="https://m0saan.github.io/minima/operators.html#ewiseadd"><code>EWiseAdd</code></a> operation:</p>
<p>We have the function <code>f(a, b) = a + b</code>, where <code>a</code> and <code>b</code> are tensors. Our goal is to compute the partial derivatives with respect to <code>a</code> and <code>b</code>.</p>
<p>Let’s start by calculating the derivative of <code>f</code> with respect to <code>a</code>, denoted as <code>df/da</code>:</p>
<p>Step 1: Compute the derivative of <code>f</code> with respect to <code>a</code>.</p>
<p><span class="math inline">\(\frac{{\partial f}}{{\partial a}} = \frac{{\partial}}{{\partial a}} (a + b)\)</span></p>
<p>Since <code>a</code> is the variable we are differentiating with respect to, the derivative of <code>a</code> with respect to itself is 1:</p>
<p><span class="math display">\[\frac{{\partial f}}{{\partial a}} = 1\]</span></p>
<p>Therefore, <span class="math display">\[\frac{{\partial f}}{{\partial a}} = 1.\]</span></p>
<p>Step 2: Compute the derivative of <code>f</code> with respect to <code>b</code>.</p>
<p><span class="math display">\[\frac{{\partial f}}{{\partial b}} = \frac{{\partial}}{{\partial b}} (a + b)\]</span></p>
<p>Again, since <code>b</code> is the variable we are differentiating with respect to, the derivative of <code>b</code> with respect to itself is 1:</p>
<p><span class="math display">\[\frac{{\partial f}}{{\partial b}} = 1\]</span></p>
<p>Therefore, <span class="math display">\[\frac{{\partial f}}{{\partial b}} = 1\]</span></p>
<p>Hence, the partial derivatives of <code>f(a, b) = a + b</code> with respect to <code>a</code> and <code>b</code> are both equal to 1.</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/operators.py#L61" target="_blank" style="float:right; font-size:smaller">source</a></p>
<section id="add" class="level3">
<h3 class="anchored" data-anchor-id="add">add</h3>
<blockquote class="blockquote">
<pre><code> add (a:minima.autograd.Tensor, b:minima.autograd.Tensor)</code></pre>
</blockquote>
<p>Adds two tensors element-wise.</p>
<p>Args: - a: The first tensor. - b: The second tensor.</p>
<p>Returns: The element-wise sum of a and b.</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/operators.py#L22" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="ewiseadd" class="level3">
<h3 class="anchored" data-anchor-id="ewiseadd">EWiseAdd</h3>
<blockquote class="blockquote">
<pre><code> EWiseAdd ()</code></pre>
</blockquote>
<p>Performs element-wise addition of two tensors.</p>
<p>Example: &gt;&gt;&gt; a = Tensor([1, 2, 3]) &gt;&gt;&gt; b = Tensor([4, 5, 6]) &gt;&gt;&gt; op = EWiseAdd() &gt;&gt;&gt; result = op.compute(a, b) &gt;&gt;&gt; print(result) Tensor([5, 7, 9])</p>
<div class="cell" data-tags="[]" data-execution_count="4">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create two 1-D tensors</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> Tensor([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>])</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> Tensor([<span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>])</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Create an EWiseAdd operation</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>op <span class="op">=</span> EWiseAdd()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="scalar-addition" class="level2">
<h2 class="anchored" data-anchor-id="scalar-addition">Scalar Addition</h2>
<p>Explanation for the derivative of the <a href="https://m0saan.github.io/minima/operators.html#addscalar"><code>AddScalar</code></a> operator:</p>
<p>Let’s denote the scalar as <code>c</code> and <code>a</code> as the tensor being added by the scalar. The operation can be described as <code>f(a) = a + c</code>.</p>
<p>The function for the backward pass (i.e., the gradient) is <code>df/da = 1</code>, which means the derivative of <code>f(a)</code> with respect to <code>a</code> is simply <code>1</code>.</p>
<p>We are given a function <span class="math inline">\(f(a) = a + c\)</span>, where <span class="math inline">\(a\)</span> is a tensor and <span class="math inline">\(c\)</span> is a scalar. Our task is to find the derivative of this function with respect to <span class="math inline">\(a\)</span>.</p>
<p>By differentiating the function <span class="math inline">\(f(a)\)</span> with respect to <span class="math inline">\(a\)</span>, we find:</p>
<p><span class="math display">\[\begin{align*}
\frac{df}{da} &amp;= \frac{d}{da} (a + c) \\
&amp;= 1
\end{align*}\]</span></p>
<p>Therefore, the gradient of <span class="math inline">\(f(a)\)</span> with respect to <span class="math inline">\(a\)</span> is <span class="math inline">\(1\)</span>.</p>
<p>We starts by defining the function <code>f(a) = a + c</code>. It then explains that when we differentiate <code>f(a)</code> with respect to <code>a</code>, we find that the derivative is <code>1</code>. This means that the gradient of <code>f(a)</code> with respect to <code>a</code> is <code>1</code>, which matches the behavior of the <a href="https://m0saan.github.io/minima/operators.html#addscalar"><code>AddScalar</code></a> operator as provided in the <code>gradient</code> method.</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/operators.py#L120" target="_blank" style="float:right; font-size:smaller">source</a></p>
<section id="add_scalar" class="level3">
<h3 class="anchored" data-anchor-id="add_scalar">add_scalar</h3>
<blockquote class="blockquote">
<pre><code> add_scalar (a:minima.autograd.Tensor, scalar:Union[int,float])</code></pre>
</blockquote>
<p>Adds a scalar to a tensor.</p>
<p>Args: - a: The tensor. - scalar: The scalar to add.</p>
<p>Returns: The sum of a and the scalar.</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/operators.py#L75" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="addscalar" class="level3">
<h3 class="anchored" data-anchor-id="addscalar">AddScalar</h3>
<blockquote class="blockquote">
<pre><code> AddScalar (scalar:Union[int,float])</code></pre>
</blockquote>
<p>Performs addition of a tensor and a scalar.</p>
<p>Example: &gt;&gt;&gt; a = Tensor([1, 2, 3]) &gt;&gt;&gt; op = AddScalar(5) &gt;&gt;&gt; result = op.compute(a) &gt;&gt;&gt; print(result) Tensor([6, 7, 8])</p>
</section>
</section>
<section id="element-wise-multiplication" class="level2">
<h2 class="anchored" data-anchor-id="element-wise-multiplication">Element Wise Multiplication</h2>
<p>Explanation for the derivative of the <a href="https://m0saan.github.io/minima/operators.html#ewisemul"><code>EWiseMul</code></a> (element-wise multiplication) operator:</p>
<p>Let’s denote the two input tensors as <code>a</code> and <code>b</code>. The operation can be described as <code>f(a, b) = a * b</code>, where <code>*</code> represents element-wise multiplication.</p>
<p>The function for the backward pass (i.e., the gradient) is <code>df/da = b</code> and <code>df/db = a</code>. This means that the derivative of <code>f(a, b)</code> with respect to <code>a</code> is <code>b</code>, and the derivative with respect to <code>b</code> is <code>a</code>.</p>
<p>We are given a function <span class="math inline">\(f(a, b) = a \odot b\)</span>, where <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are tensors, and <span class="math inline">\(\odot\)</span> represents element-wise multiplication. Our task is to find the derivatives of this function with respect to <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>.</p>
<p>By differentiating the function <span class="math inline">\(f(a, b)\)</span> with respect to <span class="math inline">\(a\)</span>, we find:</p>
<p><span class="math display">\[\begin{align*}
\frac{df}{da} &amp;= \frac{d}{da} (a \odot b) \\
&amp;= b
\end{align*}\]</span></p>
<p>Therefore, the gradient of <span class="math inline">\(f(a, b)\)</span> with respect to <span class="math inline">\(a\)</span> is <span class="math inline">\(b\)</span>.</p>
<p>Similarly, by differentiating the function <span class="math inline">\(f(a, b)\)</span> with respect to <span class="math inline">\(b\)</span>, we find:</p>
<p><span class="math display">\[\begin{align*}
\frac{df}{db} &amp;= \frac{d}{db} (a \odot b) \\
&amp;= a
\end{align*}\]</span></p>
<p>Therefore, the gradient of <span class="math inline">\(f(a, b)\)</span> with respect to <span class="math inline">\(b\)</span> is <span class="math inline">\(a\)</span>.</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/operators.py#L173" target="_blank" style="float:right; font-size:smaller">source</a></p>
<section id="multiply" class="level3">
<h3 class="anchored" data-anchor-id="multiply">multiply</h3>
<blockquote class="blockquote">
<pre><code> multiply (a:minima.autograd.Tensor, b:minima.autograd.Tensor)</code></pre>
</blockquote>
<p>Multiplies two tensors element-wise.</p>
<p>Args: - a: The first tensor. - b: The second tensor.</p>
<p>Returns: The element-wise product of a and b.</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/operators.py#L134" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="ewisemul" class="level3">
<h3 class="anchored" data-anchor-id="ewisemul">EWiseMul</h3>
<blockquote class="blockquote">
<pre><code> EWiseMul ()</code></pre>
</blockquote>
<p>Performs element-wise multiplication of two tensors.</p>
<p>Example: &gt;&gt;&gt; a = Tensor([1, 2, 3]) &gt;&gt;&gt; b = Tensor([4, 5, 6]) &gt;&gt;&gt; op = EWiseMul() &gt;&gt;&gt; result = op.compute(a, b) &gt;&gt;&gt; print(result) Tensor([4, 10, 18])</p>
</section>
</section>
<section id="scalar-multiplication" class="level2">
<h2 class="anchored" data-anchor-id="scalar-multiplication">Scalar Multiplication</h2>
<p>Let’s denote the scalar as <code>c</code> and <code>a</code> as the tensor being multiplied by the scalar. The operation can be described as <code>f(a) = a * c</code>.</p>
<p>The function for the backward pass (i.e., the gradient) is <code>df/da = c</code>, which means the derivative of <code>f(a)</code> with respect to <code>a</code> is <code>c</code>.</p>
<p>The LaTeX document will look as follows:</p>
<p>We are given a function <span class="math inline">\(f(a) = a \cdot c\)</span>, where <span class="math inline">\(a\)</span> is a tensor and <span class="math inline">\(c\)</span> is a scalar. Our task is to find the derivative of this function with respect to <span class="math inline">\(a\)</span>.</p>
<p>By differentiating the function <span class="math inline">\(f(a)\)</span> with respect to <span class="math inline">\(a\)</span>, we find:</p>
<p><span class="math display">\[\begin{align*}
\frac{df}{da} &amp;= \frac{d}{da} (a \cdot c) \\
&amp;= c
\end{align*}\]</span></p>
<p>Therefore, the gradient of <span class="math inline">\(f(a)\)</span> with respect to <span class="math inline">\(a\)</span> is <span class="math inline">\(c\)</span>.</p>
<p>We starts by defining the function <code>f(a) = a * c</code>. It then explains that when we differentiate <code>f(a)</code> with respect to <code>a</code>, we find that the derivative is <code>c</code>. This means that the gradient of <code>f(a)</code> with respect to <code>a</code> is <code>c</code>, which matches the behavior of the <a href="https://m0saan.github.io/minima/operators.html#mulscalar"><code>MulScalar</code></a> operator as provided in the <code>gradient</code> method.</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/operators.py#L232" target="_blank" style="float:right; font-size:smaller">source</a></p>
<section id="mul_scalar" class="level3">
<h3 class="anchored" data-anchor-id="mul_scalar">mul_scalar</h3>
<blockquote class="blockquote">
<pre><code> mul_scalar (a:minima.autograd.Tensor, scalar:Union[int,float])</code></pre>
</blockquote>
<p>Multiplies a tensor by a scalar.</p>
<p>Args: - a: The tensor. - scalar: The scalar to multiply.</p>
<p>Returns: The product of a and the scalar.</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/operators.py#L187" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="mulscalar" class="level3">
<h3 class="anchored" data-anchor-id="mulscalar">MulScalar</h3>
<blockquote class="blockquote">
<pre><code> MulScalar (scalar:Union[int,float])</code></pre>
</blockquote>
<p>Performs multiplication of a tensor and a scalar.</p>
<p>Example: &gt;&gt;&gt; a = Tensor([1, 2, 3]) &gt;&gt;&gt; op = MulScalar(5) &gt;&gt;&gt; result = op.compute(a) &gt;&gt;&gt; print(result) Tensor([5, 10, 15])</p>
</section>
</section>
<section id="negation" class="level2">
<h2 class="anchored" data-anchor-id="negation">Negation</h2>
<p>Let’s denote <code>a</code> as the tensor being negated. The operation can be described as <code>f(a) = -a</code>.</p>
<p>The function for the backward pass (i.e., the gradient) is <code>df/da = -1</code>.</p>
<p>We are given a function <span class="math inline">\(f(a) = -a\)</span>, where <span class="math inline">\(a\)</span> is a tensor. Our task is to find the derivative of this function with respect to <span class="math inline">\(a\)</span>.</p>
<p>By differentiating the function <span class="math inline">\(f(a)\)</span> with respect to <span class="math inline">\(a\)</span>, we find:</p>
<p><span class="math display">\[\begin{align*}
\frac{df}{da} &amp;= \frac{d}{da} (-a) \\
&amp;= -1
\end{align*}\]</span></p>
<p>Therefore, the gradient of <span class="math inline">\(f(a)\)</span> with respect to <span class="math inline">\(a\)</span> is <span class="math inline">\(-1\)</span>.</p>
<div class="cell" data-tags="[]" data-execution_count="8">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Negate(TensorOp):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Negates the given tensor.</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Example:</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="co">    &gt;&gt;&gt; a = Tensor([1, -2, 3])</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="co">    &gt;&gt;&gt; op = Negate()</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="co">    &gt;&gt;&gt; result = op.compute(a)</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="co">    &gt;&gt;&gt; print(result)</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Tensor([-1, 2, -3])</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> compute(<span class="va">self</span>, a: NDArray) <span class="op">-&gt;</span> NDArray:</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a><span class="co">        Computes the negation of a tensor.</span></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a><span class="co">        - a: The tensor to negate.</span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a><span class="co">        The negation of a.</span></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="op">-</span><span class="dv">1</span> <span class="op">*</span> a</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> gradient(<span class="va">self</span>, out_grad: Tensor, node: Tensor) <span class="op">-&gt;</span> Tuple[Tensor,]:</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a><span class="co">        Computes the gradient of the negation operation.</span></span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a><span class="co">        - out_grad: The gradient of the output of the operation.</span></span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a><span class="co">        - node: The node in the computational graph where the operation was performed.</span></span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a><span class="co">        The gradients with respect to the inputs.</span></span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (negate(out_grad), )</span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> negate(a: Tensor) <span class="op">-&gt;</span> Tensor:</span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a><span class="co">    Negates the given tensor.</span></span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a><span class="co">    - a: The tensor to negate.</span></span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a><span class="co">    The negation of a.</span></span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a><span class="co">    Example:</span></span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a><span class="co">    &gt;&gt;&gt; a = Tensor([1, -2, 3])</span></span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a><span class="co">    &gt;&gt;&gt; result = negate(a)</span></span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a><span class="co">    &gt;&gt;&gt; print(result)</span></span>
<span id="cb10-53"><a href="#cb10-53" aria-hidden="true" tabindex="-1"></a><span class="co">    Tensor([-1, 2, -3])</span></span>
<span id="cb10-54"><a href="#cb10-54" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb10-55"><a href="#cb10-55" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Negate()(a)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="exp" class="level2">
<h2 class="anchored" data-anchor-id="exp">Exp</h2>
<p>Explanation for the derivative of the <code>Exp</code> operator:</p>
<p>Let’s denote <code>a</code> as the tensor on which the exponential function is applied. The operation can be described as <code>f(a) = exp(a)</code>, where <code>exp</code> represents the exponential function.</p>
<p>The function for the backward pass (i.e., the gradient) is <code>df/da = exp(a)</code>.</p>
<p>We are given a function <span class="math inline">\(f(a) = \exp(a)\)</span>, where <span class="math inline">\(a\)</span> is a tensor. Our task is to find the derivative of this function with respect to <span class="math inline">\(a\)</span>.</p>
<p>By differentiating the function <span class="math inline">\(f(a)\)</span> with respect to <span class="math inline">\(a\)</span>, we find:</p>
<p><span class="math display">\[\begin{align*}
\frac{df}{da} &amp;= \frac{d}{da} (\exp(a)) \\
&amp;= \exp(a)
\end{align*}\]</span></p>
<p>Therefore, the gradient of <span class="math inline">\(f(a)\)</span> with respect to <span class="math inline">\(a\)</span> is <span class="math inline">\(\exp(a)\)</span>.</p>
<div class="cell" data-tags="[]" data-execution_count="9">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Exp(TensorOp):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Calculates the exponential of the given tensor.</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Example:</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="co">    &gt;&gt;&gt; a = Tensor([1, 2, 3])</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="co">    &gt;&gt;&gt; op = Exp()</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="co">    &gt;&gt;&gt; result = op.compute(a)</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="co">    &gt;&gt;&gt; print(result)</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Tensor([2.71828183, 7.3890561, 20.08553692])</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> compute(<span class="va">self</span>, a: NDArray) <span class="op">-&gt;</span> NDArray:</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a><span class="co">        Computes the exponential of a tensor.</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a><span class="co">        - a: The tensor.</span></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a><span class="co">        The exponential of a.</span></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.out <span class="op">=</span> array_api.exp(a)</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.out</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> gradient(<span class="va">self</span>, out_grad: Tensor, node: Tensor) <span class="op">-&gt;</span> Tuple[Tensor,]:</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a><span class="co">        Computes the gradient of the exponential operation.</span></span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a><span class="co">        - out_grad: The gradient of the output of the operation.</span></span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a><span class="co">        - node: The node in the computational graph where the operation was performed.</span></span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a><span class="co">        The gradients with respect to the inputs.</span></span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (out_grad <span class="op">*</span> <span class="va">self</span>.out, )</span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> exp(a: Tensor) <span class="op">-&gt;</span> Tensor:</span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a><span class="co">    Calculates the exponential of the given tensor.</span></span>
<span id="cb11-42"><a href="#cb11-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-43"><a href="#cb11-43" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb11-44"><a href="#cb11-44" aria-hidden="true" tabindex="-1"></a><span class="co">    - a: The tensor.</span></span>
<span id="cb11-45"><a href="#cb11-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-46"><a href="#cb11-46" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb11-47"><a href="#cb11-47" aria-hidden="true" tabindex="-1"></a><span class="co">    The exponential of a.</span></span>
<span id="cb11-48"><a href="#cb11-48" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb11-49"><a href="#cb11-49" aria-hidden="true" tabindex="-1"></a><span class="co">    Example:</span></span>
<span id="cb11-50"><a href="#cb11-50" aria-hidden="true" tabindex="-1"></a><span class="co">    &gt;&gt;&gt; a = Tensor([1, 2, 3])</span></span>
<span id="cb11-51"><a href="#cb11-51" aria-hidden="true" tabindex="-1"></a><span class="co">    &gt;&gt;&gt; result = exp(a)</span></span>
<span id="cb11-52"><a href="#cb11-52" aria-hidden="true" tabindex="-1"></a><span class="co">    &gt;&gt;&gt; print(result)</span></span>
<span id="cb11-53"><a href="#cb11-53" aria-hidden="true" tabindex="-1"></a><span class="co">    Tensor([2.71828183, 7.3890561, 20.08553692])</span></span>
<span id="cb11-54"><a href="#cb11-54" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb11-55"><a href="#cb11-55" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Exp()(a)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="relu" class="level2">
<h2 class="anchored" data-anchor-id="relu">ReLU</h2>
<p>The derivative of the <code>ReLU</code> (Rectified Linear Unit) operator:</p>
<p>Let’s denote <code>a</code> as the tensor on which the ReLU function is applied. The ReLU function is defined as follows:</p>
<p><span class="math display">\[
f(a) =
\begin{cases}
a, &amp; \text{if } a \geq 0 \\
0, &amp; \text{if } a &lt; 0
\end{cases}
\]</span></p>
<p>The function for the backward pass (i.e., the gradient) is <code>df/da = 1</code> if <code>a &gt;= 0</code>, and <code>df/da = 0</code> if <code>a &lt; 0</code>.</p>
<p>We are given a function <span class="math inline">\(f(a) = \max(0, a)\)</span>, where <span class="math inline">\(a\)</span> is a tensor. Our task is to find the derivative of this function with respect to <span class="math inline">\(a\)</span>.</p>
<p>By considering the definition of the ReLU function, we can write <span class="math inline">\(f(a)\)</span> as:</p>
<p><span class="math display">\[
f(a) =
\begin{cases}
a, &amp; \text{if } a \geq 0 \\
0, &amp; \text{if } a &lt; 0
\end{cases}
\]</span></p>
<p>Now, let’s differentiate <span class="math inline">\(f(a)\)</span> with respect to <span class="math inline">\(a\)</span>:</p>
<p><span class="math display">\[
\frac{df}{da} =
\begin{cases}
1, &amp; \text{if } a \geq 0 \\
0, &amp; \text{if } a &lt; 0
\end{cases}
\]</span></p>
<p>Therefore, the gradient of <span class="math inline">\(f(a)\)</span> with respect to <span class="math inline">\(a\)</span> is <span class="math inline">\(1\)</span> if <span class="math inline">\(a \geq 0\)</span>, and <span class="math inline">\(0\)</span> if <span class="math inline">\(a &lt; 0\)</span>.</p>
<div class="cell" data-tags="[]" data-execution_count="10">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ReLU(TensorOp):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Applies the ReLU (Rectified Linear Unit) activation function to the given tensor.</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Example:</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="co">    &gt;&gt;&gt; a = Tensor([1, -2, 3])</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="co">    &gt;&gt;&gt; op = ReLU()</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="co">    &gt;&gt;&gt; result = op.compute(a)</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="co">    &gt;&gt;&gt; print(result)</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Tensor([1, 0, 3])</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> compute(<span class="va">self</span>, a: NDArray) <span class="op">-&gt;</span> NDArray:</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a><span class="co">        Computes the ReLU activation function on a tensor.</span></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a><span class="co">        - a: The tensor.</span></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a><span class="co">        The result of applying ReLU to a.</span></span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.out <span class="op">=</span> array_api.clip(a, a_min<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.out</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> gradient(<span class="va">self</span>, out_grad: Tensor, node: Tensor) <span class="op">-&gt;</span> Tuple[Tensor,]:</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a><span class="co">        Computes the gradient of the ReLU operation.</span></span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a><span class="co">        - out_grad: The gradient of the output of the operation.</span></span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a><span class="co">        - node: The node in the computational graph where the operation was performed.</span></span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a><span class="co">        The gradients with respect to the inputs.</span></span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (out_grad <span class="op">*</span> Tensor(node.children[<span class="dv">0</span>] <span class="op">&gt;=</span> <span class="dv">0</span>), )</span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> relu(a: Tensor) <span class="op">-&gt;</span> Tensor:</span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a><span class="co">    Applies the ReLU (Rectified Linear Unit) activation function to the given tensor.</span></span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a><span class="co">    - a: The tensor.</span></span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-46"><a href="#cb12-46" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb12-47"><a href="#cb12-47" aria-hidden="true" tabindex="-1"></a><span class="co">    The result of applying ReLU to a.</span></span>
<span id="cb12-48"><a href="#cb12-48" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb12-49"><a href="#cb12-49" aria-hidden="true" tabindex="-1"></a><span class="co">    Example:</span></span>
<span id="cb12-50"><a href="#cb12-50" aria-hidden="true" tabindex="-1"></a><span class="co">    &gt;&gt;&gt; a = Tensor([1, -2, 3])</span></span>
<span id="cb12-51"><a href="#cb12-51" aria-hidden="true" tabindex="-1"></a><span class="co">    &gt;&gt;&gt; result = relu(a)</span></span>
<span id="cb12-52"><a href="#cb12-52" aria-hidden="true" tabindex="-1"></a><span class="co">    &gt;&gt;&gt; print(result)</span></span>
<span id="cb12-53"><a href="#cb12-53" aria-hidden="true" tabindex="-1"></a><span class="co">    Tensor([1, 0, 3])</span></span>
<span id="cb12-54"><a href="#cb12-54" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb12-55"><a href="#cb12-55" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ReLU()(a)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="power-scalar" class="level2">
<h2 class="anchored" data-anchor-id="power-scalar">Power Scalar</h2>
<p>The derivative of the <code>PowerScalar</code> operator:</p>
<p>Let’s denote the scalar as <code>n</code> and <code>a</code> as the tensor being raised to the power of the scalar. The operation can be described as <code>f(a) = a^n</code>.</p>
<p>The function for the backward pass (i.e., the gradient) is <code>df/da = n * a^(n-1)</code>.</p>
<p>We are given a function <span class="math inline">\(f(a) = a^n\)</span>, where <span class="math inline">\(a\)</span> is a tensor and <span class="math inline">\(n\)</span> is a scalar. Our task is to find the derivative of this function with respect to <span class="math inline">\(a\)</span>.</p>
<p>By differentiating the function <span class="math inline">\(f(a)\)</span> with respect to <span class="math inline">\(a\)</span>, we find:</p>
<p><span class="math display">\[\begin{align*}
\frac{df}{da} &amp;= \frac{d}{da} (a^n) \\
&amp;= n \cdot a^{n-1}
\end{align*}\]</span></p>
<p>Therefore, the gradient of <span class="math inline">\(f(a)\)</span> with respect to <span class="math inline">\(a\)</span> is <span class="math inline">\(n \cdot a^{n-1}\)</span>.</p>
<div class="cell" data-tags="[]" data-execution_count="11">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PowerScalar(TensorOp):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="co">    The PowerScalar operation raises a tensor to an (integer) power.</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Attributes:</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="co">        scalar (int): The power to raise the tensor to.</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Example:</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; import numpy as np</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; tensor = Tensor(np.array([1, 2, 3]))</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; pow_scalar = PowerScalar(2)</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; result = pow_scalar.compute(tensor.data)</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; print(result)</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a><span class="co">        array([1, 4, 9])</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, scalar: <span class="bu">int</span>):</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a><span class="co">        Constructs the PowerScalar operation.</span></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a><span class="co">            scalar (int): The power to raise the tensor to.</span></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scalar <span class="op">=</span> scalar</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> compute(<span class="va">self</span>, a: NDArray) <span class="op">-&gt;</span> NDArray:</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a><span class="co">        Computes the power operation on the input tensor.</span></span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a><span class="co">            a (NDArray): The input tensor.</span></span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a><span class="co">            NDArray: The resulting tensor after the power operation.</span></span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> array_api.power(a, <span class="va">self</span>.scalar)</span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> gradient(<span class="va">self</span>, out_grad: Tensor, node: Tensor) <span class="op">-&gt;</span> Tuple[Tensor, ]:</span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a><span class="co">        Computes the gradient of the power operation.</span></span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a><span class="co">            out_grad (Tensor): The gradient of the output tensor.</span></span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a><span class="co">            node (Tensor): The node in the computational graph where the operation was performed.</span></span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-47"><a href="#cb13-47" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb13-48"><a href="#cb13-48" aria-hidden="true" tabindex="-1"></a><span class="co">            Tuple[Tensor, ]: The gradient with respect to the input tensor.</span></span>
<span id="cb13-49"><a href="#cb13-49" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb13-50"><a href="#cb13-50" aria-hidden="true" tabindex="-1"></a>        a <span class="op">=</span> node.children[<span class="dv">0</span>]</span>
<span id="cb13-51"><a href="#cb13-51" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (<span class="va">self</span>.scalar <span class="op">*</span> power_scalar(a, <span class="va">self</span>.scalar <span class="op">-</span> <span class="dv">1</span>) <span class="op">*</span> out_grad, )</span>
<span id="cb13-52"><a href="#cb13-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-53"><a href="#cb13-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-54"><a href="#cb13-54" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> power_scalar(a: Tensor, scalar: <span class="bu">int</span>) <span class="op">-&gt;</span> Tensor:</span>
<span id="cb13-55"><a href="#cb13-55" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb13-56"><a href="#cb13-56" aria-hidden="true" tabindex="-1"></a><span class="co">    Raises a tensor to a power.</span></span>
<span id="cb13-57"><a href="#cb13-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-58"><a href="#cb13-58" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb13-59"><a href="#cb13-59" aria-hidden="true" tabindex="-1"></a><span class="co">        a (Tensor): The input tensor.</span></span>
<span id="cb13-60"><a href="#cb13-60" aria-hidden="true" tabindex="-1"></a><span class="co">        scalar (int): The power to raise the tensor to.</span></span>
<span id="cb13-61"><a href="#cb13-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-62"><a href="#cb13-62" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb13-63"><a href="#cb13-63" aria-hidden="true" tabindex="-1"></a><span class="co">        Tensor: The resulting tensor after the power operation.</span></span>
<span id="cb13-64"><a href="#cb13-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-65"><a href="#cb13-65" aria-hidden="true" tabindex="-1"></a><span class="co">    Example:</span></span>
<span id="cb13-66"><a href="#cb13-66" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; import numpy as np</span></span>
<span id="cb13-67"><a href="#cb13-67" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; tensor = Tensor(np.array([1, 2, 3]))</span></span>
<span id="cb13-68"><a href="#cb13-68" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; result = power_scalar(tensor, 2)</span></span>
<span id="cb13-69"><a href="#cb13-69" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; print(result)</span></span>
<span id="cb13-70"><a href="#cb13-70" aria-hidden="true" tabindex="-1"></a><span class="co">        Tensor([1, 4, 9])</span></span>
<span id="cb13-71"><a href="#cb13-71" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb13-72"><a href="#cb13-72" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> PowerScalar(scalar)(a)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="element-wise-divide" class="level2">
<h2 class="anchored" data-anchor-id="element-wise-divide">Element Wise Divide</h2>
<p>The operation described here is an element-wise division of two tensors, <code>a</code> and <code>b</code>, where the operation can be described as <code>f(a, b) = a / b</code>.</p>
<p>We’ll compute the partial derivatives with respect to <code>a</code> and <code>b</code>:</p>
<ol type="1">
<li><p>The partial derivative of <code>f(a, b)</code> with respect to <code>a</code> (<code>df/da</code>) is <code>1/b</code>.</p></li>
<li><p>The partial derivative of <code>f(a, b)</code> with respect to <code>b</code> (<code>df/db</code>) is <code>-a / b^2</code>.</p></li>
</ol>
<p>We are given a function <span class="math inline">\(f(a, b) = \frac{a}{b}\)</span>, where <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are tensors. Our task is to find the partial derivatives of this function with respect to <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>.</p>
<p>Let’s start with <span class="math inline">\(\frac{\partial f}{\partial a}\)</span>:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial f}{\partial a} &amp;= \frac{\partial}{\partial a} \left(\frac{a}{b}\right) \\
&amp;= \frac{1}{b}
\end{align*}\]</span></p>
<p>Now, let’s compute <span class="math inline">\(\frac{\partial f}{\partial b}\)</span>:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial f}{\partial b} &amp;= \frac{\partial}{\partial b} \left(\frac{a}{b}\right) \\
&amp;= - \frac{a}{b^{2}}
\end{align*}\]</span></p>
<p>Here is a detailed derivative:</p>
<p>Given a function of the form <span class="math inline">\(y = \frac{u}{v}\)</span>, where both <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span> are functions of <span class="math inline">\(x\)</span>, the quotient rule of differentiation states:</p>
<p><span class="math display">\[\frac{dy}{dx} = \frac{v \cdot \frac{du}{dx} - u \cdot \frac{dv}{dx}}{v^2}\]</span></p>
<p>In our case, we’re looking at the function <span class="math inline">\(y = \frac{a}{b}\)</span>, where <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are tensors. We want to find the derivative with respect to <span class="math inline">\(b\)</span> (instead of <span class="math inline">\(x\)</span> in our general formula). So we have:</p>
<p><span class="math display">\[\frac{dy}{db} = \frac{b \cdot \frac{da}{db} - a \cdot \frac{db}{db}}{b^2}\]</span></p>
<p>Since <span class="math inline">\(a\)</span> does not depend on <span class="math inline">\(b\)</span>, <span class="math inline">\(\frac{da}{db} = 0\)</span>, and since any variable is equal to itself, <span class="math inline">\(\frac{db}{db} = 1\)</span>.</p>
<p>So the derivative <span class="math inline">\(\frac{dy}{db}\)</span> simplifies to:</p>
<p><span class="math display">\[\frac{dy}{db} = \frac{b \cdot 0 - a \cdot 1}{b^2}\]</span></p>
<p>Therefore, the derivative of <span class="math inline">\(y\)</span> with respect to <span class="math inline">\(b\)</span> is <span class="math inline">\(-\frac{a}{b^2}\)</span>.</p>
<p>Therefore, the gradient of <span class="math inline">\(f(a, b)\)</span> with respect to <span class="math inline">\(a\)</span> is <span class="math inline">\(\frac{1}{b}\)</span>, and the gradient of <span class="math inline">\(f(a, b)\)</span> with respect to <span class="math inline">\(b\)</span> is <span class="math inline">\(- \frac{a}{b^{2}}\)</span>.</p>
<div class="cell" data-tags="[]" data-execution_count="12">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> EWiseDiv(TensorOp):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="co">    The EWiseDiv operation divides two tensors element-wise.</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Example:</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; import numpy as np</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; a = Tensor(np.array([1, 2, 3]))</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; b = Tensor(np.array([4, 5, 6]))</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; div = EWiseDiv()</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; result = div.compute(a.data, b.data)</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; print(result)</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a><span class="co">        array([0.25, 0.4, 0.5])</span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> compute(<span class="va">self</span>, a: NDArray, b: NDArray) <span class="op">-&gt;</span> NDArray:</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a><span class="co">        Computes the element-wise division of two tensors.</span></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a><span class="co">            a (NDArray): The dividend tensor.</span></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a><span class="co">            b (NDArray): The divisor tensor.</span></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a><span class="co">            NDArray: The resulting tensor after element-wise division.</span></span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> a <span class="op">/</span> b</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> gradient(<span class="va">self</span>, out_grad: Tensor, node: Tensor) <span class="op">-&gt;</span> Tuple[Tensor, Tensor]:</span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a><span class="co">        Computes the gradient of the element-wise division operation.</span></span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a><span class="co">            out_grad (Tensor): The gradient of the output tensor.</span></span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a><span class="co">            node (Tensor): The node in the computational graph where the operation was performed.</span></span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a><span class="co">            Tuple[Tensor, Tensor]: The gradients with respect to the dividend and divisor tensors.</span></span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a>        a, b <span class="op">=</span> node.inputs</span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> divide(out_grad, b), out_grad <span class="op">*</span> negate(divide(a, power_scalar(b, <span class="dv">2</span>)))</span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> divide(a: Tensor, b: Tensor) <span class="op">-&gt;</span> Tensor:</span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a><span class="co">    Divides two tensors element-wise.</span></span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb14-49"><a href="#cb14-49" aria-hidden="true" tabindex="-1"></a><span class="co">        a (Tensor): The dividend tensor.</span></span>
<span id="cb14-50"><a href="#cb14-50" aria-hidden="true" tabindex="-1"></a><span class="co">        b (Tensor): The divisor tensor.</span></span>
<span id="cb14-51"><a href="#cb14-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-52"><a href="#cb14-52" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb14-53"><a href="#cb14-53" aria-hidden="true" tabindex="-1"></a><span class="co">        Tensor: The resulting tensor after element-wise division.</span></span>
<span id="cb14-54"><a href="#cb14-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-55"><a href="#cb14-55" aria-hidden="true" tabindex="-1"></a><span class="co">    Example:</span></span>
<span id="cb14-56"><a href="#cb14-56" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; import numpy as np</span></span>
<span id="cb14-57"><a href="#cb14-57" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; a = Tensor(np.array([1, 2, 3]))</span></span>
<span id="cb14-58"><a href="#cb14-58" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; b = Tensor(np.array([4, 5, 6]))</span></span>
<span id="cb14-59"><a href="#cb14-59" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; result = divide(a, b)</span></span>
<span id="cb14-60"><a href="#cb14-60" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; print(result)</span></span>
<span id="cb14-61"><a href="#cb14-61" aria-hidden="true" tabindex="-1"></a><span class="co">        Tensor([0.25, 0.4, 0.5])</span></span>
<span id="cb14-62"><a href="#cb14-62" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb14-63"><a href="#cb14-63" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> EWiseDiv()(a, b)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="divide-scalar" class="level2">
<h2 class="anchored" data-anchor-id="divide-scalar">Divide Scalar</h2>
<p>Let’s denote the scalar as <code>c</code>, and <code>a</code> as the tensor being divided by the scalar. The operation can be described as <code>f(a) = a / c</code>.</p>
<p>The function for the backward pass (i.e., the gradient) is <code>df/da = 1/c</code>.</p>
<p>This is the derivative of <code>f(a)</code> with respect to <code>a</code>.</p>
<p>We are given a function <span class="math inline">\(f(a) = \frac{a}{c}\)</span>, where <span class="math inline">\(a\)</span> is a tensor and <span class="math inline">\(c\)</span> is a scalar. Our task is to find the derivative of this function with respect to <span class="math inline">\(a\)</span>.</p>
<p>By using the power rule of differentiation, where the derivative of <span class="math inline">\(a^n\)</span> is <span class="math inline">\(n \cdot a^{n-1}\)</span>, we can rewrite <span class="math inline">\(f(a)\)</span> as <span class="math inline">\(f(a) = c^{-1}a\)</span>.</p>
<p>Now, we can differentiate this with respect to <span class="math inline">\(a\)</span>:</p>
<p><span class="math display">\[\begin{align*}
\frac{df}{da} &amp;= \frac{d}{da} (c^{-1}a) \\
&amp;= c^{-1} \frac{d}{da} (a) \\
&amp;= c^{-1} \\
&amp;= \frac{1}{c}
\end{align*}\]</span></p>
<p>Therefore, the gradient of <span class="math inline">\(f(a)\)</span> with respect to <span class="math inline">\(a\)</span> is <span class="math inline">\(\frac{1}{c}\)</span>.</p>
<div class="cell" data-tags="[]" data-execution_count="13">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DivScalar(TensorOp):</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="co">    The DivScalar operation divides a tensor by a scalar.</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Example:</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; import numpy as np</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; a = Tensor(np.array([1, 2, 3]))</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; scalar = 2</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; div_scalar = DivScalar(scalar)</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; result = div_scalar.compute(a.data)</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; print(result)</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="co">        array([0.5, 1.0, 1.5])</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, scalar: Union[<span class="bu">int</span>, <span class="bu">float</span>]):</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a><span class="co">        Initialize the DivScalar operation with the scalar to divide by.</span></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a><span class="co">            scalar (int, float): The scalar to divide the tensor by.</span></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scalar <span class="op">=</span> scalar</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> compute(<span class="va">self</span>, a: NDArray) <span class="op">-&gt;</span> NDArray:</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a><span class="co">        Divides the tensor by the scalar.</span></span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a><span class="co">            a (NDArray): The tensor to divide.</span></span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a><span class="co">            NDArray: The resulting tensor after division.</span></span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> a <span class="op">/</span> <span class="va">self</span>.scalar</span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> gradient(<span class="va">self</span>, out_grad: Tensor, node: Tensor) <span class="op">-&gt;</span> Tuple[Tensor, ...]:</span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a><span class="co">        Computes the gradient of the division operation.</span></span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-41"><a href="#cb15-41" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb15-42"><a href="#cb15-42" aria-hidden="true" tabindex="-1"></a><span class="co">            out_grad (Tensor): The gradient of the output tensor.</span></span>
<span id="cb15-43"><a href="#cb15-43" aria-hidden="true" tabindex="-1"></a><span class="co">            node (Tensor): The node in the computational graph where the operation was performed.</span></span>
<span id="cb15-44"><a href="#cb15-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-45"><a href="#cb15-45" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb15-46"><a href="#cb15-46" aria-hidden="true" tabindex="-1"></a><span class="co">            Tuple[Tensor, ...]: The gradient with respect to the tensor.</span></span>
<span id="cb15-47"><a href="#cb15-47" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb15-48"><a href="#cb15-48" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (out_grad <span class="op">/</span> <span class="va">self</span>.scalar, )</span>
<span id="cb15-49"><a href="#cb15-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-50"><a href="#cb15-50" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> divide_scalar(a: Tensor, scalar: Union[<span class="bu">int</span>, <span class="bu">float</span>]) <span class="op">-&gt;</span> Tensor:</span>
<span id="cb15-51"><a href="#cb15-51" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb15-52"><a href="#cb15-52" aria-hidden="true" tabindex="-1"></a><span class="co">    Divides a tensor by a scalar.</span></span>
<span id="cb15-53"><a href="#cb15-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-54"><a href="#cb15-54" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb15-55"><a href="#cb15-55" aria-hidden="true" tabindex="-1"></a><span class="co">        a (Tensor): The tensor to divide.</span></span>
<span id="cb15-56"><a href="#cb15-56" aria-hidden="true" tabindex="-1"></a><span class="co">        scalar (int, float): The scalar to divide the tensor by.</span></span>
<span id="cb15-57"><a href="#cb15-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-58"><a href="#cb15-58" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb15-59"><a href="#cb15-59" aria-hidden="true" tabindex="-1"></a><span class="co">        Tensor: The resulting tensor after division.</span></span>
<span id="cb15-60"><a href="#cb15-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-61"><a href="#cb15-61" aria-hidden="true" tabindex="-1"></a><span class="co">    Example:</span></span>
<span id="cb15-62"><a href="#cb15-62" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; import numpy as np</span></span>
<span id="cb15-63"><a href="#cb15-63" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; a = Tensor(np.array([1, 2, 3]))</span></span>
<span id="cb15-64"><a href="#cb15-64" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; scalar = 2</span></span>
<span id="cb15-65"><a href="#cb15-65" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; result = divide_scalar(a, scalar)</span></span>
<span id="cb15-66"><a href="#cb15-66" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; print(result)</span></span>
<span id="cb15-67"><a href="#cb15-67" aria-hidden="true" tabindex="-1"></a><span class="co">        Tensor([0.5, 1.0, 1.5])</span></span>
<span id="cb15-68"><a href="#cb15-68" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb15-69"><a href="#cb15-69" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> DivScalar(scalar)(a)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nbdev<span class="op">;</span> nbdev.nbdev_export()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>