<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="The operators module in this framework provides a collection of tensor operations for building computational graphs in deep learning. Each class in this module represents a different type of operation that can be performed on tensors, such as element-wise addition, scalar multiplication, division, exponentiation, etc.">

<title>minima - operators</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="styles.css">
<meta property="og:title" content="minima - operators">
<meta property="og:description" content="The `operators` module in this framework provides a collection of tensor operations for building computational graphs in deep learning.">
<meta property="og:site-name" content="minima">
<meta name="twitter:title" content="minima - operators">
<meta name="twitter:description" content="The `operators` module in this framework provides a collection of tensor operations for building computational graphs in deep learning.">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">minima</span>
    </a>
  </div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">operators</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Welcome to minima</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./autograd.html" class="sidebar-item-text sidebar-link">autograd</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./operators.html" class="sidebar-item-text sidebar-link active">operators</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#element-wise-addition" id="toc-element-wise-addition" class="nav-link active" data-scroll-target="#element-wise-addition">Element Wise Addition</a>
  <ul class="collapse">
  <li><a href="#add" id="toc-add" class="nav-link" data-scroll-target="#add">add</a></li>
  <li><a href="#ewiseadd" id="toc-ewiseadd" class="nav-link" data-scroll-target="#ewiseadd">EWiseAdd</a></li>
  </ul></li>
  <li><a href="#scalar-addition" id="toc-scalar-addition" class="nav-link" data-scroll-target="#scalar-addition">Scalar Addition</a>
  <ul class="collapse">
  <li><a href="#add_scalar" id="toc-add_scalar" class="nav-link" data-scroll-target="#add_scalar">add_scalar</a></li>
  <li><a href="#addscalar" id="toc-addscalar" class="nav-link" data-scroll-target="#addscalar">AddScalar</a></li>
  </ul></li>
  <li><a href="#element-wise-multiplication" id="toc-element-wise-multiplication" class="nav-link" data-scroll-target="#element-wise-multiplication">Element Wise Multiplication</a>
  <ul class="collapse">
  <li><a href="#multiply" id="toc-multiply" class="nav-link" data-scroll-target="#multiply">multiply</a></li>
  <li><a href="#ewisemul" id="toc-ewisemul" class="nav-link" data-scroll-target="#ewisemul">EWiseMul</a></li>
  </ul></li>
  <li><a href="#scalar-multiplication" id="toc-scalar-multiplication" class="nav-link" data-scroll-target="#scalar-multiplication">Scalar Multiplication</a>
  <ul class="collapse">
  <li><a href="#mul_scalar" id="toc-mul_scalar" class="nav-link" data-scroll-target="#mul_scalar">mul_scalar</a></li>
  <li><a href="#mulscalar" id="toc-mulscalar" class="nav-link" data-scroll-target="#mulscalar">MulScalar</a></li>
  </ul></li>
  <li><a href="#negation" id="toc-negation" class="nav-link" data-scroll-target="#negation">Negation</a>
  <ul class="collapse">
  <li><a href="#negate" id="toc-negate" class="nav-link" data-scroll-target="#negate">negate</a></li>
  <li><a href="#negate-1" id="toc-negate-1" class="nav-link" data-scroll-target="#negate-1">Negate</a></li>
  </ul></li>
  <li><a href="#exp" id="toc-exp" class="nav-link" data-scroll-target="#exp">Exp</a>
  <ul class="collapse">
  <li><a href="#exp-1" id="toc-exp-1" class="nav-link" data-scroll-target="#exp-1">exp</a></li>
  <li><a href="#exp-2" id="toc-exp-2" class="nav-link" data-scroll-target="#exp-2">Exp</a></li>
  </ul></li>
  <li><a href="#relu" id="toc-relu" class="nav-link" data-scroll-target="#relu">ReLU</a>
  <ul class="collapse">
  <li><a href="#relu-1" id="toc-relu-1" class="nav-link" data-scroll-target="#relu-1">relu</a></li>
  <li><a href="#relu-2" id="toc-relu-2" class="nav-link" data-scroll-target="#relu-2">ReLU</a></li>
  </ul></li>
  <li><a href="#power-scalar" id="toc-power-scalar" class="nav-link" data-scroll-target="#power-scalar">Power Scalar</a>
  <ul class="collapse">
  <li><a href="#power_scalar" id="toc-power_scalar" class="nav-link" data-scroll-target="#power_scalar">power_scalar</a></li>
  <li><a href="#powerscalar" id="toc-powerscalar" class="nav-link" data-scroll-target="#powerscalar">PowerScalar</a></li>
  </ul></li>
  <li><a href="#element-wise-divide" id="toc-element-wise-divide" class="nav-link" data-scroll-target="#element-wise-divide">Element Wise Divide</a>
  <ul class="collapse">
  <li><a href="#divide" id="toc-divide" class="nav-link" data-scroll-target="#divide">divide</a></li>
  <li><a href="#ewisediv" id="toc-ewisediv" class="nav-link" data-scroll-target="#ewisediv">EWiseDiv</a></li>
  </ul></li>
  <li><a href="#divide-scalar" id="toc-divide-scalar" class="nav-link" data-scroll-target="#divide-scalar">Divide Scalar</a>
  <ul class="collapse">
  <li><a href="#divide_scalar" id="toc-divide_scalar" class="nav-link" data-scroll-target="#divide_scalar">divide_scalar</a></li>
  <li><a href="#divscalar" id="toc-divscalar" class="nav-link" data-scroll-target="#divscalar">DivScalar</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/m0saan/minima/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block">operators</h1>
</div>

<div>
  <div class="description">
    The <code>operators</code> module in this framework provides a collection of tensor operations for building computational graphs in deep learning. Each class in this module represents a different type of operation that can be performed on tensors, such as element-wise addition, scalar multiplication, division, exponentiation, etc.
  </div>
</div>


<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->
<p>The <code>out_grad</code> parameter refers to the gradient of the loss function with respect to the output of the node. Multiplying this with the local gradient gives the gradient of the loss with respect to the input to the node, according to the chain rule of calculus, which is the basis for backpropagation in neural networks.</p>
<p>The chain rule is a fundamental concept in calculus that provides a method to compute the derivative of composite functions. In simple terms, the chain rule states that the derivative of a composite function is the derivative of the outer function multiplied by the derivative of the inner function.</p>
<p>Given a composite function that is the composition of two functions, say, <span class="math inline">\(f(g(x))\)</span>, the chain rule can be stated as follows:</p>
<p><span class="math display">\[\frac{df}{dx} = \frac{df}{dg} \cdot \frac{dg}{dx}\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(\frac{df}{dx}\)</span> is the derivative of the composite function <span class="math inline">\(f(g(x))\)</span> with respect to <span class="math inline">\(x\)</span>,</li>
<li><span class="math inline">\(\frac{df}{dg}\)</span> is the derivative of the outer function <span class="math inline">\(f\)</span> with respect to its argument <span class="math inline">\(g(x)\)</span>, and</li>
<li><span class="math inline">\(\frac{dg}{dx}\)</span> is the derivative of the inner function <span class="math inline">\(g(x)\)</span> with respect to <span class="math inline">\(x\)</span>.</li>
</ul>
<p>The chain rule can be extended to the case where we have more than two composite functions.</p>
<section id="element-wise-addition" class="level2">
<h2 class="anchored" data-anchor-id="element-wise-addition">Element Wise Addition</h2>
<p>Let’s walk through the step-by-step derivative calculation for the <a href="https://m0saan.github.io/minima/operators.html#ewiseadd"><code>EWiseAdd</code></a> operation:</p>
<p>We have the function <code>f(a, b) = a + b</code>, where <code>a</code> and <code>b</code> are tensors. Our goal is to compute the partial derivatives with respect to <code>a</code> and <code>b</code>.</p>
<p>Let’s start by calculating the derivative of <code>f</code> with respect to <code>a</code>, denoted as <code>df/da</code>:</p>
<p>Step 1: Compute the derivative of <code>f</code> with respect to <code>a</code>.</p>
<p><span class="math inline">\(\frac{{\partial f}}{{\partial a}} = \frac{{\partial}}{{\partial a}} (a + b)\)</span></p>
<p>Since <code>a</code> is the variable we are differentiating with respect to, the derivative of <code>a</code> with respect to itself is 1:</p>
<p><span class="math display">\[\frac{{\partial f}}{{\partial a}} = 1\]</span></p>
<p>Therefore, <span class="math display">\[\frac{{\partial f}}{{\partial a}} = 1.\]</span></p>
<p>Step 2: Compute the derivative of <code>f</code> with respect to <code>b</code>.</p>
<p><span class="math display">\[\frac{{\partial f}}{{\partial b}} = \frac{{\partial}}{{\partial b}} (a + b)\]</span></p>
<p>Again, since <code>b</code> is the variable we are differentiating with respect to, the derivative of <code>b</code> with respect to itself is 1:</p>
<p><span class="math display">\[\frac{{\partial f}}{{\partial b}} = 1\]</span></p>
<p>Therefore, <span class="math display">\[\frac{{\partial f}}{{\partial b}} = 1\]</span></p>
<p>Hence, the partial derivatives of <code>f(a, b) = a + b</code> with respect to <code>a</code> and <code>b</code> are both equal to 1.</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/operators.py#L61" target="_blank" style="float:right; font-size:smaller">source</a></p>
<section id="add" class="level3">
<h3 class="anchored" data-anchor-id="add">add</h3>
<blockquote class="blockquote">
<pre><code> add (a:minima.autograd.Tensor, b:minima.autograd.Tensor)</code></pre>
</blockquote>
<p>Adds two tensors element-wise.</p>
<p>Args: - a: The first tensor. - b: The second tensor.</p>
<p>Returns: The element-wise sum of a and b.</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/operators.py#L22" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="ewiseadd" class="level3">
<h3 class="anchored" data-anchor-id="ewiseadd">EWiseAdd</h3>
<blockquote class="blockquote">
<pre><code> EWiseAdd ()</code></pre>
</blockquote>
<p>Performs element-wise addition of two tensors.</p>
<p>Example: &gt;&gt;&gt; a = Tensor([1, 2, 3]) &gt;&gt;&gt; b = Tensor([4, 5, 6]) &gt;&gt;&gt; op = EWiseAdd() &gt;&gt;&gt; result = op.compute(a, b) &gt;&gt;&gt; print(result) Tensor([5, 7, 9])</p>
<p>Create two 1-D tensors</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> Tensor([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>])</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> Tensor([<span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Create an EWiseAdd operation instance</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>op <span class="op">=</span> EWiseAdd()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Compute the element-wise sum of a and b</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> op.compute(a, b)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(result) <span class="co"># Output: Tensor([5, 7, 9])</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[5 7 9]</code></pre>
</div>
</div>
<p>Alternatively, you can use the add function directly</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> add(a, b)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>result</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>minima.Tensor([5 7 9])</code></pre>
</div>
</div>
<p>or</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>op(a,b)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>minima.Tensor([5 7 9])</code></pre>
</div>
</div>
<p>For 2-D tensors, we can compute the element-wise sum of a and b in the same way</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> Tensor([[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], [<span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>]])</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> Tensor([[<span class="dv">7</span>, <span class="dv">8</span>, <span class="dv">9</span>], [<span class="dv">10</span>, <span class="dv">11</span>, <span class="dv">12</span>]])</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> op.compute(a, b)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>result</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>minima.Tensor([[ 8 10 12]
 [14 16 18]])</code></pre>
</div>
</div>
</section>
</section>
<section id="scalar-addition" class="level2">
<h2 class="anchored" data-anchor-id="scalar-addition">Scalar Addition</h2>
<p>Explanation for the derivative of the <a href="https://m0saan.github.io/minima/operators.html#addscalar"><code>AddScalar</code></a> operator:</p>
<p>Let’s denote the scalar as <code>c</code> and <code>a</code> as the tensor being added by the scalar. The operation can be described as <code>f(a) = a + c</code>.</p>
<p>The function for the backward pass (i.e., the gradient) is <code>df/da = 1</code>, which means the derivative of <code>f(a)</code> with respect to <code>a</code> is simply <code>1</code>.</p>
<p>We are given a function <span class="math inline">\(f(a) = a + c\)</span>, where <span class="math inline">\(a\)</span> is a tensor and <span class="math inline">\(c\)</span> is a scalar. Our task is to find the derivative of this function with respect to <span class="math inline">\(a\)</span>.</p>
<p>By differentiating the function <span class="math inline">\(f(a)\)</span> with respect to <span class="math inline">\(a\)</span>, we find:</p>
<p><span class="math display">\[\begin{align*}
\frac{df}{da} &amp;= \frac{d}{da} (a + c) \\
&amp;= 1
\end{align*}\]</span></p>
<p>Therefore, the gradient of <span class="math inline">\(f(a)\)</span> with respect to <span class="math inline">\(a\)</span> is <span class="math inline">\(1\)</span>.</p>
<p>We starts by defining the function <code>f(a) = a + c</code>. It then explains that when we differentiate <code>f(a)</code> with respect to <code>a</code>, we find that the derivative is <code>1</code>. This means that the gradient of <code>f(a)</code> with respect to <code>a</code> is <code>1</code>, which matches the behavior of the <a href="https://m0saan.github.io/minima/operators.html#addscalar"><code>AddScalar</code></a> operator as provided in the <code>gradient</code> method.</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/operators.py#L120" target="_blank" style="float:right; font-size:smaller">source</a></p>
<section id="add_scalar" class="level3">
<h3 class="anchored" data-anchor-id="add_scalar">add_scalar</h3>
<blockquote class="blockquote">
<pre><code> add_scalar (a:minima.autograd.Tensor, scalar:Union[int,float])</code></pre>
</blockquote>
<p>Adds a scalar to a tensor.</p>
<p>Args: - a: The tensor. - scalar: The scalar to add.</p>
<p>Returns: The sum of a and the scalar.</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/operators.py#L75" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="addscalar" class="level3">
<h3 class="anchored" data-anchor-id="addscalar">AddScalar</h3>
<blockquote class="blockquote">
<pre><code> AddScalar (scalar:Union[int,float])</code></pre>
</blockquote>
<p>Performs addition of a tensor and a scalar.</p>
<p>Example: &gt;&gt;&gt; a = Tensor([1, 2, 3]) &gt;&gt;&gt; op = AddScalar(5) &gt;&gt;&gt; result = op.compute(a) &gt;&gt;&gt; print(result) Tensor([6, 7, 8])</p>
</section>
</section>
<section id="element-wise-multiplication" class="level2">
<h2 class="anchored" data-anchor-id="element-wise-multiplication">Element Wise Multiplication</h2>
<p>Explanation for the derivative of the <a href="https://m0saan.github.io/minima/operators.html#ewisemul"><code>EWiseMul</code></a> (element-wise multiplication) operator:</p>
<p>Let’s denote the two input tensors as <code>a</code> and <code>b</code>. The operation can be described as <code>f(a, b) = a * b</code>, where <code>*</code> represents element-wise multiplication.</p>
<p>The function for the backward pass (i.e., the gradient) is <code>df/da = b</code> and <code>df/db = a</code>. This means that the derivative of <code>f(a, b)</code> with respect to <code>a</code> is <code>b</code>, and the derivative with respect to <code>b</code> is <code>a</code>.</p>
<p>We are given a function <span class="math inline">\(f(a, b) = a \odot b\)</span>, where <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are tensors, and <span class="math inline">\(\odot\)</span> represents element-wise multiplication. Our task is to find the derivatives of this function with respect to <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>.</p>
<p>By differentiating the function <span class="math inline">\(f(a, b)\)</span> with respect to <span class="math inline">\(a\)</span>, we find:</p>
<p><span class="math display">\[\begin{align*}
\frac{df}{da} &amp;= \frac{d}{da} (a \odot b) \\
&amp;= b
\end{align*}\]</span></p>
<p>Therefore, the gradient of <span class="math inline">\(f(a, b)\)</span> with respect to <span class="math inline">\(a\)</span> is <span class="math inline">\(b\)</span>.</p>
<p>Similarly, by differentiating the function <span class="math inline">\(f(a, b)\)</span> with respect to <span class="math inline">\(b\)</span>, we find:</p>
<p><span class="math display">\[\begin{align*}
\frac{df}{db} &amp;= \frac{d}{db} (a \odot b) \\
&amp;= a
\end{align*}\]</span></p>
<p>Therefore, the gradient of <span class="math inline">\(f(a, b)\)</span> with respect to <span class="math inline">\(b\)</span> is <span class="math inline">\(a\)</span>.</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/operators.py#L173" target="_blank" style="float:right; font-size:smaller">source</a></p>
<section id="multiply" class="level3">
<h3 class="anchored" data-anchor-id="multiply">multiply</h3>
<blockquote class="blockquote">
<pre><code> multiply (a:minima.autograd.Tensor, b:minima.autograd.Tensor)</code></pre>
</blockquote>
<p>Multiplies two tensors element-wise.</p>
<p>Args: - a: The first tensor. - b: The second tensor.</p>
<p>Returns: The element-wise product of a and b.</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/operators.py#L134" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="ewisemul" class="level3">
<h3 class="anchored" data-anchor-id="ewisemul">EWiseMul</h3>
<blockquote class="blockquote">
<pre><code> EWiseMul ()</code></pre>
</blockquote>
<p>Performs element-wise multiplication of two tensors.</p>
<p>Example: &gt;&gt;&gt; a = Tensor([1, 2, 3]) &gt;&gt;&gt; b = Tensor([4, 5, 6]) &gt;&gt;&gt; op = EWiseMul() &gt;&gt;&gt; result = op.compute(a, b) &gt;&gt;&gt; print(result) Tensor([4, 10, 18])</p>
</section>
</section>
<section id="scalar-multiplication" class="level2">
<h2 class="anchored" data-anchor-id="scalar-multiplication">Scalar Multiplication</h2>
<p>Let’s denote the scalar as <code>c</code> and <code>a</code> as the tensor being multiplied by the scalar. The operation can be described as <code>f(a) = a * c</code>.</p>
<p>The function for the backward pass (i.e., the gradient) is <code>df/da = c</code>, which means the derivative of <code>f(a)</code> with respect to <code>a</code> is <code>c</code>.</p>
<p>The LaTeX document will look as follows:</p>
<p>We are given a function <span class="math inline">\(f(a) = a \cdot c\)</span>, where <span class="math inline">\(a\)</span> is a tensor and <span class="math inline">\(c\)</span> is a scalar. Our task is to find the derivative of this function with respect to <span class="math inline">\(a\)</span>.</p>
<p>By differentiating the function <span class="math inline">\(f(a)\)</span> with respect to <span class="math inline">\(a\)</span>, we find:</p>
<p><span class="math display">\[\begin{align*}
\frac{df}{da} &amp;= \frac{d}{da} (a \cdot c) \\
&amp;= c
\end{align*}\]</span></p>
<p>Therefore, the gradient of <span class="math inline">\(f(a)\)</span> with respect to <span class="math inline">\(a\)</span> is <span class="math inline">\(c\)</span>.</p>
<p>We starts by defining the function <code>f(a) = a * c</code>. It then explains that when we differentiate <code>f(a)</code> with respect to <code>a</code>, we find that the derivative is <code>c</code>. This means that the gradient of <code>f(a)</code> with respect to <code>a</code> is <code>c</code>, which matches the behavior of the <a href="https://m0saan.github.io/minima/operators.html#mulscalar"><code>MulScalar</code></a> operator as provided in the <code>gradient</code> method.</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/operators.py#L232" target="_blank" style="float:right; font-size:smaller">source</a></p>
<section id="mul_scalar" class="level3">
<h3 class="anchored" data-anchor-id="mul_scalar">mul_scalar</h3>
<blockquote class="blockquote">
<pre><code> mul_scalar (a:minima.autograd.Tensor, scalar:Union[int,float])</code></pre>
</blockquote>
<p>Multiplies a tensor by a scalar.</p>
<p>Args: - a: The tensor. - scalar: The scalar to multiply.</p>
<p>Returns: The product of a and the scalar.</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/operators.py#L187" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="mulscalar" class="level3">
<h3 class="anchored" data-anchor-id="mulscalar">MulScalar</h3>
<blockquote class="blockquote">
<pre><code> MulScalar (scalar:Union[int,float])</code></pre>
</blockquote>
<p>Performs multiplication of a tensor and a scalar.</p>
<p>Example: &gt;&gt;&gt; a = Tensor([1, 2, 3]) &gt;&gt;&gt; op = MulScalar(5) &gt;&gt;&gt; result = op.compute(a) &gt;&gt;&gt; print(result) Tensor([5, 10, 15])</p>
</section>
</section>
<section id="negation" class="level2">
<h2 class="anchored" data-anchor-id="negation">Negation</h2>
<p>Let’s denote <code>a</code> as the tensor being negated. The operation can be described as <code>f(a) = -a</code>.</p>
<p>The function for the backward pass (i.e., the gradient) is <code>df/da = -1</code>.</p>
<p>We are given a function <span class="math inline">\(f(a) = -a\)</span>, where <span class="math inline">\(a\)</span> is a tensor. Our task is to find the derivative of this function with respect to <span class="math inline">\(a\)</span>.</p>
<p>By differentiating the function <span class="math inline">\(f(a)\)</span> with respect to <span class="math inline">\(a\)</span>, we find:</p>
<p><span class="math display">\[\begin{align*}
\frac{df}{da} &amp;= \frac{d}{da} (-a) \\
&amp;= -1
\end{align*}\]</span></p>
<p>Therefore, the gradient of <span class="math inline">\(f(a)\)</span> with respect to <span class="math inline">\(a\)</span> is <span class="math inline">\(-1\)</span>.</p>
<hr>
<section id="negate" class="level3">
<h3 class="anchored" data-anchor-id="negate">negate</h3>
<blockquote class="blockquote">
<pre><code> negate (a:minima.autograd.Tensor)</code></pre>
</blockquote>
<p>Negates the given tensor.</p>
<p>Args: - a: The tensor to negate.</p>
<p>Returns: The negation of a.</p>
<p>Example: &gt;&gt;&gt; a = Tensor([1, -2, 3]) &gt;&gt;&gt; result = negate(a) &gt;&gt;&gt; print(result) Tensor([-1, 2, -3])</p>
<hr>
</section>
<section id="negate-1" class="level3">
<h3 class="anchored" data-anchor-id="negate-1">Negate</h3>
<blockquote class="blockquote">
<pre><code> Negate ()</code></pre>
</blockquote>
<p>Negates the given tensor.</p>
<p>Example: &gt;&gt;&gt; a = Tensor([1, -2, 3]) &gt;&gt;&gt; op = Negate() &gt;&gt;&gt; result = op.compute(a) &gt;&gt;&gt; print(result) Tensor([-1, 2, -3])</p>
</section>
</section>
<section id="exp" class="level2">
<h2 class="anchored" data-anchor-id="exp">Exp</h2>
<p>Explanation for the derivative of the <code>Exp</code> operator:</p>
<p>Let’s denote <code>a</code> as the tensor on which the exponential function is applied. The operation can be described as <code>f(a) = exp(a)</code>, where <code>exp</code> represents the exponential function.</p>
<p>The function for the backward pass (i.e., the gradient) is <code>df/da = exp(a)</code>.</p>
<p>We are given a function <span class="math inline">\(f(a) = \exp(a)\)</span>, where <span class="math inline">\(a\)</span> is a tensor. Our task is to find the derivative of this function with respect to <span class="math inline">\(a\)</span>.</p>
<p>By differentiating the function <span class="math inline">\(f(a)\)</span> with respect to <span class="math inline">\(a\)</span>, we find:</p>
<p><span class="math display">\[\begin{align*}
\frac{df}{da} &amp;= \frac{d}{da} (\exp(a)) \\
&amp;= \exp(a)
\end{align*}\]</span></p>
<p>Therefore, the gradient of <span class="math inline">\(f(a)\)</span> with respect to <span class="math inline">\(a\)</span> is <span class="math inline">\(\exp(a)\)</span>.</p>
<hr>
<section id="exp-1" class="level3">
<h3 class="anchored" data-anchor-id="exp-1">exp</h3>
<blockquote class="blockquote">
<pre><code> exp (a:minima.autograd.Tensor)</code></pre>
</blockquote>
<p>Calculates the exponential of the given tensor.</p>
<p>Args: - a: The tensor.</p>
<p>Returns: The exponential of a.</p>
<p>Example: &gt;&gt;&gt; a = Tensor([1, 2, 3]) &gt;&gt;&gt; result = exp(a) &gt;&gt;&gt; print(result) Tensor([2.71828183, 7.3890561, 20.08553692])</p>
<hr>
</section>
<section id="exp-2" class="level3">
<h3 class="anchored" data-anchor-id="exp-2">Exp</h3>
<blockquote class="blockquote">
<pre><code> Exp ()</code></pre>
</blockquote>
<p>Calculates the exponential of the given tensor.</p>
<p>Example: &gt;&gt;&gt; a = Tensor([1, 2, 3]) &gt;&gt;&gt; op = Exp() &gt;&gt;&gt; result = op.compute(a) &gt;&gt;&gt; print(result) Tensor([2.71828183, 7.3890561, 20.08553692])</p>
</section>
</section>
<section id="relu" class="level2">
<h2 class="anchored" data-anchor-id="relu">ReLU</h2>
<p>The derivative of the <code>ReLU</code> (Rectified Linear Unit) operator:</p>
<p>Let’s denote <code>a</code> as the tensor on which the ReLU function is applied. The ReLU function is defined as follows:</p>
<p><span class="math display">\[
f(a) =
\begin{cases}
a, &amp; \text{if } a \geq 0 \\
0, &amp; \text{if } a &lt; 0
\end{cases}
\]</span></p>
<p>The function for the backward pass (i.e., the gradient) is <code>df/da = 1</code> if <code>a &gt;= 0</code>, and <code>df/da = 0</code> if <code>a &lt; 0</code>.</p>
<p>We are given a function <span class="math inline">\(f(a) = \max(0, a)\)</span>, where <span class="math inline">\(a\)</span> is a tensor. Our task is to find the derivative of this function with respect to <span class="math inline">\(a\)</span>.</p>
<p>By considering the definition of the ReLU function, we can write <span class="math inline">\(f(a)\)</span> as:</p>
<p><span class="math display">\[
f(a) =
\begin{cases}
a, &amp; \text{if } a \geq 0 \\
0, &amp; \text{if } a &lt; 0
\end{cases}
\]</span></p>
<p>Now, let’s differentiate <span class="math inline">\(f(a)\)</span> with respect to <span class="math inline">\(a\)</span>:</p>
<p><span class="math display">\[
\frac{df}{da} =
\begin{cases}
1, &amp; \text{if } a \geq 0 \\
0, &amp; \text{if } a &lt; 0
\end{cases}
\]</span></p>
<p>Therefore, the gradient of <span class="math inline">\(f(a)\)</span> with respect to <span class="math inline">\(a\)</span> is <span class="math inline">\(1\)</span> if <span class="math inline">\(a \geq 0\)</span>, and <span class="math inline">\(0\)</span> if <span class="math inline">\(a &lt; 0\)</span>.</p>
<hr>
<section id="relu-1" class="level3">
<h3 class="anchored" data-anchor-id="relu-1">relu</h3>
<blockquote class="blockquote">
<pre><code> relu (a:minima.autograd.Tensor)</code></pre>
</blockquote>
<p>Applies the ReLU (Rectified Linear Unit) activation function to the given tensor.</p>
<p>Args: - a: The tensor.</p>
<p>Returns: The result of applying ReLU to a.</p>
<p>Example: &gt;&gt;&gt; a = Tensor([1, -2, 3]) &gt;&gt;&gt; result = relu(a) &gt;&gt;&gt; print(result) Tensor([1, 0, 3])</p>
<hr>
</section>
<section id="relu-2" class="level3">
<h3 class="anchored" data-anchor-id="relu-2">ReLU</h3>
<blockquote class="blockquote">
<pre><code> ReLU ()</code></pre>
</blockquote>
<p>Applies the ReLU (Rectified Linear Unit) activation function to the given tensor.</p>
<p>Example: &gt;&gt;&gt; a = Tensor([1, -2, 3]) &gt;&gt;&gt; op = ReLU() &gt;&gt;&gt; result = op.compute(a) &gt;&gt;&gt; print(result) Tensor([1, 0, 3])</p>
</section>
</section>
<section id="power-scalar" class="level2">
<h2 class="anchored" data-anchor-id="power-scalar">Power Scalar</h2>
<p>The derivative of the <code>PowerScalar</code> operator:</p>
<p>Let’s denote the scalar as <code>n</code> and <code>a</code> as the tensor being raised to the power of the scalar. The operation can be described as <code>f(a) = a^n</code>.</p>
<p>The function for the backward pass (i.e., the gradient) is <code>df/da = n * a^(n-1)</code>.</p>
<p>We are given a function <span class="math inline">\(f(a) = a^n\)</span>, where <span class="math inline">\(a\)</span> is a tensor and <span class="math inline">\(n\)</span> is a scalar. Our task is to find the derivative of this function with respect to <span class="math inline">\(a\)</span>.</p>
<p>By differentiating the function <span class="math inline">\(f(a)\)</span> with respect to <span class="math inline">\(a\)</span>, we find:</p>
<p><span class="math display">\[\begin{align*}
\frac{df}{da} &amp;= \frac{d}{da} (a^n) \\
&amp;= n \cdot a^{n-1}
\end{align*}\]</span></p>
<p>Therefore, the gradient of <span class="math inline">\(f(a)\)</span> with respect to <span class="math inline">\(a\)</span> is <span class="math inline">\(n \cdot a^{n-1}\)</span>.</p>
<hr>
<section id="power_scalar" class="level3">
<h3 class="anchored" data-anchor-id="power_scalar">power_scalar</h3>
<blockquote class="blockquote">
<pre><code> power_scalar (a:minima.autograd.Tensor, scalar:int)</code></pre>
</blockquote>
<p>Raises a tensor to a power.</p>
<p>Args: a (Tensor): The input tensor. scalar (int): The power to raise the tensor to.</p>
<p>Returns: Tensor: The resulting tensor after the power operation.</p>
<p>Example: &gt;&gt;&gt; import numpy as np &gt;&gt;&gt; tensor = Tensor(np.array([1, 2, 3])) &gt;&gt;&gt; result = power_scalar(tensor, 2) &gt;&gt;&gt; print(result) Tensor([1, 4, 9])</p>
<hr>
</section>
<section id="powerscalar" class="level3">
<h3 class="anchored" data-anchor-id="powerscalar">PowerScalar</h3>
<blockquote class="blockquote">
<pre><code> PowerScalar (scalar:int)</code></pre>
</blockquote>
<p>The PowerScalar operation raises a tensor to an (integer) power.</p>
<p>Attributes: scalar (int): The power to raise the tensor to.</p>
<p>Example: &gt;&gt;&gt; import numpy as np &gt;&gt;&gt; tensor = Tensor(np.array([1, 2, 3])) &gt;&gt;&gt; pow_scalar = PowerScalar(2) &gt;&gt;&gt; result = pow_scalar.compute(tensor.data) &gt;&gt;&gt; print(result) array([1, 4, 9])</p>
</section>
</section>
<section id="element-wise-divide" class="level2">
<h2 class="anchored" data-anchor-id="element-wise-divide">Element Wise Divide</h2>
<p>The operation described here is an element-wise division of two tensors, <code>a</code> and <code>b</code>, where the operation can be described as <code>f(a, b) = a / b</code>.</p>
<p>We’ll compute the partial derivatives with respect to <code>a</code> and <code>b</code>:</p>
<ol type="1">
<li><p>The partial derivative of <code>f(a, b)</code> with respect to <code>a</code> (<code>df/da</code>) is <code>1/b</code>.</p></li>
<li><p>The partial derivative of <code>f(a, b)</code> with respect to <code>b</code> (<code>df/db</code>) is <code>-a / b^2</code>.</p></li>
</ol>
<p>We are given a function <span class="math inline">\(f(a, b) = \frac{a}{b}\)</span>, where <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are tensors. Our task is to find the partial derivatives of this function with respect to <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>.</p>
<p>Let’s start with <span class="math inline">\(\frac{\partial f}{\partial a}\)</span>:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial f}{\partial a} &amp;= \frac{\partial}{\partial a} \left(\frac{a}{b}\right) \\
&amp;= \frac{1}{b}
\end{align*}\]</span></p>
<p>Now, let’s compute <span class="math inline">\(\frac{\partial f}{\partial b}\)</span>:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial f}{\partial b} &amp;= \frac{\partial}{\partial b} \left(\frac{a}{b}\right) \\
&amp;= - \frac{a}{b^{2}}
\end{align*}\]</span></p>
<p>Here is a detailed derivative:</p>
<p>Given a function of the form <span class="math inline">\(y = \frac{u}{v}\)</span>, where both <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span> are functions of <span class="math inline">\(x\)</span>, the quotient rule of differentiation states:</p>
<p><span class="math display">\[\frac{dy}{dx} = \frac{v \cdot \frac{du}{dx} - u \cdot \frac{dv}{dx}}{v^2}\]</span></p>
<p>In our case, we’re looking at the function <span class="math inline">\(y = \frac{a}{b}\)</span>, where <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are tensors. We want to find the derivative with respect to <span class="math inline">\(b\)</span> (instead of <span class="math inline">\(x\)</span> in our general formula). So we have:</p>
<p><span class="math display">\[\frac{dy}{db} = \frac{b \cdot \frac{da}{db} - a \cdot \frac{db}{db}}{b^2}\]</span></p>
<p>Since <span class="math inline">\(a\)</span> does not depend on <span class="math inline">\(b\)</span>, <span class="math inline">\(\frac{da}{db} = 0\)</span>, and since any variable is equal to itself, <span class="math inline">\(\frac{db}{db} = 1\)</span>.</p>
<p>So the derivative <span class="math inline">\(\frac{dy}{db}\)</span> simplifies to:</p>
<p><span class="math display">\[\frac{dy}{db} = \frac{b \cdot 0 - a \cdot 1}{b^2}\]</span></p>
<p>Therefore, the derivative of <span class="math inline">\(y\)</span> with respect to <span class="math inline">\(b\)</span> is <span class="math inline">\(-\frac{a}{b^2}\)</span>.</p>
<p>Therefore, the gradient of <span class="math inline">\(f(a, b)\)</span> with respect to <span class="math inline">\(a\)</span> is <span class="math inline">\(\frac{1}{b}\)</span>, and the gradient of <span class="math inline">\(f(a, b)\)</span> with respect to <span class="math inline">\(b\)</span> is <span class="math inline">\(- \frac{a}{b^{2}}\)</span>.</p>
<hr>
<section id="divide" class="level3">
<h3 class="anchored" data-anchor-id="divide">divide</h3>
<blockquote class="blockquote">
<pre><code> divide (a:minima.autograd.Tensor, b:minima.autograd.Tensor)</code></pre>
</blockquote>
<p>Divides two tensors element-wise.</p>
<p>Args: a (Tensor): The dividend tensor. b (Tensor): The divisor tensor.</p>
<p>Returns: Tensor: The resulting tensor after element-wise division.</p>
<p>Example: &gt;&gt;&gt; import numpy as np &gt;&gt;&gt; a = Tensor(np.array([1, 2, 3])) &gt;&gt;&gt; b = Tensor(np.array([4, 5, 6])) &gt;&gt;&gt; result = divide(a, b) &gt;&gt;&gt; print(result) Tensor([0.25, 0.4, 0.5])</p>
<hr>
</section>
<section id="ewisediv" class="level3">
<h3 class="anchored" data-anchor-id="ewisediv">EWiseDiv</h3>
<blockquote class="blockquote">
<pre><code> EWiseDiv ()</code></pre>
</blockquote>
<p>The EWiseDiv operation divides two tensors element-wise.</p>
<p>Example: &gt;&gt;&gt; import numpy as np &gt;&gt;&gt; a = Tensor(np.array([1, 2, 3])) &gt;&gt;&gt; b = Tensor(np.array([4, 5, 6])) &gt;&gt;&gt; div = EWiseDiv() &gt;&gt;&gt; result = div.compute(a.data, b.data) &gt;&gt;&gt; print(result) array([0.25, 0.4, 0.5])</p>
</section>
</section>
<section id="divide-scalar" class="level2">
<h2 class="anchored" data-anchor-id="divide-scalar">Divide Scalar</h2>
<p>Let’s denote the scalar as <code>c</code>, and <code>a</code> as the tensor being divided by the scalar. The operation can be described as <code>f(a) = a / c</code>.</p>
<p>The function for the backward pass (i.e., the gradient) is <code>df/da = 1/c</code>.</p>
<p>This is the derivative of <code>f(a)</code> with respect to <code>a</code>.</p>
<p>We are given a function <span class="math inline">\(f(a) = \frac{a}{c}\)</span>, where <span class="math inline">\(a\)</span> is a tensor and <span class="math inline">\(c\)</span> is a scalar. Our task is to find the derivative of this function with respect to <span class="math inline">\(a\)</span>.</p>
<p>By using the power rule of differentiation, where the derivative of <span class="math inline">\(a^n\)</span> is <span class="math inline">\(n \cdot a^{n-1}\)</span>, we can rewrite <span class="math inline">\(f(a)\)</span> as <span class="math inline">\(f(a) = c^{-1}a\)</span>.</p>
<p>Now, we can differentiate this with respect to <span class="math inline">\(a\)</span>:</p>
<p><span class="math display">\[\begin{align*}
\frac{df}{da} &amp;= \frac{d}{da} (c^{-1}a) \\
&amp;= c^{-1} \frac{d}{da} (a) \\
&amp;= c^{-1} \\
&amp;= \frac{1}{c}
\end{align*}\]</span></p>
<p>Therefore, the gradient of <span class="math inline">\(f(a)\)</span> with respect to <span class="math inline">\(a\)</span> is <span class="math inline">\(\frac{1}{c}\)</span>.</p>
<hr>
<section id="divide_scalar" class="level3">
<h3 class="anchored" data-anchor-id="divide_scalar">divide_scalar</h3>
<blockquote class="blockquote">
<pre><code> divide_scalar (a:minima.autograd.Tensor, scalar:Union[int,float])</code></pre>
</blockquote>
<p>Divides a tensor by a scalar.</p>
<p>Args: a (Tensor): The tensor to divide. scalar (int, float): The scalar to divide the tensor by.</p>
<p>Returns: Tensor: The resulting tensor after division.</p>
<p>Example: &gt;&gt;&gt; import numpy as np &gt;&gt;&gt; a = Tensor(np.array([1, 2, 3])) &gt;&gt;&gt; scalar = 2 &gt;&gt;&gt; result = divide_scalar(a, scalar) &gt;&gt;&gt; print(result) Tensor([0.5, 1.0, 1.5])</p>
<hr>
</section>
<section id="divscalar" class="level3">
<h3 class="anchored" data-anchor-id="divscalar">DivScalar</h3>
<blockquote class="blockquote">
<pre><code> DivScalar (scalar:Union[int,float])</code></pre>
</blockquote>
<p>The DivScalar operation divides a tensor by a scalar.</p>
<p>Example: &gt;&gt;&gt; import numpy as np &gt;&gt;&gt; a = Tensor(np.array([1, 2, 3])) &gt;&gt;&gt; scalar = 2 &gt;&gt;&gt; div_scalar = DivScalar(scalar) &gt;&gt;&gt; result = div_scalar.compute(a.data) &gt;&gt;&gt; print(result) array([0.5, 1.0, 1.5])</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nbdev<span class="op">;</span> nbdev.nbdev_export()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>