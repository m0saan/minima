[
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "foo\n\n foo ()"
  },
  {
    "objectID": "autograd.html",
    "href": "autograd.html",
    "title": "autograd",
    "section": "",
    "text": "Value\n\n Value ()\n\nInitialize self. See help(type(self)) for accurate signature."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to minima",
    "section": "",
    "text": "minima is a lightweight deep learning framewor, lean yet effective tailor-made for educational exploration.\nJust like a delicate sapling inspired by the towering strength of an oak, Minima draws its inspiration from PyTorch.\nYet, it carves its own identity with a straightforward interface and a curated set of features.\nThis makes learning and using it a breeze, allowing you to effortlessly build and train neural networks.\nIndeed, Minima is your friendly companion on the journey to understanding deep learning, where less is often more.\n\n\n\nYou can install minima on your own machines with conda\nIf you’re using miniconda (recommended) then run:\nconda install minima\n…or if you’re using Anaconda then run:\nconda install minima anaconda\nTo install with pip, use: pip install minima.\nIf you plan to develop Minima yourself, or want to be on the cutting edge, you can use an editable install.\ngit clone https://github.com/yourusername/minima\npip install -e \"minima[dev]\"\n\n\n\n\nEasy to install and use\nSimple and intuitive API for defining and training neural networks\nBuilt-in support for common layers and activation functions\nSupports both CPU and GPU acceleration\nCompatible with NumPy arrays for easy data manipulation\n\n\n\n\nHere’s a simple example of how to define and train a neural network using Minima:\nimport minima as mi\n\n# Define the neural network architecture\nmodel = mi.Sequential(\n    mi.Linear(784, 128),\n    mi.ReLU(),\n    mi.Linear(128, 10),\n    mi.Softmax()\n)\n\n# Load the dataset\nx_train, y_train, x_test, y_test = load_data()\n\n# Train the model\nloss_fn = mi.CrossEntropyLoss()\noptimizer = mi.SGD(model.parameters(), lr=0.01)\nfor epoch in range(10):\n    for x_batch, y_batch in minibatch(x_train, y_train, batch_size=32):\n        y_pred = model(x_batch)\n        loss = loss_fn(y_pred, y_batch)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n# Evaluate the model\ny_pred = model(x_test)\naccuracy = compute_accuracy(y_pred, y_test)\nprint(f\"Accuracy: {accuracy:.2f}\")\nThis example defines a simple neural network with two linear layers and two activation functions, trains it on a dataset using stochastic gradient descent, and evaluates its accuracy on a test set.\n\n\n\nFor more information on how to use minima, please refer to the documentation, which can be found in the website above.\n\n\n\ncomming soon!\n\n\n\nminima is released under the Apache License 2.0. See LICENSE for more information."
  }
]