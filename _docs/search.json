[
  {
    "objectID": "operators.html",
    "href": "operators.html",
    "title": "operators",
    "section": "",
    "text": "The out_grad parameter refers to the gradient of the loss function with respect to the output of the node. Multiplying this with the local gradient gives the gradient of the loss with respect to the input to the node, according to the chain rule of calculus, which is the basis for backpropagation in neural networks.\nThe chain rule is a fundamental concept in calculus that provides a method to compute the derivative of composite functions. In simple terms, the chain rule states that the derivative of a composite function is the derivative of the outer function multiplied by the derivative of the inner function.\nGiven a composite function that is the composition of two functions, say, \\(f(g(x))\\), the chain rule can be stated as follows:\n\\[\\frac{df}{dx} = \\frac{df}{dg} \\cdot \\frac{dg}{dx}\\]\nWhere:\n\n\\(\\frac{df}{dx}\\) is the derivative of the composite function \\(f(g(x))\\) with respect to \\(x\\),\n\\(\\frac{df}{dg}\\) is the derivative of the outer function \\(f\\) with respect to its argument \\(g(x)\\), and\n\\(\\frac{dg}{dx}\\) is the derivative of the inner function \\(g(x)\\) with respect to \\(x\\).\n\nThe chain rule can be extended to the case where we have more than two composite functions."
  },
  {
    "objectID": "operators.html#element-wise-addition",
    "href": "operators.html#element-wise-addition",
    "title": "operators",
    "section": "Element Wise Addition",
    "text": "Element Wise Addition\nLet’s walk through the step-by-step derivative calculation for the EWiseAdd operation:\nWe have the function f(a, b) = a + b, where a and b are tensors. Our goal is to compute the partial derivatives with respect to a and b.\nLet’s start by calculating the derivative of f with respect to a, denoted as df/da:\nStep 1: Compute the derivative of f with respect to a.\n\\(\\frac{{\\partial f}}{{\\partial a}} = \\frac{{\\partial}}{{\\partial a}} (a + b)\\)\nSince a is the variable we are differentiating with respect to, the derivative of a with respect to itself is 1:\n\\[\\frac{{\\partial f}}{{\\partial a}} = 1\\]\nTherefore, \\[\\frac{{\\partial f}}{{\\partial a}} = 1.\\]\nStep 2: Compute the derivative of f with respect to b.\n\\[\\frac{{\\partial f}}{{\\partial b}} = \\frac{{\\partial}}{{\\partial b}} (a + b)\\]\nAgain, since b is the variable we are differentiating with respect to, the derivative of b with respect to itself is 1:\n\\[\\frac{{\\partial f}}{{\\partial b}} = 1\\]\nTherefore, \\[\\frac{{\\partial f}}{{\\partial b}} = 1\\]\nHence, the partial derivatives of f(a, b) = a + b with respect to a and b are both equal to 1.\n\nsource\n\nadd\n\n add (a:minima.autograd.Tensor, b:minima.autograd.Tensor)\n\nAdds two tensors element-wise.\nArgs: - a: The first tensor. - b: The second tensor.\nReturns: The element-wise sum of a and b.\n\nsource\n\n\nEWiseAdd\n\n EWiseAdd ()\n\nPerforms element-wise addition of two tensors.\nExample: >>> a = Tensor([1, 2, 3]) >>> b = Tensor([4, 5, 6]) >>> op = EWiseAdd() >>> result = op.compute(a, b) >>> print(result) Tensor([5, 7, 9])\nCreate two 1-D tensors\n\na = Tensor([1, 2, 3])\nb = Tensor([4, 5, 6])\n\nCreate an EWiseAdd operation instance\n\nop = EWiseAdd()\n\nCompute the element-wise sum of a and b\n\nresult = op.compute(a, b)\nresult\n\nminima.Tensor([5 7 9])\n\n\nAlternatively, you can use the add function directly\n\nresult = add(a, b)\nresult\n\nminima.Tensor([5 7 9])\n\n\nor\n\nop(a,b)\n\nminima.Tensor([5 7 9])\n\n\nFor 2-D tensors, we can compute the element-wise sum of a and b in the same way\n\na = Tensor([[1, 2, 3], [4, 5, 6]])\nb = Tensor([[7, 8, 9], [10, 11, 12]])\n\nresult = op.compute(a, b)\nresult\n\nminima.Tensor([[ 8 10 12]\n [14 16 18]])"
  },
  {
    "objectID": "operators.html#scalar-addition",
    "href": "operators.html#scalar-addition",
    "title": "operators",
    "section": "Scalar Addition",
    "text": "Scalar Addition\nExplanation for the derivative of the AddScalar operator:\nLet’s denote the scalar as c and a as the tensor being added by the scalar. The operation can be described as f(a) = a + c.\nThe function for the backward pass (i.e., the gradient) is df/da = 1, which means the derivative of f(a) with respect to a is simply 1.\nWe are given a function \\(f(a) = a + c\\), where \\(a\\) is a tensor and \\(c\\) is a scalar. Our task is to find the derivative of this function with respect to \\(a\\).\nBy differentiating the function \\(f(a)\\) with respect to \\(a\\), we find:\n\\[\\begin{align*}\n\\frac{df}{da} &= \\frac{d}{da} (a + c) \\\\\n&= 1\n\\end{align*}\\]\nTherefore, the gradient of \\(f(a)\\) with respect to \\(a\\) is \\(1\\).\nWe starts by defining the function f(a) = a + c. It then explains that when we differentiate f(a) with respect to a, we find that the derivative is 1. This means that the gradient of f(a) with respect to a is 1, which matches the behavior of the AddScalar operator as provided in the gradient method.\n\nsource\n\nadd_scalar\n\n add_scalar (a:minima.autograd.Tensor, scalar:Union[int,float])\n\nAdds a scalar to a tensor.\nArgs: - a: The tensor. - scalar: The scalar to add.\nReturns: The sum of a and the scalar.\n\nsource\n\n\nAddScalar\n\n AddScalar (scalar:Union[int,float])\n\nPerforms addition of a tensor and a scalar.\nExample: >>> a = Tensor([1, 2, 3]) >>> op = AddScalar(5) >>> result = op.compute(a) >>> print(result) Tensor([6, 7, 8])"
  },
  {
    "objectID": "operators.html#element-wise-multiplication",
    "href": "operators.html#element-wise-multiplication",
    "title": "operators",
    "section": "Element Wise Multiplication",
    "text": "Element Wise Multiplication\nExplanation for the derivative of the EWiseMul (element-wise multiplication) operator:\nLet’s denote the two input tensors as a and b. The operation can be described as f(a, b) = a * b, where * represents element-wise multiplication.\nThe function for the backward pass (i.e., the gradient) is df/da = b and df/db = a. This means that the derivative of f(a, b) with respect to a is b, and the derivative with respect to b is a.\nWe are given a function \\(f(a, b) = a \\odot b\\), where \\(a\\) and \\(b\\) are tensors, and \\(\\odot\\) represents element-wise multiplication. Our task is to find the derivatives of this function with respect to \\(a\\) and \\(b\\).\nBy differentiating the function \\(f(a, b)\\) with respect to \\(a\\), we find:\n\\[\\begin{align*}\n\\frac{df}{da} &= \\frac{d}{da} (a \\odot b) \\\\\n&= b\n\\end{align*}\\]\nTherefore, the gradient of \\(f(a, b)\\) with respect to \\(a\\) is \\(b\\).\nSimilarly, by differentiating the function \\(f(a, b)\\) with respect to \\(b\\), we find:\n\\[\\begin{align*}\n\\frac{df}{db} &= \\frac{d}{db} (a \\odot b) \\\\\n&= a\n\\end{align*}\\]\nTherefore, the gradient of \\(f(a, b)\\) with respect to \\(b\\) is \\(a\\).\n\nsource\n\nmultiply\n\n multiply (a:minima.autograd.Tensor, b:minima.autograd.Tensor)\n\nMultiplies two tensors element-wise.\nArgs: - a: The first tensor. - b: The second tensor.\nReturns: The element-wise product of a and b.\n\nsource\n\n\nEWiseMul\n\n EWiseMul ()\n\nPerforms element-wise multiplication of two tensors.\nExample: >>> a = Tensor([1, 2, 3]) >>> b = Tensor([4, 5, 6]) >>> op = EWiseMul() >>> result = op.compute(a, b) >>> print(result) Tensor([4, 10, 18])"
  },
  {
    "objectID": "operators.html#scalar-multiplication",
    "href": "operators.html#scalar-multiplication",
    "title": "operators",
    "section": "Scalar Multiplication",
    "text": "Scalar Multiplication\nLet’s denote the scalar as c and a as the tensor being multiplied by the scalar. The operation can be described as f(a) = a * c.\nThe function for the backward pass (i.e., the gradient) is df/da = c, which means the derivative of f(a) with respect to a is c.\nThe LaTeX document will look as follows:\nWe are given a function \\(f(a) = a \\cdot c\\), where \\(a\\) is a tensor and \\(c\\) is a scalar. Our task is to find the derivative of this function with respect to \\(a\\).\nBy differentiating the function \\(f(a)\\) with respect to \\(a\\), we find:\n\\[\\begin{align*}\n\\frac{df}{da} &= \\frac{d}{da} (a \\cdot c) \\\\\n&= c\n\\end{align*}\\]\nTherefore, the gradient of \\(f(a)\\) with respect to \\(a\\) is \\(c\\).\nWe starts by defining the function f(a) = a * c. It then explains that when we differentiate f(a) with respect to a, we find that the derivative is c. This means that the gradient of f(a) with respect to a is c, which matches the behavior of the MulScalar operator as provided in the gradient method.\n\nsource\n\nmul_scalar\n\n mul_scalar (a:minima.autograd.Tensor, scalar:Union[int,float])\n\nMultiplies a tensor by a scalar.\nArgs: - a: The tensor. - scalar: The scalar to multiply.\nReturns: The product of a and the scalar.\n\nsource\n\n\nMulScalar\n\n MulScalar (scalar:Union[int,float])\n\nPerforms multiplication of a tensor and a scalar.\nExample: >>> a = Tensor([1, 2, 3]) >>> op = MulScalar(5) >>> result = op.compute(a) >>> print(result) Tensor([5, 10, 15])"
  },
  {
    "objectID": "operators.html#element-wise-divide",
    "href": "operators.html#element-wise-divide",
    "title": "operators",
    "section": "Element Wise Divide",
    "text": "Element Wise Divide\nThe operation described here is an element-wise division of two tensors, a and b, where the operation can be described as f(a, b) = a / b.\nWe’ll compute the partial derivatives with respect to a and b:\n\nThe partial derivative of f(a, b) with respect to a (df/da) is 1/b.\nThe partial derivative of f(a, b) with respect to b (df/db) is -a / b^2.\n\nWe are given a function \\(f(a, b) = \\frac{a}{b}\\), where \\(a\\) and \\(b\\) are tensors. Our task is to find the partial derivatives of this function with respect to \\(a\\) and \\(b\\).\nLet’s start with \\(\\frac{\\partial f}{\\partial a}\\):\n\\[\\begin{align*}\n\\frac{\\partial f}{\\partial a} &= \\frac{\\partial}{\\partial a} \\left(\\frac{a}{b}\\right) \\\\\n&= \\frac{1}{b}\n\\end{align*}\\]\nNow, let’s compute \\(\\frac{\\partial f}{\\partial b}\\):\n\\[\\begin{align*}\n\\frac{\\partial f}{\\partial b} &= \\frac{\\partial}{\\partial b} \\left(\\frac{a}{b}\\right) \\\\\n&= - \\frac{a}{b^{2}}\n\\end{align*}\\]\nHere is a detailed derivative:\nGiven a function of the form \\(y = \\frac{u}{v}\\), where both \\(u\\) and \\(v\\) are functions of \\(x\\), the quotient rule of differentiation states:\n\\[\\frac{dy}{dx} = \\frac{v \\cdot \\frac{du}{dx} - u \\cdot \\frac{dv}{dx}}{v^2}\\]\nIn our case, we’re looking at the function \\(y = \\frac{a}{b}\\), where \\(a\\) and \\(b\\) are tensors. We want to find the derivative with respect to \\(b\\) (instead of \\(x\\) in our general formula). So we have:\n\\[\\frac{dy}{db} = \\frac{b \\cdot \\frac{da}{db} - a \\cdot \\frac{db}{db}}{b^2}\\]\nSince \\(a\\) does not depend on \\(b\\), \\(\\frac{da}{db} = 0\\), and since any variable is equal to itself, \\(\\frac{db}{db} = 1\\).\nSo the derivative \\(\\frac{dy}{db}\\) simplifies to:\n\\[\\frac{dy}{db} = \\frac{b \\cdot 0 - a \\cdot 1}{b^2}\\]\nTherefore, the derivative of \\(y\\) with respect to \\(b\\) is \\(-\\frac{a}{b^2}\\).\nTherefore, the gradient of \\(f(a, b)\\) with respect to \\(a\\) is \\(\\frac{1}{b}\\), and the gradient of \\(f(a, b)\\) with respect to \\(b\\) is \\(- \\frac{a}{b^{2}}\\).\n\nsource\n\ndivide\n\n divide (a:minima.autograd.Tensor, b:minima.autograd.Tensor)\n\nDivides two tensors element-wise.\nArgs: a (Tensor): The dividend tensor. b (Tensor): The divisor tensor.\nReturns: Tensor: The resulting tensor after element-wise division.\nExample: >>> import numpy as np >>> a = Tensor(np.array([1, 2, 3])) >>> b = Tensor(np.array([4, 5, 6])) >>> result = divide(a, b) >>> print(result) Tensor([0.25, 0.4, 0.5])\n\nsource\n\n\nEWiseDiv\n\n EWiseDiv ()\n\nThe EWiseDiv operation divides two tensors element-wise.\nExample: >>> import numpy as np >>> a = Tensor(np.array([1, 2, 3])) >>> b = Tensor(np.array([4, 5, 6])) >>> div = EWiseDiv() >>> result = div.compute(a.data, b.data) >>> print(result) array([0.25, 0.4, 0.5])"
  },
  {
    "objectID": "operators.html#scalar-division",
    "href": "operators.html#scalar-division",
    "title": "operators",
    "section": "Scalar Division",
    "text": "Scalar Division\nLet’s denote the scalar as c, and a as the tensor being divided by the scalar. The operation can be described as f(a) = a / c.\nThe function for the backward pass (i.e., the gradient) is df/da = 1/c.\nThis is the derivative of f(a) with respect to a.\nWe are given a function \\(f(a) = \\frac{a}{c}\\), where \\(a\\) is a tensor and \\(c\\) is a scalar. Our task is to find the derivative of this function with respect to \\(a\\).\nBy using the power rule of differentiation, where the derivative of \\(a^n\\) is \\(n \\cdot a^{n-1}\\), we can rewrite \\(f(a)\\) as \\(f(a) = c^{-1}a\\).\nNow, we can differentiate this with respect to \\(a\\):\n\\[\\begin{align*}\n\\frac{df}{da} &= \\frac{d}{da} (c^{-1}a) \\\\\n&= c^{-1} \\frac{d}{da} (a) \\\\\n&= c^{-1} \\\\\n&= \\frac{1}{c}\n\\end{align*}\\]\nTherefore, the gradient of \\(f(a)\\) with respect to \\(a\\) is \\(\\frac{1}{c}\\).\n\nsource\n\ndivide_scalar\n\n divide_scalar (a:minima.autograd.Tensor, scalar:Union[int,float])\n\nDivides a tensor by a scalar.\nArgs: a (Tensor): The tensor to divide. scalar (int, float): The scalar to divide the tensor by.\nReturns: Tensor: The resulting tensor after division.\nExample: >>> import numpy as np >>> a = Tensor(np.array([1, 2, 3])) >>> scalar = 2 >>> result = divide_scalar(a, scalar) >>> print(result) Tensor([0.5, 1.0, 1.5])\n\nsource\n\n\nDivScalar\n\n DivScalar (scalar:Union[int,float])\n\nThe DivScalar operation divides a tensor by a scalar.\nExample: >>> import numpy as np >>> a = Tensor(np.array([1, 2, 3])) >>> scalar = 2 >>> div_scalar = DivScalar(scalar) >>> result = div_scalar.compute(a.data) >>> print(result) array([0.5, 1.0, 1.5])"
  },
  {
    "objectID": "operators.html#negation",
    "href": "operators.html#negation",
    "title": "operators",
    "section": "Negation",
    "text": "Negation\nLet’s denote a as the tensor being negated. The operation can be described as f(a) = -a.\nThe function for the backward pass (i.e., the gradient) is df/da = -1.\nWe are given a function \\(f(a) = -a\\), where \\(a\\) is a tensor. Our task is to find the derivative of this function with respect to \\(a\\).\nBy differentiating the function \\(f(a)\\) with respect to \\(a\\), we find:\n\\[\\begin{align*}\n\\frac{df}{da} &= \\frac{d}{da} (-a) \\\\\n&= -1\n\\end{align*}\\]\nTherefore, the gradient of \\(f(a)\\) with respect to \\(a\\) is \\(-1\\).\n\nsource\n\nnegate\n\n negate (a:minima.autograd.Tensor)\n\nNegates the given tensor.\nArgs: - a: The tensor to negate.\nReturns: The negation of a.\nExample: >>> a = Tensor([1, -2, 3]) >>> result = negate(a) >>> print(result) Tensor([-1, 2, -3])\n\nsource\n\n\nNegate\n\n Negate ()\n\nNegates the given tensor.\nExample: >>> a = Tensor([1, -2, 3]) >>> op = Negate() >>> result = op.compute(a) >>> print(result) Tensor([-1, 2, -3])"
  },
  {
    "objectID": "operators.html#exp",
    "href": "operators.html#exp",
    "title": "operators",
    "section": "Exp",
    "text": "Exp\nExplanation for the derivative of the Exp operator:\nLet’s denote a as the tensor on which the exponential function is applied. The operation can be described as f(a) = exp(a), where exp represents the exponential function.\nThe function for the backward pass (i.e., the gradient) is df/da = exp(a).\nWe are given a function \\(f(a) = \\exp(a)\\), where \\(a\\) is a tensor. Our task is to find the derivative of this function with respect to \\(a\\).\nBy differentiating the function \\(f(a)\\) with respect to \\(a\\), we find:\n\\[\\begin{align*}\n\\frac{df}{da} &= \\frac{d}{da} (\\exp(a)) \\\\\n&= \\exp(a)\n\\end{align*}\\]\nTherefore, the gradient of \\(f(a)\\) with respect to \\(a\\) is \\(\\exp(a)\\).\n\nsource\n\nexp\n\n exp (a:minima.autograd.Tensor)\n\nCalculates the exponential of the given tensor.\nArgs: - a: The tensor.\nReturns: The exponential of a.\nExample: >>> a = Tensor([1, 2, 3]) >>> result = exp(a) >>> print(result) Tensor([2.71828183, 7.3890561, 20.08553692])\n\nsource\n\n\nExp\n\n Exp ()\n\nCalculates the exponential of the given tensor.\nExample: >>> a = Tensor([1, 2, 3]) >>> op = Exp() >>> result = op.compute(a) >>> print(result) Tensor([2.71828183, 7.3890561, 20.08553692])"
  },
  {
    "objectID": "operators.html#relu",
    "href": "operators.html#relu",
    "title": "operators",
    "section": "ReLU",
    "text": "ReLU\nThe derivative of the ReLU (Rectified Linear Unit) operator:\nLet’s denote a as the tensor on which the ReLU function is applied. The ReLU function is defined as follows:\n\\[\nf(a) =\n\\begin{cases}\na, & \\text{if } a \\geq 0 \\\\\n0, & \\text{if } a < 0\n\\end{cases}\n\\]\nThe function for the backward pass (i.e., the gradient) is df/da = 1 if a >= 0, and df/da = 0 if a < 0.\nWe are given a function \\(f(a) = \\max(0, a)\\), where \\(a\\) is a tensor. Our task is to find the derivative of this function with respect to \\(a\\).\nBy considering the definition of the ReLU function, we can write \\(f(a)\\) as:\n\\[\nf(a) =\n\\begin{cases}\na, & \\text{if } a \\geq 0 \\\\\n0, & \\text{if } a < 0\n\\end{cases}\n\\]\nNow, let’s differentiate \\(f(a)\\) with respect to \\(a\\):\n\\[\n\\frac{df}{da} =\n\\begin{cases}\n1, & \\text{if } a \\geq 0 \\\\\n0, & \\text{if } a < 0\n\\end{cases}\n\\]\nTherefore, the gradient of \\(f(a)\\) with respect to \\(a\\) is \\(1\\) if \\(a \\geq 0\\), and \\(0\\) if \\(a < 0\\).\n\nsource\n\nrelu\n\n relu (a:minima.autograd.Tensor)\n\nApplies the ReLU (Rectified Linear Unit) activation function to the given tensor.\nArgs: - a: The tensor.\nReturns: The result of applying ReLU to a.\nExample: >>> a = Tensor([1, -2, 3]) >>> result = relu(a) >>> print(result) Tensor([1, 0, 3])\n\nsource\n\n\nReLU\n\n ReLU ()\n\nApplies the ReLU (Rectified Linear Unit) activation function to the given tensor.\nExample: >>> a = Tensor([1, -2, 3]) >>> op = ReLU() >>> result = op.compute(a) >>> print(result) Tensor([1, 0, 3])"
  },
  {
    "objectID": "operators.html#power-scalar",
    "href": "operators.html#power-scalar",
    "title": "operators",
    "section": "Power Scalar",
    "text": "Power Scalar\nThe derivative of the PowerScalar operator:\nLet’s denote the scalar as n and a as the tensor being raised to the power of the scalar. The operation can be described as f(a) = a^n.\nThe function for the backward pass (i.e., the gradient) is df/da = n * a^(n-1).\nWe are given a function \\(f(a) = a^n\\), where \\(a\\) is a tensor and \\(n\\) is a scalar. Our task is to find the derivative of this function with respect to \\(a\\).\nBy differentiating the function \\(f(a)\\) with respect to \\(a\\), we find:\n\\[\\begin{align*}\n\\frac{df}{da} &= \\frac{d}{da} (a^n) \\\\\n&= n \\cdot a^{n-1}\n\\end{align*}\\]\nTherefore, the gradient of \\(f(a)\\) with respect to \\(a\\) is \\(n \\cdot a^{n-1}\\).\n\nsource\n\npower_scalar\n\n power_scalar (a:minima.autograd.Tensor, scalar:int)\n\nRaises a tensor to a power.\nArgs: a (Tensor): The input tensor. scalar (int): The power to raise the tensor to.\nReturns: Tensor: The resulting tensor after the power operation.\nExample: >>> import numpy as np >>> tensor = Tensor(np.array([1, 2, 3])) >>> result = power_scalar(tensor, 2) >>> print(result) Tensor([1, 4, 9])\n\nsource\n\n\nPowerScalar\n\n PowerScalar (scalar:int)\n\nThe PowerScalar operation raises a tensor to an (integer) power.\nAttributes: scalar (int): The power to raise the tensor to.\nExample: >>> import numpy as np >>> tensor = Tensor(np.array([1, 2, 3])) >>> pow_scalar = PowerScalar(2) >>> result = pow_scalar.compute(tensor.data) >>> print(result) array([1, 4, 9])"
  },
  {
    "objectID": "operators.html#log",
    "href": "operators.html#log",
    "title": "operators",
    "section": "Log",
    "text": "Log\nExplanation for the derivative of the Log operator:\nLet’s denote a as the tensor on which the logarithm is applied. The operation can be described as f(a) = log(a), where log represents the natural logarithm.\nThe function for the backward pass (i.e., the gradient) is df/da = 1/a.\nWe are given a function \\(f(a) = \\log(a)\\), where \\(a\\) is a tensor. Our task is to find the derivative of this function with respect to \\(a\\).\nBy differentiating the function \\(f(a)\\) with respect to \\(a\\), we find:\n\\[\\begin{align*}\n\\frac{df}{da} &= \\frac{d}{da} (\\log(a)) \\\\\n&= \\frac{1}{a}\n\\end{align*}\\]\nWe started by defining the function f(a) = log(a), where log represents the natural logarithm. It then explains that when we differentiate f(a) with respect to a, we find that the derivative is 1/a. This means that the gradient of f(a) with respect to a is 1/a, which represents the behavior of the Log operator.\n\nclass Log(TensorOp):\n    \"\"\"\n    The Log operation applies the natural logarithm element-wise on the tensor.\n\n    Example:\n        >>> import numpy as np\n        >>> a = Tensor(np.array([1.0, 2.0, 3.0]))\n        >>> log_op = Log()\n        >>> result = log_op.compute(a.data)\n        >>> print(result)\n        array([0., 0.69314718, 1.09861229])\n    \"\"\"\n\n    def compute(self, a: NDArray) -> NDArray:\n        \"\"\"\n        Applies the natural logarithm to the tensor.\n\n        Args:\n            a (NDArray): The input tensor.\n\n        Returns:\n            NDArray: The resulting tensor after applying the natural logarithm.\n        \"\"\"\n        return array_api.log(a)\n\n    def gradient(self, out_grad: Tensor, node: Tensor) -> Tuple[Tensor, ...]:\n        \"\"\"\n        Computes the gradient of the log operation.\n\n        Args:\n            out_grad (Tensor): The gradient of the output tensor.\n            node (Tensor): The node in the computational graph where the operation was performed.\n\n        Returns:\n            Tuple[Tensor, ...]: The gradient with respect to the input tensor.\n        \"\"\"\n        a = node.children[0]\n        return (out_grad / a, )\n\ndef log(a: Tensor) -> Tensor:\n    \"\"\"\n    Applies the natural logarithm to the tensor.\n\n    Args:\n        a (Tensor): The input tensor.\n\n    Returns:\n        Tensor: The resulting tensor after applying the natural logarithm.\n\n    Example:\n        >>> import numpy as np\n        >>> a = Tensor(np.array([1.0, 2.0, 3.0]))\n        >>> result = log(a)\n        >>> print(result)\n        Tensor([0., 0.69314718, 1.09861229])\n    \"\"\"\n    return Log()(a)"
  },
  {
    "objectID": "operators.html#transpose",
    "href": "operators.html#transpose",
    "title": "operators",
    "section": "Transpose",
    "text": "Transpose\nThe operation described here is a transposition of a tensor a, where the operation can be described as f(a) = a^T.\nWe’ll compute the derivative of this operation.\nFirst, we note that the transpose operation doesn’t change the values of the tensor elements but only their positions. This means that the gradient of a transposed tensor is just the transposed gradient of the original tensor.\nLet’s denote the gradient of the transposed tensor as g = df/da, where f(a) = a^T.\nGiven this, we can derive the following:\n\nThe derivative of f(a) with respect to a is df/da = g^T.\n\nThis conclusion can be illustrated as follows in Latex:\nWe are given a function \\(f(a) = a^T\\), where \\(a\\) is a tensor and \\(a^T\\) is the transpose of the tensor. Our task is to find the derivative of this function with respect to \\(a\\).\nLet’s compute \\(\\frac{df}{da}\\):\n\\[\\begin{align*}\n\\frac{df}{da} &= \\frac{d}{da} (a^T) \\\\\n&= (g)^T\n\\end{align*}\\]\nHere, \\(g\\) is the gradient of the transposed tensor. The derivative of a transposed tensor is the transposed derivative of the original tensor.\nNow, let’s apply this to the Transpose class.\nThe gradient method in the Transpose class computes the gradient of the transpose operation. The gradient of the transposed tensor is just the transposed gradient of the original tensor. This is implemented by applying the transpose function to out_grad, which is the gradient of the output tensor, and then returning this transposed gradient. The axes used for the transpose operation are the same as the ones used in the forward pass.\nTherefore, the gradient of the transposition operation with respect to the input tensor a is the transpose of the output gradient out_grad.\nIn this code, transpose(out_grad, axes=self.axes) performs the transposition of out_grad along the specified axes.\n\nsource\n\ntranspose\n\n transpose (a:minima.autograd.Tensor, axes:Optional[tuple]=None)\n\nPerform the transpose operation on the input tensor along the specified axes. If no axes are specified, it swaps the last two dimensions of the input tensor.\nArgs: a (Tensor): The input tensor. axes (Optional[tuple]): The pair of axes that should be swapped. If not provided, the last two axes are swapped.\nReturns: Tensor: The transposed tensor.\nExample: >>> a = Tensor(np.arange(1, 7).reshape(2, 3)) >>> result = transpose(a) >>> print(result) Tensor([[1, 4], [2, 5], [3, 6]])\n\nsource\n\n\nTranspose\n\n Transpose (axes:Optional[tuple]=None)\n\nTensor operation class that performs transposition of a tensor along specified axes.\nIf no axes are specified, it swaps the last two dimensions of the input tensor.\nExample: >>> a = Tensor(np.arange(1, 7).reshape(2, 3)) >>> op = Transpose() >>> result = op.compute(a.data) >>> print(result) array([[1, 4], [2, 5], [3, 6]])"
  },
  {
    "objectID": "operators.html#reshape",
    "href": "operators.html#reshape",
    "title": "operators",
    "section": "Reshape",
    "text": "Reshape\nThe operation described here is a reshaping of a tensor a, where the operation can be described as f(a) = reshape(a, new_shape).\nWe’ll compute the derivative of this operation.\nThe reshaping operation doesn’t change the values of the tensor elements but only rearranges them. This means that the gradient of a reshaped tensor is just the reshaped gradient of the original tensor.\nLet’s denote the gradient of the reshaped tensor as g = df/da, where f(a) = reshape(a, new_shape).\nGiven this, we can derive the following:\n\nThe derivative of f(a) with respect to a is df/da = reshape(g, original_shape).\n\nThis conclusion can be illustrated as follows in Latex:\nWe are given a function \\(f(a) = reshape(a, new\\_shape)\\), where \\(a\\) is a tensor and reshape(a, new_shape) is the reshaped tensor. Our task is to find the derivative of this function with respect to \\(a\\).\nLet’s compute \\(\\frac{df}{da}\\):\n\\[\\begin{align*}\n\\frac{df}{da} &= \\frac{d}{da} (reshape(a, new\\_shape)) \\\\\n&= reshape(g, original\\_shape)\n\\end{align*}\\]\nHere, \\(g\\) is the gradient of the reshaped tensor. The derivative of a reshaped tensor is the reshaped derivative of the original tensor. The reshaped derivative has the same shape as the original tensor.\nNow, let’s apply this to the Reshape class.\nThe gradient method in the Reshape class computes the gradient of the reshape operation. The gradient of the reshaped tensor is just the reshaped gradient of the original tensor. This is implemented by applying the reshape function to out_grad, which is the gradient of the output tensor, and then returning this reshaped gradient. The shape used for the reshaping is the shape of the original tensor, which is obtained from node.children[0].shape.\nTherefore, the gradient of the reshape operation with respect to the input tensor a is the reshaping of the output gradient out_grad to the shape of the original tensor.\nHere is the corresponding Python code:\ndef gradient(self, out_grad: Tensor, node: Tensor) -> Tuple[Tensor, ...]:\n    \"\"\"\n    Compute the gradient of the reshape operation.\n\n    Args:\n        out_grad (Tensor): The gradient of the output tensor.\n        node (Tensor): The node in the computational graph where the operation was performed.\n\n    Returns:\n        Tuple[Tensor, ...]: The gradient with respect to the input tensor.\n    \"\"\"\n    input_shape = node.children[0].shape\n    return reshape(out_grad, input_shape),\nIn this code, reshape(out_grad, input_shape) performs the reshaping of out_grad to the shape of the original tensor.\n\nsource\n\nreshape\n\n reshape (a:minima.autograd.Tensor, shape:Tuple[int,...])\n\nReshape the input tensor to the specified shape.\nArgs: a (Tensor): The input tensor. shape (Tuple[int, …]): The desired shape of the output tensor.\nReturns: Tensor: The reshaped tensor.\nExample: >>> a = Tensor([1, 2, 3, 4, 5, 6]) >>> result = reshape(a, (2, 3)) >>> print(result) Tensor([[1, 2, 3], [4, 5, 6]])\n\nsource\n\n\nReshape\n\n Reshape (shape:Tuple[int,...])\n\nTensor operation class that reshapes a tensor.\nExample: >>> a = Tensor([1, 2, 3, 4, 5, 6]) >>> op = Reshape((2, 3)) >>> result = op.compute(a) >>> print(result) Tensor([[1, 2, 3], [4, 5, 6]])\n\nimport nbdev; nbdev.nbdev_export()"
  },
  {
    "objectID": "autograd.html",
    "href": "autograd.html",
    "title": "autograd",
    "section": "",
    "text": "Today, I’d like to introduce you to the automatic differentiation component of my deep learning framework, Minima. I am going to explain a specific module, called Micrograd, in detail. I had released the Micrograd module on Github a couple of years ago, but never provided an in-depth explanation of its functioning. So here we go.\nMicrograd, at its core, is an autograd (short for automatic gradient) engine. It implements the backpropagation algorithm, a crucial aspect of deep learning. Backpropagation enables efficient computation of the gradient (rate of change) of a loss function (something we aim to minimize) with respect to the weights of a neural network. This enables us to fine-tune the weights of the neural network iteratively, reducing the loss function, and in turn, increasing the accuracy of the network. Backpropagation is central to most modern deep neural network libraries, such as PyTorch or Jax.\nTo help illustrate what Micrograd does, consider a simple example. We start with two inputs, a and b, encapsulated in special objects we call “Value” objects. We then construct a mathematical expression involving a and b. The Value objects help Micrograd to keep track of how these inputs are being transformed, and the operations performed on them, resulting in a complete mathematical “expression graph”.\nMicrograd then performs two key tasks. First, it carries out a ‘forward pass’ - it evaluates the final value of the expression we’ve created. But more importantly, it then performs a ‘backward pass’ - essentially a run of the backpropagation algorithm. It starts from the final value and traces back through the expression graph, calculating the derivative (rate of change) of the final value with respect to each of the original inputs and intermediate nodes, using the chain rule of calculus.\nThis is crucial as these derivatives tell us how much our final value is affected by small changes in the inputs. In essence, the derivative of the final value with respect to an input is a measure of the “sensitivity” of the final value to that input.\nNow, the example above might seem abstract - the expression we constructed didn’t have any particular meaning, it was just a demonstration of the capabilities of Micrograd. But the reason it’s useful is that this kind of mathematical expression is exactly what neural networks are - they take input data and network weights as inputs, and transform them through a series of mathematical operations into a final output, usually a prediction or a loss value.\nOne thing to note here is that Micrograd operates at the level of individual scalar values, not n-dimensional tensors as you’d typically find in full-scale deep learning libraries. This is for simplicity and instructional clarity. In real-world, high-performance neural network libraries, tensors are used to bundle up large arrays of scalar values, enabling efficient, parallel computation. But fundamentally, the math stays the same.\nSo, what’s the magic behind Micrograd? Surprisingly, the core autograd engine, the part that handles backpropagation and makes neural network training possible, is a mere 100 lines of simple Python code. On top of that, the neural network library, constructed based on this autograd engine, is only an additional 50 lines of code. It’s pretty impressive how much power you can get from just a handful of well-written Python code lines.\nAll of this is to say, understanding automatic differentiation and neural network training doesn’t require an enormous, complicated codebase. It’s essentially about understanding a relatively small number of key concepts and how they work together. Of course, making these things run fast and efficiently in practice does require additional complexity, but at a fundamental level, what’s happening isn’t that complicated. And Micrograd serves as an excellent tool for understanding these fundamentals. Now,"
  },
  {
    "objectID": "autograd.html#derivatives",
    "href": "autograd.html#derivatives",
    "title": "autograd",
    "section": "Derivatives",
    "text": "Derivatives\nIn calculus, the derivative of a function at a certain point is a measure of how the function changes at that point. It is defined as the limit of the ratio of the change in the function value (f(x)) to the change in the x value (Δx) as Δx approaches zero. This can be written as:\n\\[f'(x) = \\lim_{{Δx \\to 0}} \\frac{{f(x + Δx) - f(x)}}{{Δx}}\\]\nThis equation represents the slope of the tangent line to the function at a specific point x, which can also be interpreted as the instantaneous rate of change of the function at that point.\nIf you have a function y = f(x) = x^n, where n is a constant, the power rule of differentiation tells us that the derivative of f(x) with respect to x is:\n\\[f'(x) = n * x^{n-1}\\]\nIn the context of the function d = a*b + c which we’re going to use below, since a is the variable and b and c are constants, the derivative of d with respect to a is just b. This can be written in LaTeX as:\n\\[ \\frac{{dd}}{{da}} = b \\]\nwe begin by assigning values to three variables a, b, and c. We then create a fourth variable, d, which is equal to the product of a and b, added to c. When you execute this cell, it should display the value of d.\n\na = 4\nb = -2\nc = 11\nd = a*b + c\nd\n\n3\n\n\nwe define a function f_a(a,b,c), which helps us estimate the slope of the function at the point a. The function first calculates d1 using the given inputs, a, b, and c. Then it increments a by a small value h and recalculates the value d2. The function then prints the original d1, the new d2, and the estimated slope which is (d2 - d1) / h.\n\ndef f_a(a,b,c):\n    h = 0.01\n    d1 = a*b + c\n    a += h\n    d2 = a*b + c\n    \n    print(f'd1: {d1}')\n    print(f'd2: {d2}')\n    print(f'slope: {(d2 - d1) / h}')\n    \nf_a(a,b,c)\n\nd1: 3\nd2: 2.9800000000000004\nslope: -1.9999999999999574\n\n\nthat states that the derivative of d with respect to a, denoted as (db/da), is analytically equal to b. This is because in the expression d = a*b + c, the coefficient of a is b, so by the power rule of differentiation, the derivative is b. In this case, b equals -2.\nNow if we do this with b\n\ndef f_b(a,b,c):\n    h = 0.01\n    d1 = a*b + c\n    b += h\n    d2 = a*b + c\n    \n    print(f'd1: {d1}')\n    print(f'd2: {d2}')\n    print(f'slope: {(d2 - d1) / h}')\n    \nf_b(a,b,c)\n\nd1: 3\nd2: 3.04\nslope: 4.0000000000000036\n\n\nHere’s what happens in the function: 1. It begins by defining a small change h which is set to 0.01. 2. Then, the function calculates d1, which is the result of a*b + c with the original inputs a, b, and c. 3. It increments b by the small value h. 4. Next, the function calculates a new d2, which is the result of a*b + c after the increment to b. 5. Finally, the function prints out the original d1, the new d2, and the estimated slope calculated as (d2 - d1) / h.\nWhen you call f_b(a,b,c), the function performs all these operations using the values of a, b, and c from the previous context.\nThe output will give you an approximate value of the derivative of d with respect to b (noted as dd/db in mathematical notation), assuming that the function d(a, b, c) = a*b + c is relatively smooth and continuous near the point b."
  },
  {
    "objectID": "autograd.html#derivatives-in-the-context-of-neural-nets-autograd",
    "href": "autograd.html#derivatives-in-the-context-of-neural-nets-autograd",
    "title": "autograd",
    "section": "Derivatives in the context of neural nets (Autograd)",
    "text": "Derivatives in the context of neural nets (Autograd)\nAutomatic differentiation, or auto grad as it’s often referred to in the context of deep learning, is a powerful tool that greatly simplifies the process of working with derivatives. It does this by automatically computing the derivatives (or gradients) of functions, thus relieving the need to manually calculate these derivatives as we have done above.\nThe use of auto grad is fundamental to the training process of deep learning models. Deep learning models, such as neural networks, are essentially complex mathematical functions with many parameters (weights and biases). Training these models involves adjusting these parameters to minimize a loss function, which quantifies how well the model is performing on a given task. The most common method for doing this is gradient descent, which uses the gradients of the loss function with respect to the parameters to update the parameters in a way that decreases the loss.\nHowever, the manual calculation of these gradients, especially for complex models, is not only tedious but also prone to errors. Here’s where auto grad comes in. By using automatic differentiation, we can compute these gradients automatically and accurately, no matter how complex the model is.\nIn a deep learning framework, when we define our model and loss function, the framework uses auto grad to build a computational graph under the hood. This graph captures all the computations that are done in the forward pass (i.e., when we pass our inputs through the model to get the output). Then, when we need to compute the gradients during the backward pass, the framework uses this computational graph and the chain rule from calculus to compute the gradients automatically. This process is often referred to as backpropagation.\nThe main advantage of using auto grad in deep learning is that it allows us to focus on designing our models and defining our loss functions without worrying about the details of computing the gradients. This simplifies our code, reduces the chance of errors, and allows for greater flexibility in designing complex models. In fact, with auto grad, we can easily experiment with new types of models and loss functions, as we can rely on the framework to correctly compute the gradients no matter how complex our design is.\nLet’s start building a mini autograd engine\n\ndef trace(root):\n    nodes, edges = set(), set()\n    def build(v):\n        if v not in nodes:\n            nodes.add(v)\n            for child in v.children:\n                edges.add((child, v))\n                build(child)\n    build(root)\n    return nodes, edges\n\ndef draw_dot(root, format='svg', rankdir='LR'):\n    \"\"\"\n    format: png | svg | ...\n    rankdir: TB (top to bottom graph) | LR (left to right)\n    \"\"\"\n    assert rankdir in ['LR', 'TB']\n    nodes, edges = trace(root)\n    dot = Digraph(format=format, graph_attr={'rankdir': rankdir}) #, node_attr={'rankdir': 'TB'})\n    \n    for n in nodes:\n        dot.node(name=str(id(n)), label = \"{%s data %.4f | grad %.4f }\" % (n.label, n._data, n.grad), shape='record')\n        if n._op:\n            dot.node(name=str(id(n)) + n._op, label=n._op)\n            dot.edge(str(id(n)) + n._op, str(id(n)))\n    \n    for n1, n2 in edges:\n        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n    \n    return dot\n\nIn the context of deep learning and automatic differentiation, the Value class is designed to encapsulate a scalar value and its relationships within a computational graph. This abstraction is essential for constructing mathematical expressions from basic operations and for performing the forward pass, which evaluates the expression.\nThe Value class is initialized with data and optional parameters specifying its children (or dependencies) and the operation that produced it. Each instance of the Value class can have a gradient, which is initialized as zero and can be updated during backpropagation.\nTwo fundamental operations are implemented for instances of the Value class: addition (add) and multiplication (mul). These methods allow two Value instances (or a Value and a scalar) to be added or multiplied, respectively. The results of these operations are also Value instances, maintaining the relationships in the computational graph.\nThis ability to build out mathematical expressions using only addition and multiplication allows for the construction of a broad variety of functions. For example, given multiple inputs (a, b, c, f), we can formulate a mathematical expression that generates a single output (l). After the forward pass, the output value is calculated and can be visualized, as demonstrated in the example where the forward pass output is -8.\nIn summary, the Value class is a fundamental building block for creating and navigating a computational graph in the context of automatic differentiation, making it an invaluable tool in any deep learning framework.\n\nsource\n\nValue\n\n Value (data, _children=(), _op='', label='')\n\nRepresents a node within a computational graph.\nThis class encapsulates a single value and its relationships in the graph, making it easy to track and manage the value’s dependencies, the operation that produced it, and whether it requires a gradient for backpropagation. It’s central to the functioning of automatic differentiation within deep learning frameworks.\nAttributes: op (Operator) _prev (Set[‘Value’]) cached_data (NDArray) requires_grad (bool)\n\na = Value(2.0, label='a')\nb = Value(-3.0, label='b')\nc = Value(10.0, label='c')\ne = a*b; e.label='e'\nd = e + c; d.label='d'\nf = Value(-2.0, label='f')\nL = d*f; L.label='L' \n\ndraw_dot(L)\n\n\n\n\nThe code provided builds upon the previously discussed Value class, which acts as a node within a computational graph in the context of automatic differentiation. It demonstrates how to define scalar values a, b, c, and f and use them to build a computational graph. The graph computes the expression L = (a * b + c) * f, represented in nodes labeled ‘e’, ‘d’, and ‘L’.\nThe focus of this explanation is the process of backpropagation and the computation of gradients for every node in the graph, which is crucial for training neural networks. In a neural network setting, the loss function L would typically be calculated with respect to the network’s weights. Here, these weights are abstractly represented by the scalar variables a, b, c, and f.\nThe fundamental idea behind backpropagation is to compute the derivative of the output value L with respect to every node in the graph. These derivatives represent the impact each node has on the final output. They are stored in the grad attribute of the Value class, which is initialized to zero, signifying that there is initially no effect on the output.\nIn this context, a gradient of zero means changing the value of a node has no effect on the final output, or loss function. After performing backpropagation, the grad attribute will store the actual derivative of L with respect to that node. This is essential information when training a neural network because it dictates how to adjust the weights (in this example, a, b, c, and f) to minimize the loss function L.\nThe function draw_dot(L) is presumably used to visualize this computational graph, including both the data and the grad of each node. This visualization aids in understanding the forward and backward passes of computation within the graph.\nIn conclusion, this code snippet creates a simple computational graph using the Value class, computes a mathematical expression, and prepares for backpropagation. The next steps would involve the actual calculation of the gradients, enabling the iterative optimization of weights based on their influence on the final output."
  },
  {
    "objectID": "autograd.html#manual-gradient",
    "href": "autograd.html#manual-gradient",
    "title": "autograd",
    "section": "Manual gradient",
    "text": "Manual gradient\n\nbase case (L grad)\n\ndef lol():\n    h = 0.001\n    \n    a = Value(2.0, label='a')\n    b = Value(-3.0, label='b')\n    c = Value(10.0, label='c')\n    e = a*b; e.label='e'\n    d = e + c; d.label='d'\n    f = Value(-2.0, label='f')\n    L = d*f; L.label='L'\n    \n    L1 = L.data\n    \n    a = Value(2.0, label='a')\n    b = Value(-3.0, label='b')\n    c = Value(10.0, label='c')\n    e = a*b; e.label='e'\n    d = e + c; d.label='d'\n    f = Value(-2.0, label='f')\n    L = d*f; L.label='L' \n    \n    L2 = L.data + h\n    \n    print(f'grad: {(L2 - L1) / h}')\n\nlol()\n\ngrad: 1.000000000000334\n\n\nsure enough it’s 1\n\nL.grad = 1\n\n\nf\nHere is a generic version of lol\n\ndef lol(label):\n    def foo(v, label):\n        if v.label == label: v.data += h\n        \n    h = 0.001\n    \n    a = Value(2.0, label='a')\n    b = Value(-3.0, label='b')\n    c = Value(10.0, label='c')\n    e = a*b; e.label='e'\n    d = e + c; d.label='d'\n    f = Value(-2.0, label='f')\n    L = d*f; L.label='L'\n    \n    L1 = L.data\n    \n    a = Value(2.0, label='a'); foo(a, label)\n    b = Value(-3.0, label='b'); foo(b, label)\n    c = Value(10.0, label='c'); foo(c, label)\n    e = a*b; e.label='e'; foo(e, label)\n    d = e + c; d.label='d'; foo(d, label)\n    f = Value(-2.0, label='f'); foo(f, label)\n    L = d*f; L.label='L'; foo(L, label) \n    \n    L2 = L.data\n    \n    print(f'grad: {(L2 - L1) / h}')\n\nlol('f')\n\ngrad: 3.9999999999995595\n\n\n\nf.grad = 4\n\n\nlol('d')\n\ngrad: -2.000000000000668\n\n\n\nd.grad = -2\n\nLet’s draw what we have up to this point\n\ndraw_dot(L)\n\n\n\n\nSure, here’s the step by step derivation for each of the variables:\n\nWith respect to a:\n\nGiven that L = (a*b + c) * f, we will apply the product rule for differentiation.\nThe derivative of a*b with respect to a is b, and the derivative of c with respect to a is 0. Therefore:\n\\[\n\\frac{dL}{da} = f \\cdot \\frac{d(a*b + c)}{da} = f \\cdot (b + 0) = b \\cdot f\n\\]\n\nWith respect to b:\n\nThe derivative of a*b with respect to b is a, and the derivative of c with respect to b is 0. Therefore:\n\\[\n\\frac{dL}{db} = f \\cdot \\frac{d(a*b + c)}{db} = f \\cdot (a + 0) = a \\cdot f\n\\]\n\nWith respect to c:\n\nThe derivative of a*b with respect to c is 0, and the derivative of c with respect to c is 1. Therefore:\n\\[\n\\frac{dL}{dc} = f \\cdot \\frac{d(a*b + c)}{dc} = f \\cdot (0 + 1) = f\n\\]\n\nWith respect to f:\n\nThe derivative of (a*b + c) with respect to f is 0, and f is just f, therefore:\n\\[\n\\frac{dL}{df} = (a*b + c) \\cdot \\frac{df}{df} = a*b + c\n\\]\n\nWith respect to e (where e = a*b):\n\nThe derivative of e + c with respect to e is 1. Therefore:\n\\[\n\\frac{dL}{de} = f \\cdot \\frac{d(e + c)}{de} = f \\cdot 1 = f\n\\]\n\nWith respect to d (where d = e + c):\n\nThe derivative of d with respect to d is 1. Therefore:\n\\[\n\\frac{dL}{dd} = f \\cdot \\frac{df}{df} = f\n\\]\n\nlol('e')\n\ngrad: -2.000000000000668\n\n\n\ne.grad = -2 # 1 * d.grad\n\n\nlol('c')\n\ngrad: -1.9999999999988916\n\n\n\nc.grad = -2 # 1 * d.grad\n\n\ndraw_dot(L)\n\n\n\n\n\nlol('a')\n\ngrad: 6.000000000000227\n\n\n\na.grad = 6  # b * e.grad\n\n\nlol('b')\n\ngrad: -3.9999999999995595\n\n\n\nb.grad = -4 # a * e.grad\n\n\ndraw_dot(L)\n\n\n\n\n\nsource\n\n\n\nValue\n\n Value (data, _children=(), _op='', label='')\n\nRepresents a node within a computational graph.\nThis class encapsulates a single value and its relationships in the graph, making it easy to track and manage the value’s dependencies, the operation that produced it, and whether it requires a gradient for backpropagation. It’s central to the functioning of automatic differentiation within deep learning frameworks.\nAttributes: op (Operator) _prev (Set[‘Value’]) cached_data (NDArray) requires_grad (bool)\n\na = Value(2.0, label='a')\nb = Value(-3.0, label='b')\nc = Value(10.0, label='c')\ne = a*b; e.label='e'\nd = e + c; d.label='d'\nf = Value(-2.0, label='f')\nL = d*f; L.label='L' \n\ndraw_dot(L)\n\n\n\n\n\nL.grad = 1\n\n\nL._backward()\n\n\ndraw_dot(L)\n\n\n\n\n\nd._backward()\n\n\ndraw_dot(L)\n\n\n\n\n\nc._backward()\n\nWe expect that nothing will happen\n\ndraw_dot(L)\n\n\n\n\n\ne._backward()\n\n\ndraw_dot(L)\n\n\n\n\nsure enough, exactly as we did before\nWe can do thid process automatically using topo sort algorithms, which’s will give us the correct order on which to call _backward on\n\na = Value(2.0, label='a')\nb = Value(-3.0, label='b')\nc = Value(10.0, label='c')\ne = a*b; e.label='e'\nd = e + c; d.label='d'\nf = Value(-2.0, label='f')\nL = d*f; L.label='L' \n\ndraw_dot(L)\n\n\n\n\n\n# topological order all of the children in the graph\ntopo = []\nvisited = set()\ndef build_topo(v):\n    if v not in visited:\n        visited.add(v)\n        for child in v.children:\n            build_topo(child)\n        topo.append(v)\n\nbuild_topo(L)\n\n\ntopo\n\n[Value(data=-2.0, grad=0),\n Value(data=10.0, grad=0),\n Value(data=2.0, grad=0),\n Value(data=-3.0, grad=0),\n Value(data=-6.0, grad=0),\n Value(data=4.0, grad=0),\n Value(data=-8.0, grad=0)]\n\n\n\n# go one variable at a time and apply the chain rule to get its gradient\nL.grad = 1\nfor v in reversed(topo):\n    v._backward()\n\n\ndraw_dot(L)\n\n\n\n\nSo let’s now update the Value class with this logic\n\nsource\n\n\nValue\n\n Value (data, _children=(), _op='', label='')\n\nRepresents a node within a computational graph.\nThis class encapsulates a single value and its relationships in the graph, making it easy to track and manage the value’s dependencies, the operation that produced it, and whether it requires a gradient for backpropagation. It’s central to the functioning of automatic differentiation within deep learning frameworks.\nAttributes: op (Operator) _prev (Set[‘Value’]) cached_data (NDArray) requires_grad (bool)\n\na = Value(2.0, label='a')\nb = Value(-3.0, label='b')\nc = Value(10.0, label='c')\ne = a*b; e.label='e'\nd = e + c; d.label='d'\nf = Value(-2.0, label='f')\nL = d*f; L.label='L' \n\ndraw_dot(L)\n\n\n\n\n\nL.backward()\n\n\ndraw_dot(L)\n\n\n\n\n\nsource\n\n\nValue\n\n Value (data, children=(), op='', label='')\n\nA class representing a scalar value and its gradient in a computational graph.\nAttributes: - data (float): the scalar value associated with this node - grad (float): the gradient of the output of the computational graph w.r.t. this node’s value - label (str): a label for this node, used for debugging and visualization purposes - _op (str): a string representation of the operation that produced this node in the computational graph - _prev (set of Value objects): the set of nodes that contributed to the computation of this node - _backward (function): a function that computes the gradients of this node w.r.t. its inputs\nMethods: - init(self, data, children=(), op=’‘, label=’’): Initializes a Value object with the given data, children, op, and label - repr(self): Returns a string representation of this Value object - add(self, other): Implements the addition operation between two Value objects - mul(self, other): Implements the multiplication operation between two Value objects - item(self): Returns the scalar value associated with this Value object - tanh(self): Applies the hyperbolic tangent function to this Value object and returns a new Value object\n\nsource\n\n\nall_devices\n\n all_devices ()\n\nreturn a list of all available devices\n\nsource\n\n\ncpu\n\n cpu ()\n\nReturn cpu device\n\nsource\n\n\nCPUDevice\n\n CPUDevice ()\n\nRepresents data that sits in CPU\n\nsource\n\n\nDevice\n\n Device ()\n\nIndicates the device supporting an NDArray.\n\nsource\n\n\nOperator\n\n Operator ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nTensorOp\n\n TensorOp ()\n\nOp class specialized to output tensors, will be alternate subclasses for other structures\n#| export\nclass Value:\n    \"\"\"\n    Represents a node within a computational graph.\n\n    This class encapsulates a single value and its relationships in the graph, making it easy to track and manage the value's dependencies, \n    the operation that produced it, and whether it requires a gradient for backpropagation. It's central to the functioning of automatic \n    differentiation within deep learning frameworks.\n\n    Attributes:\n        op (Operator)\n        _prev (Set['Value']) \n        cached_data (NDArray)\n        requires_grad (bool)\n    \"\"\"\n    def __init__(self,\n                 op: Operator, # The operator that produced this node. If the node was initialized from actual data, this is 'None'.\n                 prev: Set['Value'], # The set of values that this value directly depends on. It's the union of the `_next` sets of all the values in `args`.\n                 cached_data: NDArray, # The actual data for this value. It's `None` for values that aren't yet computed.\n                 requires_grad: bool): # Specifies whether this node requires a gradient. This is `False` for nodes that don't need gradients.\n        \n        self._op = op\n        self.children = op\n        self.cached_data = cached_data\n        self.requires_grad = requires_grad\n\nsource\n\n\nTensor\n\n Tensor (array, device:Optional[__main__.Device]=None, dtype=None,\n         requires_grad=True, **kwargs)\n\nA Tensor represents a multidimensional array of values in a computational graph.\nAttributes: - data: The actual data of the tensor. It is computed lazily. - children: Other tensors that this tensor depends on for computing its value. - requires_grad: Whether this tensor needs to compute gradients.\nMethods: - realize_data: Computes and returns the actual data for this tensor. - shape: Returns the shape of this tensor. - dtype: Returns the data type of this tensor.\nExample: >>> t1 = Tensor([[1.0, 2.0], [3.0, 4.0]]) >>> print(t1.shape) (2, 2) >>> print(t1.dtype) float64\n\nimport numpy as np\nimport unittest\nfrom minima.autograd import Tensor\n\nclass TestTensor(unittest.TestCase):\n    \n    def test_create_tensor(self):\n        t1 = Tensor([1, 2, 3])\n        self.assertTrue(np.array_equal(t1.realize_data(), np.array([1, 2, 3])))\n        self.assertEqual(t1.shape, (3,))\n        self.assertEqual(t1.dtype, np.float64)\n        \n        t2 = Tensor([[1, 2], [3, 4]])\n        self.assertTrue(np.array_equal(t2.realize_data(), np.array([[1, 2], [3, 4]])))\n        self.assertEqual(t2.shape, (2, 2))\n        self.assertEqual(t2.dtype, np.float64)\n        \n        t3 = Tensor(np.array([1, 2, 3]), dtype=np.int32)\n        self.assertTrue(np.array_equal(t3.realize_data(), np.array([1, 2, 3], dtype=np.int32)))\n        self.assertEqual(t3.shape, (3,))\n        self.assertEqual(t3.dtype, np.int32)\n        \n    def test_create_tensor_from_tensor(self):\n        t1 = Tensor([1, 2, 3])\n        t2 = Tensor(t1)\n        self.assertTrue(np.array_equal(t2.realize_data(), np.array([1, 2, 3])))\n        self.assertEqual(t2.shape, (3,))\n        self.assertEqual(t2.dtype, np.float64)\n        \n        t3 = Tensor(np.array([1, 2, 3]), dtype=np.int32)\n        t4 = Tensor(t3)\n        self.assertTrue(np.array_equal(t4.realize_data(), np.array([1, 2, 3], dtype=np.int32)))\n        self.assertEqual(t4.shape, (3,))\n        self.assertEqual(t4.dtype, np.int32)\n        \n    def test_create_tensor_with_device(self):\n        t1 = Tensor([1, 2, 3], device='cpu')\n        self.assertEqual(t1.device, 'cpu')\n        \n        t2 = Tensor([1, 2, 3], device='cuda')\n        self.assertEqual(t2.device, 'cuda')\n        \n    def test_create_tensor_with_requires_grad(self):\n        t1 = Tensor([1, 2, 3], requires_grad=True)\n        self.assertTrue(t1.requires_grad)\n        \n        t2 = Tensor([1, 2, 3], requires_grad=False)\n        self.assertFalse(t2.requires_grad)\n        \n    def test_create_tensor_with_kwargs(self):\n        t1 = Tensor([1, 2, 3], device='cuda', dtype=np.float32, requires_grad=True)\n        self.assertEqual(t1.device, 'cuda')\n        self.assertEqual(t1.dtype, np.float32)\n        self.assertTrue(t1.requires_grad)\n        \n    def test_create_tensor_from_numpy(self):\n        np_array = np.array([1, 2, 3])\n        t1 = Tensor(np_array)\n        self.assertTrue(np.array_equal(t1.realize_data(), np_array))\n        self.assertEqual(t1.shape, (3,))\n        self.assertEqual(t1.dtype, np.float64)\n        \n        np_array = np.array([1, 2, 3], dtype=np.int32)\n        t2 = Tensor(np_array)\n        self.assertTrue(np.array_equal(t2.realize_data(), np_array))\n        self.assertEqual(t2.shape, (3,))\n        self.assertEqual(t2.dtype, np.int32)\n        \n    def test_create_tensor_from_numpy_with_device(self):\n        np_array = np.array([1, 2, 3])\n        t1 = Tensor(np_array, device='cuda')\n        self.assertEqual(t1.device, 'cuda')\n        \n        np_array = np.array([1, 2, 3], dtype=np.int32)\n        t2 = Tensor(np_array, device='cuda')\n        self.assertEqual(t2.device, 'cuda')\n        \n    def test_create_tensor_from_numpy_with_requires_grad(self):\n        np_array = np.array([1, 2, 3])\n        t1 = Tensor(np_array, requires_grad=True)\n        self.assertTrue(t1.requires_grad)\n        \n        np_array = np.array([1, 2, 3], dtype=np.int32)\n        t2 = Tensor(np_array, requires_grad=False)\n        self.assertFalse(t2.requires_grad)\n        \n    def test_create_tensor_from_numpy_with_kwargs(self):\n        np_array = np.array([1, 2, 3])\n        t1 = Tensor(np_array, device='cuda', dtype=np.float32, requires_grad=True)\n        self.assertEqual(t1.device, 'cuda')\n        self.assertEqual(t1.dtype, np.float32)\n        self.assertTrue(t1.requires_grad)\n        \n    def test_create_tensor_from_tensor_with_device(self):\n        t1 = Tensor([1, 2, 3], device='cpu')\n        t2 = Tensor(t1, device='cuda')\n        self.assertEqual(t2.device, 'cuda')\n        \n    def test_create_tensor_from_tensor_with_requires_grad(self):\n        t1 = Tensor([1, 2, 3], requires_grad=True)\n        t2 = Tensor(t1, requires_grad=False)\n        self.assertFalse(t2.requires_grad)\n        \n    def test_create_tensor_from_tensor_with_kwargs(self):\n        t1 = Tensor([1, 2, 3], device='cpu', dtype=np.float32, requires_grad=True)\n        t2 = Tensor(t1, device='cuda', dtype=np.float64, requires_grad=False)\n        self.assertEqual(t2.device, 'cuda')\n        self.assertEqual(t2.dtype, np.float64)\n        self.assertFalse(t2.requires_grad)\n        \n    def test_create_tensor_from_tensor_with_different_device_and_dtype(self):\n        t1 = Tensor([1, 2, 3], device='cpu', dtype=np.float32)\n        t2 = Tensor(t1, device='cuda', dtype=np.float64)\n        self.assertEqual(t2.device, 'cuda')\n        self.assertEqual(t2.dtype, np.float64)\n        self.assertTrue(np.array_equal(t2.realize_data(), np.array([1, 2, 3], dtype=np.float64)))\n        \n    def test_create_tensor_from_tensor_with_same_device_and_dtype(self):\n        t1 = Tensor([1, 2, 3], device='cpu', dtype=np.float32)\n        t2 = Tensor(t1, device='cpu', dtype=np.float32)\n        self.assertEqual(t2.device, 'cpu')\n        self.assertEqual(t2.dtype, np.float32)\n        self.assertTrue(np.array_equal(t2.realize_data(), np.array([1, 2, 3], dtype=np.float32)))\n        \n    def test_create_tensor_from_tensor_with_same_device_and_dtype_and_requires_grad(self):\n        t1 = Tensor([1, 2, 3], device='cpu', dtype=np.float32, requires_grad=True)\n        t2 = Tensor(t1, device='cpu', dtype=np.float32, requires_grad=True)\n        self.assertEqual(t2.device, 'cpu')\n        self.assertEqual(t2.dtype, np.float32)\n        self.assertTrue(np.array_equal(t2.realize_data(), np.array([1, 2, 3], dtype=np.float32)))\n        self.assertTrue(t2.requires_grad)\n        \n    def test_create_tensor_from_tensor_with_same_device_and_dtype_and_requires_grad_false(self):\n        t1 = Tensor([1, 2, 3], device='cpu', dtype=np.float32, requires_grad=False)\n        t2 = Tensor(t1, device='cpu', dtype=np.float32, requires_grad=False)\n        self.assertEqual(t2.device, 'cpu')\n        self.assertEqual(t2.dtype, np.float32)\n        self.assertTrue(np.array_equal(t2.realize_data(), np.array([1, 2, 3], dtype=np.float32)))\n        self.assertFalse(t2.requires_grad)\n        \n    def test_create_tensor_from_tensor_with_same_device_and_dtype_and_requires_grad_true_false(self):\n        t1 = Tensor([1, 2, 3], device='cpu', dtype=np.float32, requires_grad=True)\n        t2 = Tensor(t1, device='cpu', dtype=np.float32, requires_grad=False)\n        self.assertEqual(t2.device, 'cpu')\n        self.assertEqual(t2.dtype, np.float32)\n        self.assertTrue(np.array_equal(t2.realize_data(), np.array([1, 2, 3], dtype=np.float32)))\n        self.assertFalse(t2.requires_grad)\n        \n    def test_create_tensor_from_tensor_with_same_device_and_dtype_and_requires_grad_false_true(self):\n        t1 = Tensor([1, 2, 3], device='cpu', dtype=np.float32, requires_grad=False)\n        t2 = Tensor(t1, device='cpu', dtype=np.float32, requires_grad=True)\n        self.assertEqual(t2.device, 'cpu')\n        self.assertEqual(t2.dtype, np.float32)\n        self.assertTrue(np.array_equal(t2.realize_data(), np.array([1, 2, 3], dtype=np.float32)))\n        self.assertTrue(t2.requires_grad)\n        \n    def test_create_tensor_from_tensor_with_different_device_and_dtype_and_requires_grad(self):\n        t1 = Tensor([1, 2, 3], device='cpu', dtype=np.float32, requires_grad=True)\n        t2 = Tensor(t1, device='cuda', dtype=np.float64, requires_grad=False)\n        self.assertEqual(t2.device, 'cuda')\n        self.assertEqual(t2.dtype, np.float64)\n        self.assertTrue(np.array_equal(t2.realize_data(), np.array([1, 2, 3], dtype=np.float64)))\n        self.assertFalse(t2.requires_grad)\n        \n    def test_create_tensor_from_tensor_with_different_device_and_dtype_and_requires_grad_false(self):\n        t1 = Tensor([1, 2, 3], device='cpu', dtype=np.float32, requires_grad=False)\n        t2 = Tensor(t1, device='cuda', dtype=np.float64, requires_grad=False)\n        self.assertEqual(t2.device, 'cuda')\n        self.assertEqual(t2.dtype, np.float64)\n        self.assertTrue(np.array_equal(t2.realize_data(), np.array([1, 2, 3], dtype=np.float64)))\n        self.assertFalse(t2.requires_grad)\n        \n    def test_create_tensor_from_tensor_with_different_device_and_dtype_and_requires_grad_true_false(self):\n        t1 = Tensor([1, 2, 3], device='cpu', dtype=np.float32, requires_grad=True)\n        t2 = Tensor(t1, device='cuda', dtype=np.float64, requires_grad=False)\n        self.assertEqual(t2.device, 'cuda')\n        self.assertEqual(t2.dtype, np.float64)\n        self.assertTrue(np.array_equal(t2.realize_data(), np.array([1, 2, 3], dtype=np.float64)))\n        self.assertFalse(t2.requires_grad)\n        \n    def test_create_tensor_from_tensor_with_different_device_and_dtype_and_requires_grad_false_true(self):\n        t1 = Tensor([1, 2, 3], device='cpu', dtype=np.float32, requires_grad=False)\n        t2 = Tensor(t1, device='cuda', dtype=np.float64, requires_grad=True)\n        self.assertEqual(t2.device, 'cuda')\n        self.assertEqual(t2.dtype, np.float64)\n        self.assertTrue(np.array_equal(t2.realize_data(), np.array([1, 2, 3], dtype=np.float64)))\n        self.assertTrue(t2.requires_grad)\n        \n    def test_create_tensor_from_tensor_with_same_device_and_dtype_and_requires_grad_true(self):\n        t1 = Tensor([1, 2, 3], device='cpu', dtype=np.float32, requires_grad=True)\n        t2 = Tensor(t1, device='cpu', dtype=np.float32, requires_grad=True)\n        self.assertEqual(t2.device, 'cpu')\n        self.assertEqual(t2.dtype, np.float32)\n        self.assertTrue(np.array_equal(t2.realize_data(), np.array([1, 2, 3], dtype=np.float32)))\n        self.assertTrue(t2.requires_grad)\n        \n    def test_create_tensor_from_tensor_with_same_device_and_dtype_and_requires_grad_false(self):\n        t1 = Tensor([1, 2, 3], device='cpu', dtype=np.float32, requires_grad=False)\n        t2 = Tensor(t1, device='cpu', dtype=np.float32, requires_grad=False)\n        self.assertEqual(t2.device, 'cpu')\n        self.assertEqual(t2.dtype, np.float32)\n        self.assertTrue(np.array_equal(t2.realize_data(), np.array([1, 2, 3], dtype=np.float32)))\n        self.assertFalse(t2.requires_grad)\n        \n    def test_create_tensor_from_tensor_with_same_device_and_dtype_and_requires_grad_true_false(self):\n        t1 = Tensor([1, 2, 3], device='cpu', dtype=np.float32, requires_grad=True)\n        t2 = Tensor(t1, device='cpu', dtype=np.float32, requires_grad=False)\n        self.assertEqual(t2.device, 'cpu')\n        self.assertEqual(t2.dtype, np.float32)\n        self.assertTrue(np.array_equal(t2.realize_data(), np.array([1, 2, 3], dtype=np.float32)))\n        self.assertFalse(t2.requires_grad)\n        \n    def test_create_tensor_from_tensor_with_same_device_and_dtype_and_requires_grad_false_true(self):\n        t1 = Tensor([1, 2, 3], device='cpu', dtype=np.float32, requires_grad=False)\n        t2 = Tensor(t1, device='cpu', dtype=np.float32, requires_grad=True)\n        self.assertEqual(t2.device, 'cpu')\n        self.assertEqual(t2.dtype, np.float32)\n        self.assertTrue(np.array_equal(t2.realize_data(), np.array([1, 2, 3], dtype=np.float32)))\n        self.assertTrue(t2.requires_grad)\n        \n    def test_create_tensor_from_tensor_with_different_device_and_dtype_and_requires_grad_true(self):\n        t1 = Tensor([1, 2, 3], device='cpu', dtype=np.float32, requires_grad=True)\n        t2 = Tensor(t1, device='cuda', dtype=np.float64, requires_grad=True)\n        self.assertEqual(t2.device, 'cuda')\n        self.assertEqual(t2.dtype, np.float64)\n        self.assertTrue(np.array_equal(t2.realize_data(), np.array([1, 2, 3], dtype=np.float64)))\n        self.assertTrue(t2.requires_grad)\n        \n    def test_create_tensor_from_tensor_with_different_device_and_dtype_and_requires_grad_false(self):\n        t1 = Tensor([1, 2, 3], device='cpu', dtype=np.float32, requires_grad=False)\n        t2 = Tensor(t1, device='cuda', dtype=np.float64, requires_grad=False)\n        self.assertEqual(t2.device, 'cuda')\n        self.assertEqual(t2.dtype, np.float64)\n        self.assertTrue(np.array_equal(t2.realize_data(), np.array([1, 2, 3], dtype=np.float64)))\n        self.assertFalse(t2.requires_grad)\n        \n    def test_create_tensor_from_tensor_with_different_device_and_dtype_and_requires_grad_true_false(self):\n        t1 = Tensor([1, 2, 3], device='cpu', dtype=np.float32, requires_grad=True)\n        t2 = Tensor(t1, device='cuda', dtype=np.float64, requires_grad=False)\n        self.assertEqual(t2.device, 'cuda')\n        self.assertEqual(t2.dtype, np.float64)\n        self.assertTrue(np.array_equal(t2.realize_data(), np.array([1, 2, 3], dtype=np.float64)))\n        self.assertFalse(t2.requires_grad)\n        \n    def test_create_tensor_from_tensor_with_different_device_and_dtype_and_requires_grad_false_true(self):\n        t1 = Tensor([1, 2, 3], device='cpu', dtype=np.float32, requires_grad=False)\n        t2 = Tensor(t1, device='cuda', dtype=np.float64, requires_grad=True)\n        self.assertEqual(t2.device, 'cuda')\n        self.assertEqual(t2.dtype, np.float64)\n        self.assertTrue(np.array_equal(t2.realize_data(), np.array([1, 2, 3], dtype=np.float64)))\n        self.assertTrue(t2.requires_grad)\n\n\nimport nbdev; nbdev.nbdev_export()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to minima",
    "section": "",
    "text": "minima is a lightweight deep learning framewor, lean yet effective tailor-made for educational exploration.\nJust like a delicate sapling inspired by the towering strength of an oak, Minima draws its inspiration from PyTorch.\nYet, it carves its own identity with a straightforward interface and a curated set of features.\nThis makes learning and using it a breeze, allowing you to effortlessly build and train neural networks.\nIndeed, Minima is your friendly companion on the journey to understanding deep learning, where less is often more.\n\n\n\nYou can install minima on your own machines with conda\nIf you’re using miniconda (recommended) then run:\nconda install minima\n…or if you’re using Anaconda then run:\nconda install minima anaconda\nTo install with pip, use: pip install minima.\nIf you plan to develop Minima yourself, or want to be on the cutting edge, you can use an editable install.\ngit clone https://github.com/m0saan/minima\npip install .\n\n\n\n\nEasy to install and use\nSimple and intuitive API for defining and training neural networks\nBuilt-in support for common layers and activation functions\nSupports both CPU and GPU acceleration\nCompatible with NumPy arrays for easy data manipulation\n\n\n\n\nHere’s a simple example of how to define and train a neural network using Minima:\nimport minima as mi\n\n# Define the neural network architecture\nmodel = mi.Sequential(\n    mi.Linear(784, 128),\n    mi.ReLU(),\n    mi.Linear(128, 10),\n    mi.Softmax()\n)\n\n# Load the dataset\nx_train, y_train, x_test, y_test = load_data()\n\n# Train the model\nloss_fn = mi.CrossEntropyLoss()\noptimizer = mi.SGD(model.parameters(), lr=0.01)\nfor epoch in range(10):\n    for x_batch, y_batch in minibatch(x_train, y_train, batch_size=32):\n        y_pred = model(x_batch)\n        loss = loss_fn(y_pred, y_batch)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n# Evaluate the model\ny_pred = model(x_test)\naccuracy = compute_accuracy(y_pred, y_test)\nprint(f\"Accuracy: {accuracy:.2f}\")\nThis example defines a simple neural network with two linear layers and two activation functions, trains it on a dataset using stochastic gradient descent, and evaluates its accuracy on a test set.\n\n\n\nFor more information on how to use minima, please refer to the documentation, which can be found in the website above.\n\n\n\ncomming soon!\n\n\n\nminima is released under the Apache License 2.0. See LICENSE for more information."
  }
]