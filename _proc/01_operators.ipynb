{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Fill in a module description here\n",
    "output-file: operators.html\n",
    "title: operators\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9a4956-5575-4950-9365-560225c3a715",
   "metadata": {},
   "source": [
    "The `out_grad` parameter refers to the gradient of the loss function with respect to the output of the node. Multiplying this with the local gradient gives the gradient of the loss with respect to the input to the node, according to the chain rule of calculus, which is the basis for backpropagation in neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c0867e-7744-4c76-95b6-da3868dc8625",
   "metadata": {},
   "source": [
    "The chain rule is a fundamental concept in calculus that provides a method to compute the derivative of composite functions. In simple terms, the chain rule states that the derivative of a composite function is the derivative of the outer function multiplied by the derivative of the inner function.\n",
    "\n",
    "Given a composite function that is the composition of two functions, say, $f(g(x))$, the chain rule can be stated as follows:\n",
    "\n",
    "$$\\frac{df}{dx} = \\frac{df}{dg} \\cdot \\frac{dg}{dx}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\frac{df}{dx}$ is the derivative of the composite function $f(g(x))$ with respect to $x$,\n",
    "- $\\frac{df}{dg}$ is the derivative of the outer function $f$ with respect to its argument $g(x)$, and\n",
    "- $\\frac{dg}{dx}$ is the derivative of the inner function $g(x)$ with respect to $x$.\n",
    "\n",
    "The chain rule can be extended to the case where we have more than two composite functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cb2320-c471-426d-ad48-0197b1daecaa",
   "metadata": {},
   "source": [
    "## Element Wise Addition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feadcd15-44d4-4a3d-aef9-39b3b7f6fcd1",
   "metadata": {},
   "source": [
    "Let's walk through the step-by-step derivative calculation for the [`EWiseAdd`](https://m0saan.github.io/minima/operators.html#ewiseadd) operation:\n",
    "\n",
    "We have the function `f(a, b) = a + b`, where `a` and `b` are tensors. Our goal is to compute the partial derivatives with respect to `a` and `b`.\n",
    "\n",
    "Let's start by calculating the derivative of `f` with respect to `a`, denoted as `df/da`:\n",
    "\n",
    "Step 1: Compute the derivative of `f` with respect to `a`.\n",
    "\n",
    "$\\frac{{\\partial f}}{{\\partial a}} = \\frac{{\\partial}}{{\\partial a}} (a + b)$\n",
    "\n",
    "Since `a` is the variable we are differentiating with respect to, the derivative of `a` with respect to itself is 1:\n",
    "\n",
    "$$\\frac{{\\partial f}}{{\\partial a}} = 1$$\n",
    "\n",
    "Therefore, $$\\frac{{\\partial f}}{{\\partial a}} = 1.$$\n",
    "\n",
    "Step 2: Compute the derivative of `f` with respect to `b`.\n",
    "\n",
    "$$\\frac{{\\partial f}}{{\\partial b}} = \\frac{{\\partial}}{{\\partial b}} (a + b)$$\n",
    "\n",
    "Again, since `b` is the variable we are differentiating with respect to, the derivative of `b` with respect to itself is 1:\n",
    "\n",
    "$$\\frac{{\\partial f}}{{\\partial b}} = 1$$\n",
    "\n",
    "Therefore, $$\\frac{{\\partial f}}{{\\partial b}} = 1$$\n",
    "\n",
    "Hence, the partial derivatives of `f(a, b) = a + b` with respect to `a` and `b` are both equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L61){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### add\n",
       "\n",
       ">      add (a:minima.autograd.Tensor, b:minima.autograd.Tensor)\n",
       "\n",
       "Adds two tensors element-wise.\n",
       "\n",
       "Args:\n",
       "- a: The first tensor.\n",
       "- b: The second tensor.\n",
       "\n",
       "Returns:\n",
       "The element-wise sum of a and b."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L61){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### add\n",
       "\n",
       ">      add (a:minima.autograd.Tensor, b:minima.autograd.Tensor)\n",
       "\n",
       "Adds two tensors element-wise.\n",
       "\n",
       "Args:\n",
       "- a: The first tensor.\n",
       "- b: The second tensor.\n",
       "\n",
       "Returns:\n",
       "The element-wise sum of a and b."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L22){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### EWiseAdd\n",
       "\n",
       ">      EWiseAdd ()\n",
       "\n",
       "Performs element-wise addition of two tensors.\n",
       "\n",
       "Example:\n",
       ">>> a = Tensor([1, 2, 3])\n",
       ">>> b = Tensor([4, 5, 6])\n",
       ">>> op = EWiseAdd()\n",
       ">>> result = op.compute(a, b)\n",
       ">>> print(result)\n",
       "Tensor([5, 7, 9])"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L22){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### EWiseAdd\n",
       "\n",
       ">      EWiseAdd ()\n",
       "\n",
       "Performs element-wise addition of two tensors.\n",
       "\n",
       "Example:\n",
       ">>> a = Tensor([1, 2, 3])\n",
       ">>> b = Tensor([4, 5, 6])\n",
       ">>> op = EWiseAdd()\n",
       ">>> result = op.compute(a, b)\n",
       ">>> print(result)\n",
       "Tensor([5, 7, 9])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(EWiseAdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358fcf64-9c44-4d44-8374-a0ef11668d6e",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Create two 1-D tensors\n",
    "a = Tensor([1, 2, 3])\n",
    "b = Tensor([4, 5, 6])\n",
    "\n",
    "# Create an EWiseAdd operation\n",
    "op = EWiseAdd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd371e81-b6e4-43da-9987-30057c5c038a",
   "metadata": {},
   "source": [
    "## Scalar Addition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dbc8dc-25ae-4793-9cc8-cdbef59a8400",
   "metadata": {},
   "source": [
    "Explanation for the derivative of the [`AddScalar`](https://m0saan.github.io/minima/operators.html#addscalar) operator:\n",
    "\n",
    "Let's denote the scalar as `c` and `a` as the tensor being added by the scalar. The operation can be described as `f(a) = a + c`.\n",
    "\n",
    "The function for the backward pass (i.e., the gradient) is `df/da = 1`, which means the derivative of `f(a)` with respect to `a` is simply `1`.\n",
    "\n",
    "We are given a function $f(a) = a + c$, where $a$ is a tensor and $c$ is a scalar. Our task is to find the derivative of this function with respect to $a$.\n",
    "\n",
    "By differentiating the function $f(a)$ with respect to $a$, we find:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{da} &= \\frac{d}{da} (a + c) \\\\\n",
    "&= 1\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, the gradient of $f(a)$ with respect to $a$ is $1$.\n",
    "\n",
    "\n",
    "We starts by defining the function `f(a) = a + c`. It then explains that when we differentiate `f(a)` with respect to `a`, we find that the derivative is `1`. This means that the gradient of `f(a)` with respect to `a` is `1`, which matches the behavior of the [`AddScalar`](https://m0saan.github.io/minima/operators.html#addscalar) operator as provided in the `gradient` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L120){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### add_scalar\n",
       "\n",
       ">      add_scalar (a:minima.autograd.Tensor, scalar:Union[int,float])\n",
       "\n",
       "Adds a scalar to a tensor.\n",
       "\n",
       "Args:\n",
       "- a: The tensor.\n",
       "- scalar: The scalar to add.\n",
       "\n",
       "Returns:\n",
       "The sum of a and the scalar."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L120){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### add_scalar\n",
       "\n",
       ">      add_scalar (a:minima.autograd.Tensor, scalar:Union[int,float])\n",
       "\n",
       "Adds a scalar to a tensor.\n",
       "\n",
       "Args:\n",
       "- a: The tensor.\n",
       "- scalar: The scalar to add.\n",
       "\n",
       "Returns:\n",
       "The sum of a and the scalar."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(add_scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L75){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AddScalar\n",
       "\n",
       ">      AddScalar (scalar:Union[int,float])\n",
       "\n",
       "Performs addition of a tensor and a scalar.\n",
       "\n",
       "Example:\n",
       ">>> a = Tensor([1, 2, 3])\n",
       ">>> op = AddScalar(5)\n",
       ">>> result = op.compute(a)\n",
       ">>> print(result)\n",
       "Tensor([6, 7, 8])"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L75){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AddScalar\n",
       "\n",
       ">      AddScalar (scalar:Union[int,float])\n",
       "\n",
       "Performs addition of a tensor and a scalar.\n",
       "\n",
       "Example:\n",
       ">>> a = Tensor([1, 2, 3])\n",
       ">>> op = AddScalar(5)\n",
       ">>> result = op.compute(a)\n",
       ">>> print(result)\n",
       "Tensor([6, 7, 8])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(AddScalar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26ed99f-b3f2-4df1-9918-ff23fc99be74",
   "metadata": {},
   "source": [
    "## Element Wise Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf5a7d0-8e8a-47cd-a4b5-cdc12a03649c",
   "metadata": {},
   "source": [
    "Explanation for the derivative of the [`EWiseMul`](https://m0saan.github.io/minima/operators.html#ewisemul) (element-wise multiplication) operator:\n",
    "\n",
    "Let's denote the two input tensors as `a` and `b`. The operation can be described as `f(a, b) = a * b`, where `*` represents element-wise multiplication.\n",
    "\n",
    "The function for the backward pass (i.e., the gradient) is `df/da = b` and `df/db = a`. This means that the derivative of `f(a, b)` with respect to `a` is `b`, and the derivative with respect to `b` is `a`.\n",
    "\n",
    "\n",
    "We are given a function $f(a, b) = a \\odot b$, where $a$ and $b$ are tensors, and $\\odot$ represents element-wise multiplication. Our task is to find the derivatives of this function with respect to $a$ and $b$.\n",
    "\n",
    "By differentiating the function $f(a, b)$ with respect to $a$, we find:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{da} &= \\frac{d}{da} (a \\odot b) \\\\\n",
    "&= b\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, the gradient of $f(a, b)$ with respect to $a$ is $b$.\n",
    "\n",
    "Similarly, by differentiating the function $f(a, b)$ with respect to $b$, we find:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{db} &= \\frac{d}{db} (a \\odot b) \\\\\n",
    "&= a\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, the gradient of $f(a, b)$ with respect to $b$ is $a$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L173){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### multiply\n",
       "\n",
       ">      multiply (a:minima.autograd.Tensor, b:minima.autograd.Tensor)\n",
       "\n",
       "Multiplies two tensors element-wise.\n",
       "\n",
       "Args:\n",
       "- a: The first tensor.\n",
       "- b: The second tensor.\n",
       "\n",
       "Returns:\n",
       "The element-wise product of a and b."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L173){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### multiply\n",
       "\n",
       ">      multiply (a:minima.autograd.Tensor, b:minima.autograd.Tensor)\n",
       "\n",
       "Multiplies two tensors element-wise.\n",
       "\n",
       "Args:\n",
       "- a: The first tensor.\n",
       "- b: The second tensor.\n",
       "\n",
       "Returns:\n",
       "The element-wise product of a and b."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(multiply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L134){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### EWiseMul\n",
       "\n",
       ">      EWiseMul ()\n",
       "\n",
       "Performs element-wise multiplication of two tensors.\n",
       "\n",
       "Example:\n",
       ">>> a = Tensor([1, 2, 3])\n",
       ">>> b = Tensor([4, 5, 6])\n",
       ">>> op = EWiseMul()\n",
       ">>> result = op.compute(a, b)\n",
       ">>> print(result)\n",
       "Tensor([4, 10, 18])"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L134){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### EWiseMul\n",
       "\n",
       ">      EWiseMul ()\n",
       "\n",
       "Performs element-wise multiplication of two tensors.\n",
       "\n",
       "Example:\n",
       ">>> a = Tensor([1, 2, 3])\n",
       ">>> b = Tensor([4, 5, 6])\n",
       ">>> op = EWiseMul()\n",
       ">>> result = op.compute(a, b)\n",
       ">>> print(result)\n",
       "Tensor([4, 10, 18])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(EWiseMul)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb531c5-f22c-40c9-901e-e373b837a846",
   "metadata": {},
   "source": [
    "## Scalar Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0246b9-ce8f-4fab-991d-7ec43745c2ea",
   "metadata": {},
   "source": [
    "Let's denote the scalar as `c` and `a` as the tensor being multiplied by the scalar. The operation can be described as `f(a) = a * c`.\n",
    "\n",
    "The function for the backward pass (i.e., the gradient) is `df/da = c`, which means the derivative of `f(a)` with respect to `a` is `c`.\n",
    "\n",
    "The LaTeX document will look as follows:\n",
    "\n",
    "We are given a function $f(a) = a \\cdot c$, where $a$ is a tensor and $c$ is a scalar. Our task is to find the derivative of this function with respect to $a$.\n",
    "\n",
    "By differentiating the function $f(a)$ with respect to $a$, we find:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{da} &= \\frac{d}{da} (a \\cdot c) \\\\\n",
    "&= c\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, the gradient of $f(a)$ with respect to $a$ is $c$.\n",
    "\n",
    "We starts by defining the function `f(a) = a * c`. It then explains that when we differentiate `f(a)` with respect to `a`, we find that the derivative is `c`. This means that the gradient of `f(a)` with respect to `a` is `c`, which matches the behavior of the [`MulScalar`](https://m0saan.github.io/minima/operators.html#mulscalar) operator as provided in the `gradient` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L232){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### mul_scalar\n",
       "\n",
       ">      mul_scalar (a:minima.autograd.Tensor, scalar:Union[int,float])\n",
       "\n",
       "Multiplies a tensor by a scalar.\n",
       "\n",
       "Args:\n",
       "- a: The tensor.\n",
       "- scalar: The scalar to multiply.\n",
       "\n",
       "Returns:\n",
       "The product of a and the scalar."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L232){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### mul_scalar\n",
       "\n",
       ">      mul_scalar (a:minima.autograd.Tensor, scalar:Union[int,float])\n",
       "\n",
       "Multiplies a tensor by a scalar.\n",
       "\n",
       "Args:\n",
       "- a: The tensor.\n",
       "- scalar: The scalar to multiply.\n",
       "\n",
       "Returns:\n",
       "The product of a and the scalar."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(mul_scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L187){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MulScalar\n",
       "\n",
       ">      MulScalar (scalar:Union[int,float])\n",
       "\n",
       "Performs multiplication of a tensor and a scalar.\n",
       "\n",
       "Example:\n",
       ">>> a = Tensor([1, 2, 3])\n",
       ">>> op = MulScalar(5)\n",
       ">>> result = op.compute(a)\n",
       ">>> print(result)\n",
       "Tensor([5, 10, 15])"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L187){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MulScalar\n",
       "\n",
       ">      MulScalar (scalar:Union[int,float])\n",
       "\n",
       "Performs multiplication of a tensor and a scalar.\n",
       "\n",
       "Example:\n",
       ">>> a = Tensor([1, 2, 3])\n",
       ">>> op = MulScalar(5)\n",
       ">>> result = op.compute(a)\n",
       ">>> print(result)\n",
       "Tensor([5, 10, 15])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(MulScalar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6497d8ab-3003-4784-a330-ad5f862b9ca5",
   "metadata": {},
   "source": [
    "## Negation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04085053-1446-4343-b720-17e04a1c4ee1",
   "metadata": {},
   "source": [
    "Let's denote `a` as the tensor being negated. The operation can be described as `f(a) = -a`.\n",
    "\n",
    "The function for the backward pass (i.e., the gradient) is `df/da = -1`.\n",
    "\n",
    "We are given a function $f(a) = -a$, where $a$ is a tensor. Our task is to find the derivative of this function with respect to $a$.\n",
    "\n",
    "By differentiating the function $f(a)$ with respect to $a$, we find:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{da} &= \\frac{d}{da} (-a) \\\\\n",
    "&= -1\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, the gradient of $f(a)$ with respect to $a$ is $-1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defd870b-d2e6-4212-bd4b-b3333d271c9e",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "class Negate(TensorOp):\n",
    "    \"\"\"\n",
    "    Negates the given tensor.\n",
    "    \n",
    "    Example:\n",
    "    >>> a = Tensor([1, -2, 3])\n",
    "    >>> op = Negate()\n",
    "    >>> result = op.compute(a)\n",
    "    >>> print(result)\n",
    "    Tensor([-1, 2, -3])\n",
    "    \"\"\"\n",
    "    \n",
    "    def compute(self, a: NDArray) -> NDArray:\n",
    "        \"\"\"\n",
    "        Computes the negation of a tensor.\n",
    "\n",
    "        Args:\n",
    "        - a: The tensor to negate.\n",
    "\n",
    "        Returns:\n",
    "        The negation of a.\n",
    "        \"\"\"\n",
    "        return -1 * a\n",
    "\n",
    "    def gradient(self, out_grad: Tensor, node: Tensor) -> Tuple[Tensor,]:\n",
    "        \"\"\"\n",
    "        Computes the gradient of the negation operation.\n",
    "\n",
    "        Args:\n",
    "        - out_grad: The gradient of the output of the operation.\n",
    "        - node: The node in the computational graph where the operation was performed.\n",
    "\n",
    "        Returns:\n",
    "        The gradients with respect to the inputs.\n",
    "        \"\"\"\n",
    "        return (negate(out_grad), )\n",
    "\n",
    "\n",
    "def negate(a: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Negates the given tensor.\n",
    "\n",
    "    Args:\n",
    "    - a: The tensor to negate.\n",
    "\n",
    "    Returns:\n",
    "    The negation of a.\n",
    "    \n",
    "    Example:\n",
    "    >>> a = Tensor([1, -2, 3])\n",
    "    >>> result = negate(a)\n",
    "    >>> print(result)\n",
    "    Tensor([-1, 2, -3])\n",
    "    \"\"\"\n",
    "    return Negate()(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e28e521-a653-45e0-a567-46c7b800d281",
   "metadata": {},
   "source": [
    "## Exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8bf43a-9a2a-4077-a54f-54df0c6c955d",
   "metadata": {},
   "source": [
    "Explanation for the derivative of the `Exp` operator:\n",
    "\n",
    "Let's denote `a` as the tensor on which the exponential function is applied. The operation can be described as `f(a) = exp(a)`, where `exp` represents the exponential function.\n",
    "\n",
    "The function for the backward pass (i.e., the gradient) is `df/da = exp(a)`.\n",
    "\n",
    "We are given a function $f(a) = \\exp(a)$, where $a$ is a tensor. Our task is to find the derivative of this function with respect to $a$.\n",
    "\n",
    "By differentiating the function $f(a)$ with respect to $a$, we find:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{da} &= \\frac{d}{da} (\\exp(a)) \\\\\n",
    "&= \\exp(a)\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, the gradient of $f(a)$ with respect to $a$ is $\\exp(a)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a810bf4e-1d48-412b-bf89-2a6fde789ae4",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "class Exp(TensorOp):\n",
    "    \"\"\"\n",
    "    Calculates the exponential of the given tensor.\n",
    "    \n",
    "    Example:\n",
    "    >>> a = Tensor([1, 2, 3])\n",
    "    >>> op = Exp()\n",
    "    >>> result = op.compute(a)\n",
    "    >>> print(result)\n",
    "    Tensor([2.71828183, 7.3890561, 20.08553692])\n",
    "    \"\"\"\n",
    "    \n",
    "    def compute(self, a: NDArray) -> NDArray:\n",
    "        \"\"\"\n",
    "        Computes the exponential of a tensor.\n",
    "\n",
    "        Args:\n",
    "        - a: The tensor.\n",
    "\n",
    "        Returns:\n",
    "        The exponential of a.\n",
    "        \"\"\"\n",
    "        self.out = array_api.exp(a)\n",
    "        return self.out\n",
    "\n",
    "    def gradient(self, out_grad: Tensor, node: Tensor) -> Tuple[Tensor,]:\n",
    "        \"\"\"\n",
    "        Computes the gradient of the exponential operation.\n",
    "\n",
    "        Args:\n",
    "        - out_grad: The gradient of the output of the operation.\n",
    "        - node: The node in the computational graph where the operation was performed.\n",
    "\n",
    "        Returns:\n",
    "        The gradients with respect to the inputs.\n",
    "        \"\"\"\n",
    "        return (out_grad * self.out, )\n",
    "\n",
    "def exp(a: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Calculates the exponential of the given tensor.\n",
    "\n",
    "    Args:\n",
    "    - a: The tensor.\n",
    "\n",
    "    Returns:\n",
    "    The exponential of a.\n",
    "    \n",
    "    Example:\n",
    "    >>> a = Tensor([1, 2, 3])\n",
    "    >>> result = exp(a)\n",
    "    >>> print(result)\n",
    "    Tensor([2.71828183, 7.3890561, 20.08553692])\n",
    "    \"\"\"\n",
    "    return Exp()(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45592e37-9a6d-42ec-8458-167a94394cc1",
   "metadata": {},
   "source": [
    "## ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8aefd6-23e2-4df7-8627-f74813b8f0bc",
   "metadata": {},
   "source": [
    "The derivative of the `ReLU` (Rectified Linear Unit) operator:\n",
    "\n",
    "Let's denote `a` as the tensor on which the ReLU function is applied. The ReLU function is defined as follows: \n",
    "\n",
    "$$\n",
    "f(a) = \n",
    "\\begin{cases}\n",
    "a, & \\text{if } a \\geq 0 \\\\\n",
    "0, & \\text{if } a < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The function for the backward pass (i.e., the gradient) is `df/da = 1` if `a >= 0`, and `df/da = 0` if `a < 0`.\n",
    "\n",
    "We are given a function $f(a) = \\max(0, a)$, where $a$ is a tensor. Our task is to find the derivative of this function with respect to $a$.\n",
    "\n",
    "By considering the definition of the ReLU function, we can write $f(a)$ as:\n",
    "\n",
    "$$\n",
    "f(a) = \n",
    "\\begin{cases}\n",
    "a, & \\text{if } a \\geq 0 \\\\\n",
    "0, & \\text{if } a < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Now, let's differentiate $f(a)$ with respect to $a$:\n",
    "\n",
    "$$\n",
    "\\frac{df}{da} = \n",
    "\\begin{cases}\n",
    "1, & \\text{if } a \\geq 0 \\\\\n",
    "0, & \\text{if } a < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Therefore, the gradient of $f(a)$ with respect to $a$ is $1$ if $a \\geq 0$, and $0$ if $a < 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32b2ce4-7d9e-4290-90d9-a56bc59cfe2a",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "class ReLU(TensorOp):\n",
    "    \"\"\"\n",
    "    Applies the ReLU (Rectified Linear Unit) activation function to the given tensor.\n",
    "    \n",
    "    Example:\n",
    "    >>> a = Tensor([1, -2, 3])\n",
    "    >>> op = ReLU()\n",
    "    >>> result = op.compute(a)\n",
    "    >>> print(result)\n",
    "    Tensor([1, 0, 3])\n",
    "    \"\"\"\n",
    "    \n",
    "    def compute(self, a: NDArray) -> NDArray:\n",
    "        \"\"\"\n",
    "        Computes the ReLU activation function on a tensor.\n",
    "\n",
    "        Args:\n",
    "        - a: The tensor.\n",
    "\n",
    "        Returns:\n",
    "        The result of applying ReLU to a.\n",
    "        \"\"\"\n",
    "        self.out = array_api.clip(a, a_min=0)\n",
    "        return self.out\n",
    "\n",
    "    def gradient(self, out_grad: Tensor, node: Tensor) -> Tuple[Tensor,]:\n",
    "        \"\"\"\n",
    "        Computes the gradient of the ReLU operation.\n",
    "\n",
    "        Args:\n",
    "        - out_grad: The gradient of the output of the operation.\n",
    "        - node: The node in the computational graph where the operation was performed.\n",
    "\n",
    "        Returns:\n",
    "        The gradients with respect to the inputs.\n",
    "        \"\"\"\n",
    "        return (out_grad * Tensor(node.children[0] >= 0), )\n",
    "\n",
    "def relu(a: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Applies the ReLU (Rectified Linear Unit) activation function to the given tensor.\n",
    "\n",
    "    Args:\n",
    "    - a: The tensor.\n",
    "\n",
    "    Returns:\n",
    "    The result of applying ReLU to a.\n",
    "    \n",
    "    Example:\n",
    "    >>> a = Tensor([1, -2, 3])\n",
    "    >>> result = relu(a)\n",
    "    >>> print(result)\n",
    "    Tensor([1, 0, 3])\n",
    "    \"\"\"\n",
    "    return ReLU()(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9f074a-e23c-4e8a-8ab7-2cfba344c461",
   "metadata": {},
   "source": [
    "## Power Scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d5e875-0981-4243-86b5-d6ca6b117d5e",
   "metadata": {},
   "source": [
    "The derivative of the `PowerScalar` operator:\n",
    "\n",
    "Let's denote the scalar as `n` and `a` as the tensor being raised to the power of the scalar. The operation can be described as `f(a) = a^n`.\n",
    "\n",
    "The function for the backward pass (i.e., the gradient) is `df/da = n * a^(n-1)`.\n",
    "\n",
    "We are given a function $f(a) = a^n$, where $a$ is a tensor and $n$ is a scalar. Our task is to find the derivative of this function with respect to $a$.\n",
    "\n",
    "By differentiating the function $f(a)$ with respect to $a$, we find:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{da} &= \\frac{d}{da} (a^n) \\\\\n",
    "&= n \\cdot a^{n-1}\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, the gradient of $f(a)$ with respect to $a$ is $n \\cdot a^{n-1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364103cb-615b-4359-8061-5a8bd1455367",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "class PowerScalar(TensorOp):\n",
    "    \"\"\"\n",
    "    The PowerScalar operation raises a tensor to an (integer) power.\n",
    "\n",
    "    Attributes:\n",
    "        scalar (int): The power to raise the tensor to.\n",
    "\n",
    "    Example:\n",
    "        >>> import numpy as np\n",
    "        >>> tensor = Tensor(np.array([1, 2, 3]))\n",
    "        >>> pow_scalar = PowerScalar(2)\n",
    "        >>> result = pow_scalar.compute(tensor.data)\n",
    "        >>> print(result)\n",
    "        array([1, 4, 9])\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scalar: int):\n",
    "        \"\"\"\n",
    "        Constructs the PowerScalar operation.\n",
    "\n",
    "        Args:\n",
    "            scalar (int): The power to raise the tensor to.\n",
    "        \"\"\"\n",
    "        self.scalar = scalar\n",
    "\n",
    "    def compute(self, a: NDArray) -> NDArray:\n",
    "        \"\"\"\n",
    "        Computes the power operation on the input tensor.\n",
    "\n",
    "        Args:\n",
    "            a (NDArray): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            NDArray: The resulting tensor after the power operation.\n",
    "        \"\"\"\n",
    "        return array_api.power(a, self.scalar)\n",
    "\n",
    "    def gradient(self, out_grad: Tensor, node: Tensor) -> Tuple[Tensor, ]:\n",
    "        \"\"\"\n",
    "        Computes the gradient of the power operation.\n",
    "\n",
    "        Args:\n",
    "            out_grad (Tensor): The gradient of the output tensor.\n",
    "            node (Tensor): The node in the computational graph where the operation was performed.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tensor, ]: The gradient with respect to the input tensor.\n",
    "        \"\"\"\n",
    "        a = node.children[0]\n",
    "        return (self.scalar * power_scalar(a, self.scalar - 1) * out_grad, )\n",
    "\n",
    "\n",
    "def power_scalar(a: Tensor, scalar: int) -> Tensor:\n",
    "    \"\"\"\n",
    "    Raises a tensor to a power.\n",
    "\n",
    "    Args:\n",
    "        a (Tensor): The input tensor.\n",
    "        scalar (int): The power to raise the tensor to.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: The resulting tensor after the power operation.\n",
    "\n",
    "    Example:\n",
    "        >>> import numpy as np\n",
    "        >>> tensor = Tensor(np.array([1, 2, 3]))\n",
    "        >>> result = power_scalar(tensor, 2)\n",
    "        >>> print(result)\n",
    "        Tensor([1, 4, 9])\n",
    "    \"\"\"\n",
    "    return PowerScalar(scalar)(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbb40d3-1ed4-4a7f-9d74-b39b70187860",
   "metadata": {},
   "source": [
    "## Element Wise Divide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a7c7e2-d71d-49c0-9c45-ffc7f0830334",
   "metadata": {},
   "source": [
    "The operation described here is an element-wise division of two tensors, `a` and `b`, where the operation can be described as `f(a, b) = a / b`. \n",
    "\n",
    "We'll compute the partial derivatives with respect to `a` and `b`:\n",
    "\n",
    "1. The partial derivative of `f(a, b)` with respect to `a` (`df/da`) is `1/b`.\n",
    "\n",
    "2. The partial derivative of `f(a, b)` with respect to `b` (`df/db`) is `-a / b^2`.\n",
    "\n",
    "We are given a function $f(a, b) = \\frac{a}{b}$, where $a$ and $b$ are tensors. Our task is to find the partial derivatives of this function with respect to $a$ and $b$.\n",
    "\n",
    "Let's start with $\\frac{\\partial f}{\\partial a}$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial f}{\\partial a} &= \\frac{\\partial}{\\partial a} \\left(\\frac{a}{b}\\right) \\\\\n",
    "&= \\frac{1}{b}\n",
    "\\end{align*}\n",
    "\n",
    "Now, let's compute $\\frac{\\partial f}{\\partial b}$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial f}{\\partial b} &= \\frac{\\partial}{\\partial b} \\left(\\frac{a}{b}\\right) \\\\\n",
    "&= - \\frac{a}{b^{2}}\n",
    "\\end{align*}\n",
    "\n",
    "Here is a detailed derivative:\n",
    "\n",
    "Given a function of the form $y = \\frac{u}{v}$, where both $u$ and $v$ are functions of $x$, the quotient rule of differentiation states:\n",
    "\n",
    "$$\\frac{dy}{dx} = \\frac{v \\cdot \\frac{du}{dx} - u \\cdot \\frac{dv}{dx}}{v^2}$$\n",
    "\n",
    "In our case, we're looking at the function $y = \\frac{a}{b}$, where $a$ and $b$ are tensors. We want to find the derivative with respect to $b$ (instead of $x$ in our general formula). So we have:\n",
    "\n",
    "$$\\frac{dy}{db} = \\frac{b \\cdot \\frac{da}{db} - a \\cdot \\frac{db}{db}}{b^2}$$\n",
    "\n",
    "Since $a$ does not depend on $b$, $\\frac{da}{db} = 0$, and since any variable is equal to itself, $\\frac{db}{db} = 1$. \n",
    "\n",
    "So the derivative $\\frac{dy}{db}$ simplifies to:\n",
    "\n",
    "$$\\frac{dy}{db} = \\frac{b \\cdot 0 - a \\cdot 1}{b^2}$$\n",
    "\n",
    "Therefore, the derivative of $y$ with respect to $b$ is $-\\frac{a}{b^2}$.\n",
    "\n",
    "Therefore, the gradient of $f(a, b)$ with respect to $a$ is $\\frac{1}{b}$, and the gradient of $f(a, b)$ with respect to $b$ is $- \\frac{a}{b^{2}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d841c0bd-e4ad-4dd8-add8-0a4626a774fe",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "class EWiseDiv(TensorOp):\n",
    "    \"\"\"\n",
    "    The EWiseDiv operation divides two tensors element-wise.\n",
    "\n",
    "    Example:\n",
    "        >>> import numpy as np\n",
    "        >>> a = Tensor(np.array([1, 2, 3]))\n",
    "        >>> b = Tensor(np.array([4, 5, 6]))\n",
    "        >>> div = EWiseDiv()\n",
    "        >>> result = div.compute(a.data, b.data)\n",
    "        >>> print(result)\n",
    "        array([0.25, 0.4, 0.5])\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def compute(self, a: NDArray, b: NDArray) -> NDArray:\n",
    "        \"\"\"\n",
    "        Computes the element-wise division of two tensors.\n",
    "\n",
    "        Args:\n",
    "            a (NDArray): The dividend tensor.\n",
    "            b (NDArray): The divisor tensor.\n",
    "\n",
    "        Returns:\n",
    "            NDArray: The resulting tensor after element-wise division.\n",
    "        \"\"\"\n",
    "        return a / b\n",
    "\n",
    "    def gradient(self, out_grad: Tensor, node: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Computes the gradient of the element-wise division operation.\n",
    "\n",
    "        Args:\n",
    "            out_grad (Tensor): The gradient of the output tensor.\n",
    "            node (Tensor): The node in the computational graph where the operation was performed.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tensor, Tensor]: The gradients with respect to the dividend and divisor tensors.\n",
    "        \"\"\"\n",
    "        a, b = node.inputs\n",
    "        return divide(out_grad, b), out_grad * negate(divide(a, power_scalar(b, 2)))\n",
    "\n",
    "\n",
    "def divide(a: Tensor, b: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Divides two tensors element-wise.\n",
    "\n",
    "    Args:\n",
    "        a (Tensor): The dividend tensor.\n",
    "        b (Tensor): The divisor tensor.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: The resulting tensor after element-wise division.\n",
    "\n",
    "    Example:\n",
    "        >>> import numpy as np\n",
    "        >>> a = Tensor(np.array([1, 2, 3]))\n",
    "        >>> b = Tensor(np.array([4, 5, 6]))\n",
    "        >>> result = divide(a, b)\n",
    "        >>> print(result)\n",
    "        Tensor([0.25, 0.4, 0.5])\n",
    "    \"\"\"\n",
    "    return EWiseDiv()(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7daf46-5e14-4bf7-9f7f-81947e7e7cd3",
   "metadata": {},
   "source": [
    "## Divide Scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d98d2f-34a5-4744-994a-9c9264bfb4a9",
   "metadata": {},
   "source": [
    "Let's denote the scalar as `c`, and `a` as the tensor being divided by the scalar. The operation can be described as `f(a) = a / c`.\n",
    "\n",
    "The function for the backward pass (i.e., the gradient) is `df/da = 1/c`.\n",
    "\n",
    "This is the derivative of `f(a)` with respect to `a`.\n",
    "\n",
    "We are given a function $f(a) = \\frac{a}{c}$, where $a$ is a tensor and $c$ is a scalar. Our task is to find the derivative of this function with respect to $a$.\n",
    "\n",
    "By using the power rule of differentiation, where the derivative of $a^n$ is $n \\cdot a^{n-1}$, we can rewrite $f(a)$ as $f(a) = c^{-1}a$. \n",
    "\n",
    "Now, we can differentiate this with respect to $a$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{da} &= \\frac{d}{da} (c^{-1}a) \\\\\n",
    "&= c^{-1} \\frac{d}{da} (a) \\\\\n",
    "&= c^{-1} \\\\\n",
    "&= \\frac{1}{c}\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, the gradient of $f(a)$ with respect to $a$ is $\\frac{1}{c}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbceaf89-d19a-43e2-9ce2-3a9c093612ca",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "class DivScalar(TensorOp):\n",
    "    \"\"\"\n",
    "    The DivScalar operation divides a tensor by a scalar.\n",
    "\n",
    "    Example:\n",
    "        >>> import numpy as np\n",
    "        >>> a = Tensor(np.array([1, 2, 3]))\n",
    "        >>> scalar = 2\n",
    "        >>> div_scalar = DivScalar(scalar)\n",
    "        >>> result = div_scalar.compute(a.data)\n",
    "        >>> print(result)\n",
    "        array([0.5, 1.0, 1.5])\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scalar: Union[int, float]):\n",
    "        \"\"\"\n",
    "        Initialize the DivScalar operation with the scalar to divide by.\n",
    "\n",
    "        Args:\n",
    "            scalar (int, float): The scalar to divide the tensor by.\n",
    "        \"\"\"\n",
    "        self.scalar = scalar\n",
    "\n",
    "    def compute(self, a: NDArray) -> NDArray:\n",
    "        \"\"\"\n",
    "        Divides the tensor by the scalar.\n",
    "\n",
    "        Args:\n",
    "            a (NDArray): The tensor to divide.\n",
    "\n",
    "        Returns:\n",
    "            NDArray: The resulting tensor after division.\n",
    "        \"\"\"\n",
    "        return a / self.scalar\n",
    "\n",
    "    def gradient(self, out_grad: Tensor, node: Tensor) -> Tuple[Tensor, ...]:\n",
    "        \"\"\"\n",
    "        Computes the gradient of the division operation.\n",
    "\n",
    "        Args:\n",
    "            out_grad (Tensor): The gradient of the output tensor.\n",
    "            node (Tensor): The node in the computational graph where the operation was performed.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tensor, ...]: The gradient with respect to the tensor.\n",
    "        \"\"\"\n",
    "        return (out_grad / self.scalar, )\n",
    "\n",
    "def divide_scalar(a: Tensor, scalar: Union[int, float]) -> Tensor:\n",
    "    \"\"\"\n",
    "    Divides a tensor by a scalar.\n",
    "\n",
    "    Args:\n",
    "        a (Tensor): The tensor to divide.\n",
    "        scalar (int, float): The scalar to divide the tensor by.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: The resulting tensor after division.\n",
    "\n",
    "    Example:\n",
    "        >>> import numpy as np\n",
    "        >>> a = Tensor(np.array([1, 2, 3]))\n",
    "        >>> scalar = 2\n",
    "        >>> result = divide_scalar(a, scalar)\n",
    "        >>> print(result)\n",
    "        Tensor([0.5, 1.0, 1.5])\n",
    "    \"\"\"\n",
    "    return DivScalar(scalar)(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc235f7-c1f4-42b5-b3a1-d9cb909617d9",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
