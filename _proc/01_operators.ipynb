{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: The `operators` module in this framework provides a collection of tensor\n",
    "  operations for building computational graphs in deep learning. Each class in this\n",
    "  module represents a different type of operation that can be performed on tensors,\n",
    "  such as element-wise addition, scalar multiplication, division, exponentiation,\n",
    "  etc.\n",
    "output-file: operators.html\n",
    "title: operators\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c758cd9-3451-4fb7-a7e1-68fa9c4933d6",
   "metadata": {},
   "source": [
    "## Note about the `out_grad` parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9a4956-5575-4950-9365-560225c3a715",
   "metadata": {},
   "source": [
    "The `out_grad` parameter refers to the gradient of the loss function with respect to the output of the node. Multiplying this with the local gradient gives the gradient of the loss with respect to the input to the node, according to the chain rule of calculus, which is the basis for backpropagation in neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c0867e-7744-4c76-95b6-da3868dc8625",
   "metadata": {},
   "source": [
    "The chain rule is a fundamental concept in calculus that provides a method to compute the derivative of composite functions. In simple terms, the chain rule states that the derivative of a composite function is the derivative of the outer function multiplied by the derivative of the inner function.\n",
    "\n",
    "Given a composite function that is the composition of two functions, say, $f(g(x))$, the chain rule can be stated as follows:\n",
    "\n",
    "$$\\frac{df}{dx} = \\frac{df}{dg} \\cdot \\frac{dg}{dx}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\frac{df}{dx}$ is the derivative of the composite function $f(g(x))$ with respect to $x$,\n",
    "- $\\frac{df}{dg}$ is the derivative of the outer function $f$ with respect to its argument $g(x)$, and\n",
    "- $\\frac{dg}{dx}$ is the derivative of the inner function $g(x)$ with respect to $x$.\n",
    "\n",
    "The chain rule can be extended to the case where we have more than two composite functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cb2320-c471-426d-ad48-0197b1daecaa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Element Wise Addition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feadcd15-44d4-4a3d-aef9-39b3b7f6fcd1",
   "metadata": {},
   "source": [
    "Let's walk through the step-by-step derivative calculation for the [`EWiseAdd`](https://m0saan.github.io/minima/operators.html#ewiseadd) operation:\n",
    "\n",
    "We have the function `f(a, b) = a + b`, where `a` and `b` are tensors. Our goal is to compute the partial derivatives with respect to `a` and `b`.\n",
    "\n",
    "Let's start by calculating the derivative of `f` with respect to `a`, denoted as `df/da`:\n",
    "\n",
    "Step 1: Compute the derivative of `f` with respect to `a`.\n",
    "\n",
    "$\\frac{{\\partial f}}{{\\partial a}} = \\frac{{\\partial}}{{\\partial a}} (a + b)$\n",
    "\n",
    "Since `a` is the variable we are differentiating with respect to, the derivative of `a` with respect to itself is 1:\n",
    "\n",
    "$$\\frac{{\\partial f}}{{\\partial a}} = 1$$\n",
    "\n",
    "Therefore, $$\\frac{{\\partial f}}{{\\partial a}} = 1.$$\n",
    "\n",
    "Step 2: Compute the derivative of `f` with respect to `b`.\n",
    "\n",
    "$$\\frac{{\\partial f}}{{\\partial b}} = \\frac{{\\partial}}{{\\partial b}} (a + b)$$\n",
    "\n",
    "Again, since `b` is the variable we are differentiating with respect to, the derivative of `b` with respect to itself is 1:\n",
    "\n",
    "$$\\frac{{\\partial f}}{{\\partial b}} = 1$$\n",
    "\n",
    "Therefore, $$\\frac{{\\partial f}}{{\\partial b}} = 1$$\n",
    "\n",
    "Hence, the partial derivatives of `f(a, b) = a + b` with respect to `a` and `b` are both equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L63){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### add\n",
       "\n",
       ">      add (a:minima.autograd.Tensor, b:minima.autograd.Tensor)\n",
       "\n",
       "Adds two tensors element-wise.\n",
       "\n",
       "Args:\n",
       "- a: The first tensor.\n",
       "- b: The second tensor.\n",
       "\n",
       "Returns:\n",
       "The element-wise sum of a and b."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L63){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### add\n",
       "\n",
       ">      add (a:minima.autograd.Tensor, b:minima.autograd.Tensor)\n",
       "\n",
       "Adds two tensors element-wise.\n",
       "\n",
       "Args:\n",
       "- a: The first tensor.\n",
       "- b: The second tensor.\n",
       "\n",
       "Returns:\n",
       "The element-wise sum of a and b."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L24){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### EWiseAdd\n",
       "\n",
       ">      EWiseAdd ()\n",
       "\n",
       "Performs element-wise addition of two tensors.\n",
       "\n",
       "Example:\n",
       ">>> a = Tensor([1, 2, 3])\n",
       ">>> b = Tensor([4, 5, 6])\n",
       ">>> op = EWiseAdd()\n",
       ">>> result = op.compute(a, b)\n",
       ">>> print(result)\n",
       "Tensor([5, 7, 9])"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L24){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### EWiseAdd\n",
       "\n",
       ">      EWiseAdd ()\n",
       "\n",
       "Performs element-wise addition of two tensors.\n",
       "\n",
       "Example:\n",
       ">>> a = Tensor([1, 2, 3])\n",
       ">>> b = Tensor([4, 5, 6])\n",
       ">>> op = EWiseAdd()\n",
       ">>> result = op.compute(a, b)\n",
       ">>> print(result)\n",
       "Tensor([5, 7, 9])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(EWiseAdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd56c389-9132-4744-bcb6-cd85dc084a2d",
   "metadata": {},
   "source": [
    "Create two 1-D tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "358fcf64-9c44-4d44-8374-a0ef11668d6e",
   "metadata": {
    "language": "python",
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = Tensor([1, 2, 3])\n",
    "b = Tensor([4, 5, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cddb288-2e63-4d19-ad07-f708fa7c2dc9",
   "metadata": {},
   "source": [
    "Create an EWiseAdd operation instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a81b16cf-a61b-4771-8799-259daefa971f",
   "metadata": {
    "language": "python",
    "tags": []
   },
   "outputs": [],
   "source": [
    "op = EWiseAdd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b5a98f-cb3f-460b-9618-5457c26af297",
   "metadata": {},
   "source": [
    "Compute the element-wise sum of a and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "349ecf89-aa8f-47e6-8088-24175baa0819",
   "metadata": {
    "language": "python",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "minima.Tensor([5 7 9])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = op.compute(a, b)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df6a9f8-8f04-4098-87dc-b497137f73e9",
   "metadata": {},
   "source": [
    "Alternatively, you can use the add function directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6db6072-65fb-4cad-af75-e87236dbc921",
   "metadata": {
    "language": "python",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "minima.Tensor([5 7 9])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = add(a, b)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c4c99b-535c-4049-aa09-ae2b2b26c03d",
   "metadata": {},
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba584690-7fe5-4d93-95d7-2cd7e373811b",
   "metadata": {
    "language": "python",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "minima.Tensor([5 7 9])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dc5f72-9776-45d4-8d47-3e0313c75885",
   "metadata": {},
   "source": [
    "For 2-D tensors, we can compute the element-wise sum of a and b in the same way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dbbc80e-a2f0-46d2-97df-35344197da88",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "minima.Tensor([[ 8 10 12]\n",
       " [14 16 18]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Tensor([[1, 2, 3], [4, 5, 6]])\n",
    "b = Tensor([[7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "result = op.compute(a, b)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd371e81-b6e4-43da-9987-30057c5c038a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Scalar Addition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dbc8dc-25ae-4793-9cc8-cdbef59a8400",
   "metadata": {},
   "source": [
    "Explanation for the derivative of the [`AddScalar`](https://m0saan.github.io/minima/operators.html#addscalar) operator:\n",
    "\n",
    "Let's denote the scalar as `c` and `a` as the tensor being added by the scalar. The operation can be described as `f(a) = a + c`.\n",
    "\n",
    "The function for the backward pass (i.e., the gradient) is `df/da = 1`, which means the derivative of `f(a)` with respect to `a` is simply `1`.\n",
    "\n",
    "We are given a function $f(a) = a + c$, where $a$ is a tensor and $c$ is a scalar. Our task is to find the derivative of this function with respect to $a$.\n",
    "\n",
    "By differentiating the function $f(a)$ with respect to $a$, we find:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{da} &= \\frac{d}{da} (a + c) \\\\\n",
    "&= 1\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, the gradient of $f(a)$ with respect to $a$ is $1$.\n",
    "\n",
    "\n",
    "We starts by defining the function `f(a) = a + c`. It then explains that when we differentiate `f(a)` with respect to `a`, we find that the derivative is `1`. This means that the gradient of `f(a)` with respect to `a` is `1`, which matches the behavior of the [`AddScalar`](https://m0saan.github.io/minima/operators.html#addscalar) operator as provided in the `gradient` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L122){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### add_scalar\n",
       "\n",
       ">      add_scalar (a:minima.autograd.Tensor, scalar:Union[int,float])\n",
       "\n",
       "Adds a scalar to a tensor.\n",
       "\n",
       "Args:\n",
       "- a: The tensor.\n",
       "- scalar: The scalar to add.\n",
       "\n",
       "Returns:\n",
       "The sum of a and the scalar."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L122){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### add_scalar\n",
       "\n",
       ">      add_scalar (a:minima.autograd.Tensor, scalar:Union[int,float])\n",
       "\n",
       "Adds a scalar to a tensor.\n",
       "\n",
       "Args:\n",
       "- a: The tensor.\n",
       "- scalar: The scalar to add.\n",
       "\n",
       "Returns:\n",
       "The sum of a and the scalar."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(add_scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L77){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AddScalar\n",
       "\n",
       ">      AddScalar (scalar:Union[int,float])\n",
       "\n",
       "Performs addition of a tensor and a scalar.\n",
       "\n",
       "Example:\n",
       ">>> a = Tensor([1, 2, 3])\n",
       ">>> op = AddScalar(5)\n",
       ">>> result = op.compute(a)\n",
       ">>> print(result)\n",
       "Tensor([6, 7, 8])"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L77){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AddScalar\n",
       "\n",
       ">      AddScalar (scalar:Union[int,float])\n",
       "\n",
       "Performs addition of a tensor and a scalar.\n",
       "\n",
       "Example:\n",
       ">>> a = Tensor([1, 2, 3])\n",
       ">>> op = AddScalar(5)\n",
       ">>> result = op.compute(a)\n",
       ">>> print(result)\n",
       "Tensor([6, 7, 8])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(AddScalar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26ed99f-b3f2-4df1-9918-ff23fc99be74",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Element Wise Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf5a7d0-8e8a-47cd-a4b5-cdc12a03649c",
   "metadata": {},
   "source": [
    "Explanation for the derivative of the [`EWiseMul`](https://m0saan.github.io/minima/operators.html#ewisemul) (element-wise multiplication) operator:\n",
    "\n",
    "Let's denote the two input tensors as `a` and `b`. The operation can be described as `f(a, b) = a * b`, where `*` represents element-wise multiplication.\n",
    "\n",
    "The function for the backward pass (i.e., the gradient) is `df/da = b` and `df/db = a`. This means that the derivative of `f(a, b)` with respect to `a` is `b`, and the derivative with respect to `b` is `a`.\n",
    "\n",
    "\n",
    "We are given a function $f(a, b) = a \\odot b$, where $a$ and $b$ are tensors, and $\\odot$ represents element-wise multiplication. Our task is to find the derivatives of this function with respect to $a$ and $b$.\n",
    "\n",
    "By differentiating the function $f(a, b)$ with respect to $a$, we find:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{da} &= \\frac{d}{da} (a \\odot b) \\\\\n",
    "&= b\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, the gradient of $f(a, b)$ with respect to $a$ is $b$.\n",
    "\n",
    "Similarly, by differentiating the function $f(a, b)$ with respect to $b$, we find:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{db} &= \\frac{d}{db} (a \\odot b) \\\\\n",
    "&= a\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, the gradient of $f(a, b)$ with respect to $b$ is $a$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L175){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### multiply\n",
       "\n",
       ">      multiply (a:minima.autograd.Tensor, b:minima.autograd.Tensor)\n",
       "\n",
       "Multiplies two tensors element-wise.\n",
       "\n",
       "Args:\n",
       "- a: The first tensor.\n",
       "- b: The second tensor.\n",
       "\n",
       "Returns:\n",
       "The element-wise product of a and b."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L175){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### multiply\n",
       "\n",
       ">      multiply (a:minima.autograd.Tensor, b:minima.autograd.Tensor)\n",
       "\n",
       "Multiplies two tensors element-wise.\n",
       "\n",
       "Args:\n",
       "- a: The first tensor.\n",
       "- b: The second tensor.\n",
       "\n",
       "Returns:\n",
       "The element-wise product of a and b."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(multiply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L136){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### EWiseMul\n",
       "\n",
       ">      EWiseMul ()\n",
       "\n",
       "Performs element-wise multiplication of two tensors.\n",
       "\n",
       "Example:\n",
       ">>> a = Tensor([1, 2, 3])\n",
       ">>> b = Tensor([4, 5, 6])\n",
       ">>> op = EWiseMul()\n",
       ">>> result = op.compute(a, b)\n",
       ">>> print(result)\n",
       "Tensor([4, 10, 18])"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L136){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### EWiseMul\n",
       "\n",
       ">      EWiseMul ()\n",
       "\n",
       "Performs element-wise multiplication of two tensors.\n",
       "\n",
       "Example:\n",
       ">>> a = Tensor([1, 2, 3])\n",
       ">>> b = Tensor([4, 5, 6])\n",
       ">>> op = EWiseMul()\n",
       ">>> result = op.compute(a, b)\n",
       ">>> print(result)\n",
       "Tensor([4, 10, 18])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(EWiseMul)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb531c5-f22c-40c9-901e-e373b837a846",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Scalar Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0246b9-ce8f-4fab-991d-7ec43745c2ea",
   "metadata": {},
   "source": [
    "Let's denote the scalar as `c` and `a` as the tensor being multiplied by the scalar. The operation can be described as `f(a) = a * c`.\n",
    "\n",
    "The function for the backward pass (i.e., the gradient) is `df/da = c`, which means the derivative of `f(a)` with respect to `a` is `c`.\n",
    "\n",
    "The LaTeX document will look as follows:\n",
    "\n",
    "We are given a function $f(a) = a \\cdot c$, where $a$ is a tensor and $c$ is a scalar. Our task is to find the derivative of this function with respect to $a$.\n",
    "\n",
    "By differentiating the function $f(a)$ with respect to $a$, we find:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{da} &= \\frac{d}{da} (a \\cdot c) \\\\\n",
    "&= c\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, the gradient of $f(a)$ with respect to $a$ is $c$.\n",
    "\n",
    "We starts by defining the function `f(a) = a * c`. It then explains that when we differentiate `f(a)` with respect to `a`, we find that the derivative is `c`. This means that the gradient of `f(a)` with respect to `a` is `c`, which matches the behavior of the [`MulScalar`](https://m0saan.github.io/minima/operators.html#mulscalar) operator as provided in the `gradient` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L234){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### mul_scalar\n",
       "\n",
       ">      mul_scalar (a:minima.autograd.Tensor, scalar:Union[int,float])\n",
       "\n",
       "Multiplies a tensor by a scalar.\n",
       "\n",
       "Args:\n",
       "- a: The tensor.\n",
       "- scalar: The scalar to multiply.\n",
       "\n",
       "Returns:\n",
       "The product of a and the scalar."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L234){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### mul_scalar\n",
       "\n",
       ">      mul_scalar (a:minima.autograd.Tensor, scalar:Union[int,float])\n",
       "\n",
       "Multiplies a tensor by a scalar.\n",
       "\n",
       "Args:\n",
       "- a: The tensor.\n",
       "- scalar: The scalar to multiply.\n",
       "\n",
       "Returns:\n",
       "The product of a and the scalar."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(mul_scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L189){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MulScalar\n",
       "\n",
       ">      MulScalar (scalar:Union[int,float])\n",
       "\n",
       "Performs multiplication of a tensor and a scalar.\n",
       "\n",
       "Example:\n",
       ">>> a = Tensor([1, 2, 3])\n",
       ">>> op = MulScalar(5)\n",
       ">>> result = op.compute(a)\n",
       ">>> print(result)\n",
       "Tensor([5, 10, 15])"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L189){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MulScalar\n",
       "\n",
       ">      MulScalar (scalar:Union[int,float])\n",
       "\n",
       "Performs multiplication of a tensor and a scalar.\n",
       "\n",
       "Example:\n",
       ">>> a = Tensor([1, 2, 3])\n",
       ">>> op = MulScalar(5)\n",
       ">>> result = op.compute(a)\n",
       ">>> print(result)\n",
       "Tensor([5, 10, 15])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(MulScalar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ce3d8e-1880-42c4-b741-b31506241caa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Element Wise Divide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4126e368-f144-4d5f-9aab-4d6de564fc0f",
   "metadata": {},
   "source": [
    "The operation described here is an element-wise division of two tensors, `a` and `b`, where the operation can be described as `f(a, b) = a / b`. \n",
    "\n",
    "We'll compute the partial derivatives with respect to `a` and `b`:\n",
    "\n",
    "1. The partial derivative of `f(a, b)` with respect to `a` (`df/da`) is `1/b`.\n",
    "\n",
    "2. The partial derivative of `f(a, b)` with respect to `b` (`df/db`) is `-a / b^2`.\n",
    "\n",
    "We are given a function $f(a, b) = \\frac{a}{b}$, where $a$ and $b$ are tensors. Our task is to find the partial derivatives of this function with respect to $a$ and $b$.\n",
    "\n",
    "Let's start with $\\frac{\\partial f}{\\partial a}$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial f}{\\partial a} &= \\frac{\\partial}{\\partial a} \\left(\\frac{a}{b}\\right) \\\\\n",
    "&= \\frac{1}{b}\n",
    "\\end{align*}\n",
    "\n",
    "Now, let's compute $\\frac{\\partial f}{\\partial b}$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial f}{\\partial b} &= \\frac{\\partial}{\\partial b} \\left(\\frac{a}{b}\\right) \\\\\n",
    "&= - \\frac{a}{b^{2}}\n",
    "\\end{align*}\n",
    "\n",
    "Here is a detailed derivative:\n",
    "\n",
    "Given a function of the form $y = \\frac{u}{v}$, where both $u$ and $v$ are functions of $x$, the quotient rule of differentiation states:\n",
    "\n",
    "$$\\frac{dy}{dx} = \\frac{v \\cdot \\frac{du}{dx} - u \\cdot \\frac{dv}{dx}}{v^2}$$\n",
    "\n",
    "In our case, we're looking at the function $y = \\frac{a}{b}$, where $a$ and $b$ are tensors. We want to find the derivative with respect to $b$ (instead of $x$ in our general formula). So we have:\n",
    "\n",
    "$$\\frac{dy}{db} = \\frac{b \\cdot \\frac{da}{db} - a \\cdot \\frac{db}{db}}{b^2}$$\n",
    "\n",
    "Since $a$ does not depend on $b$, $\\frac{da}{db} = 0$, and since any variable is equal to itself, $\\frac{db}{db} = 1$. \n",
    "\n",
    "So the derivative $\\frac{dy}{db}$ simplifies to:\n",
    "\n",
    "$$\\frac{dy}{db} = \\frac{b \\cdot 0 - a \\cdot 1}{b^2}$$\n",
    "\n",
    "Therefore, the derivative of $y$ with respect to $b$ is $-\\frac{a}{b^2}$.\n",
    "\n",
    "Therefore, the gradient of $f(a, b)$ with respect to $a$ is $\\frac{1}{b}$, and the gradient of $f(a, b)$ with respect to $b$ is $- \\frac{a}{b^{2}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L291){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### divide\n",
       "\n",
       ">      divide (a:minima.autograd.Tensor, b:minima.autograd.Tensor)\n",
       "\n",
       "Divides two tensors element-wise.\n",
       "\n",
       "Args:\n",
       "    a (Tensor): The dividend tensor.\n",
       "    b (Tensor): The divisor tensor.\n",
       "\n",
       "Returns:\n",
       "    Tensor: The resulting tensor after element-wise division.\n",
       "\n",
       "Example:\n",
       "    >>> import numpy as np\n",
       "    >>> a = Tensor(np.array([1, 2, 3]))\n",
       "    >>> b = Tensor(np.array([4, 5, 6]))\n",
       "    >>> result = divide(a, b)\n",
       "    >>> print(result)\n",
       "    Tensor([0.25, 0.4, 0.5])"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L291){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### divide\n",
       "\n",
       ">      divide (a:minima.autograd.Tensor, b:minima.autograd.Tensor)\n",
       "\n",
       "Divides two tensors element-wise.\n",
       "\n",
       "Args:\n",
       "    a (Tensor): The dividend tensor.\n",
       "    b (Tensor): The divisor tensor.\n",
       "\n",
       "Returns:\n",
       "    Tensor: The resulting tensor after element-wise division.\n",
       "\n",
       "Example:\n",
       "    >>> import numpy as np\n",
       "    >>> a = Tensor(np.array([1, 2, 3]))\n",
       "    >>> b = Tensor(np.array([4, 5, 6]))\n",
       "    >>> result = divide(a, b)\n",
       "    >>> print(result)\n",
       "    Tensor([0.25, 0.4, 0.5])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(divide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L248){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### EWiseDiv\n",
       "\n",
       ">      EWiseDiv ()\n",
       "\n",
       "The EWiseDiv operation divides two tensors element-wise.\n",
       "\n",
       "Example:\n",
       "    >>> import numpy as np\n",
       "    >>> a = Tensor(np.array([1, 2, 3]))\n",
       "    >>> b = Tensor(np.array([4, 5, 6]))\n",
       "    >>> div = EWiseDiv()\n",
       "    >>> result = div.compute(a.data, b.data)\n",
       "    >>> print(result)\n",
       "    array([0.25, 0.4, 0.5])"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L248){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### EWiseDiv\n",
       "\n",
       ">      EWiseDiv ()\n",
       "\n",
       "The EWiseDiv operation divides two tensors element-wise.\n",
       "\n",
       "Example:\n",
       "    >>> import numpy as np\n",
       "    >>> a = Tensor(np.array([1, 2, 3]))\n",
       "    >>> b = Tensor(np.array([4, 5, 6]))\n",
       "    >>> div = EWiseDiv()\n",
       "    >>> result = div.compute(a.data, b.data)\n",
       "    >>> print(result)\n",
       "    array([0.25, 0.4, 0.5])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(EWiseDiv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9983c201-4fe0-4a73-8f8f-59cdb6850753",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Scalar Division"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4463c6e-8ae4-4a1a-bc03-6728784059bf",
   "metadata": {},
   "source": [
    "Let's denote the scalar as `c`, and `a` as the tensor being divided by the scalar. The operation can be described as `f(a) = a / c`.\n",
    "\n",
    "The function for the backward pass (i.e., the gradient) is `df/da = 1/c`.\n",
    "\n",
    "This is the derivative of `f(a)` with respect to `a`.\n",
    "\n",
    "We are given a function $f(a) = \\frac{a}{c}$, where $a$ is a tensor and $c$ is a scalar. Our task is to find the derivative of this function with respect to $a$.\n",
    "\n",
    "By using the power rule of differentiation, where the derivative of $a^n$ is $n \\cdot a^{n-1}$, we can rewrite $f(a)$ as $f(a) = c^{-1}a$. \n",
    "\n",
    "Now, we can differentiate this with respect to $a$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{da} &= \\frac{d}{da} (c^{-1}a) \\\\\n",
    "&= c^{-1} \\frac{d}{da} (a) \\\\\n",
    "&= c^{-1} \\\\\n",
    "&= \\frac{1}{c}\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, the gradient of $f(a)$ with respect to $a$ is $\\frac{1}{c}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L363){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### divide_scalar\n",
       "\n",
       ">      divide_scalar (a:minima.autograd.Tensor, scalar:Union[int,float])\n",
       "\n",
       "Divides a tensor by a scalar.\n",
       "\n",
       "Args:\n",
       "    a (Tensor): The tensor to divide.\n",
       "    scalar (int, float): The scalar to divide the tensor by.\n",
       "\n",
       "Returns:\n",
       "    Tensor: The resulting tensor after division.\n",
       "\n",
       "Example:\n",
       "    >>> import numpy as np\n",
       "    >>> a = Tensor(np.array([1, 2, 3]))\n",
       "    >>> scalar = 2\n",
       "    >>> result = divide_scalar(a, scalar)\n",
       "    >>> print(result)\n",
       "    Tensor([0.5, 1.0, 1.5])"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L363){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### divide_scalar\n",
       "\n",
       ">      divide_scalar (a:minima.autograd.Tensor, scalar:Union[int,float])\n",
       "\n",
       "Divides a tensor by a scalar.\n",
       "\n",
       "Args:\n",
       "    a (Tensor): The tensor to divide.\n",
       "    scalar (int, float): The scalar to divide the tensor by.\n",
       "\n",
       "Returns:\n",
       "    Tensor: The resulting tensor after division.\n",
       "\n",
       "Example:\n",
       "    >>> import numpy as np\n",
       "    >>> a = Tensor(np.array([1, 2, 3]))\n",
       "    >>> scalar = 2\n",
       "    >>> result = divide_scalar(a, scalar)\n",
       "    >>> print(result)\n",
       "    Tensor([0.5, 1.0, 1.5])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(divide_scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L314){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DivScalar\n",
       "\n",
       ">      DivScalar (scalar:Union[int,float])\n",
       "\n",
       "The DivScalar operation divides a tensor by a scalar.\n",
       "\n",
       "Example:\n",
       "    >>> import numpy as np\n",
       "    >>> a = Tensor(np.array([1, 2, 3]))\n",
       "    >>> scalar = 2\n",
       "    >>> div_scalar = DivScalar(scalar)\n",
       "    >>> result = div_scalar.compute(a.data)\n",
       "    >>> print(result)\n",
       "    array([0.5, 1.0, 1.5])"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L314){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DivScalar\n",
       "\n",
       ">      DivScalar (scalar:Union[int,float])\n",
       "\n",
       "The DivScalar operation divides a tensor by a scalar.\n",
       "\n",
       "Example:\n",
       "    >>> import numpy as np\n",
       "    >>> a = Tensor(np.array([1, 2, 3]))\n",
       "    >>> scalar = 2\n",
       "    >>> div_scalar = DivScalar(scalar)\n",
       "    >>> result = div_scalar.compute(a.data)\n",
       "    >>> print(result)\n",
       "    array([0.5, 1.0, 1.5])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(DivScalar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6497d8ab-3003-4784-a330-ad5f862b9ca5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Negation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04085053-1446-4343-b720-17e04a1c4ee1",
   "metadata": {},
   "source": [
    "Let's denote `a` as the tensor being negated. The operation can be described as `f(a) = -a`.\n",
    "\n",
    "The function for the backward pass (i.e., the gradient) is `df/da = -1`.\n",
    "\n",
    "We are given a function $f(a) = -a$, where $a$ is a tensor. Our task is to find the derivative of this function with respect to $a$.\n",
    "\n",
    "By differentiating the function $f(a)$ with respect to $a$, we find:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{da} &= \\frac{d}{da} (-a) \\\\\n",
    "&= -1\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, the gradient of $f(a)$ with respect to $a$ is $-1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L423){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### negate\n",
       "\n",
       ">      negate (a:minima.autograd.Tensor)\n",
       "\n",
       "Negates the given tensor.\n",
       "\n",
       "Args:\n",
       "- a: The tensor to negate.\n",
       "\n",
       "Returns:\n",
       "The negation of a.\n",
       "\n",
       "Example:\n",
       ">>> a = Tensor([1, -2, 3])\n",
       ">>> result = negate(a)\n",
       ">>> print(result)\n",
       "Tensor([-1, 2, -3])"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L423){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### negate\n",
       "\n",
       ">      negate (a:minima.autograd.Tensor)\n",
       "\n",
       "Negates the given tensor.\n",
       "\n",
       "Args:\n",
       "- a: The tensor to negate.\n",
       "\n",
       "Returns:\n",
       "The negation of a.\n",
       "\n",
       "Example:\n",
       ">>> a = Tensor([1, -2, 3])\n",
       ">>> result = negate(a)\n",
       ">>> print(result)\n",
       "Tensor([-1, 2, -3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(negate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L385){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Negate\n",
       "\n",
       ">      Negate ()\n",
       "\n",
       "Negates the given tensor.\n",
       "\n",
       "Example:\n",
       ">>> a = Tensor([1, -2, 3])\n",
       ">>> op = Negate()\n",
       ">>> result = op.compute(a)\n",
       ">>> print(result)\n",
       "Tensor([-1, 2, -3])"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L385){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Negate\n",
       "\n",
       ">      Negate ()\n",
       "\n",
       "Negates the given tensor.\n",
       "\n",
       "Example:\n",
       ">>> a = Tensor([1, -2, 3])\n",
       ">>> op = Negate()\n",
       ">>> result = op.compute(a)\n",
       ">>> print(result)\n",
       "Tensor([-1, 2, -3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(Negate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e28e521-a653-45e0-a567-46c7b800d281",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8bf43a-9a2a-4077-a54f-54df0c6c955d",
   "metadata": {},
   "source": [
    "Explanation for the derivative of the [`Exp`](https://m0saan.github.io/minima/operators.html#exp) operator:\n",
    "\n",
    "Let's denote `a` as the tensor on which the exponential function is applied. The operation can be described as `f(a) = exp(a)`, where [`exp`](https://m0saan.github.io/minima/operators.html#exp) represents the exponential function.\n",
    "\n",
    "The function for the backward pass (i.e., the gradient) is `df/da = exp(a)`.\n",
    "\n",
    "We are given a function $f(a) = \\exp(a)$, where $a$ is a tensor. Our task is to find the derivative of this function with respect to $a$.\n",
    "\n",
    "By differentiating the function $f(a)$ with respect to $a$, we find:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{da} &= \\frac{d}{da} (\\exp(a)) \\\\\n",
    "&= \\exp(a)\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, the gradient of $f(a)$ with respect to $a$ is $\\exp(a)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L480){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### exp\n",
       "\n",
       ">      exp (a:minima.autograd.Tensor)\n",
       "\n",
       "Calculates the exponential of the given tensor.\n",
       "\n",
       "Args:\n",
       "- a: The tensor.\n",
       "\n",
       "Returns:\n",
       "The exponential of a.\n",
       "\n",
       "Example:\n",
       ">>> a = Tensor([1, 2, 3])\n",
       ">>> result = exp(a)\n",
       ">>> print(result)\n",
       "Tensor([2.71828183, 7.3890561, 20.08553692])"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L480){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### exp\n",
       "\n",
       ">      exp (a:minima.autograd.Tensor)\n",
       "\n",
       "Calculates the exponential of the given tensor.\n",
       "\n",
       "Args:\n",
       "- a: The tensor.\n",
       "\n",
       "Returns:\n",
       "The exponential of a.\n",
       "\n",
       "Example:\n",
       ">>> a = Tensor([1, 2, 3])\n",
       ">>> result = exp(a)\n",
       ">>> print(result)\n",
       "Tensor([2.71828183, 7.3890561, 20.08553692])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L442){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Exp\n",
       "\n",
       ">      Exp ()\n",
       "\n",
       "Calculates the exponential of the given tensor.\n",
       "\n",
       "Example:\n",
       ">>> a = Tensor([1, 2, 3])\n",
       ">>> op = Exp()\n",
       ">>> result = op.compute(a)\n",
       ">>> print(result)\n",
       "Tensor([2.71828183, 7.3890561, 20.08553692])"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L442){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Exp\n",
       "\n",
       ">      Exp ()\n",
       "\n",
       "Calculates the exponential of the given tensor.\n",
       "\n",
       "Example:\n",
       ">>> a = Tensor([1, 2, 3])\n",
       ">>> op = Exp()\n",
       ">>> result = op.compute(a)\n",
       ">>> print(result)\n",
       "Tensor([2.71828183, 7.3890561, 20.08553692])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(Exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45592e37-9a6d-42ec-8458-167a94394cc1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8aefd6-23e2-4df7-8627-f74813b8f0bc",
   "metadata": {},
   "source": [
    "The derivative of the [`ReLU`](https://m0saan.github.io/minima/operators.html#relu) (Rectified Linear Unit) operator:\n",
    "\n",
    "Let's denote `a` as the tensor on which the ReLU function is applied. The ReLU function is defined as follows: \n",
    "\n",
    "$$\n",
    "f(a) = \n",
    "\\begin{cases}\n",
    "a, & \\text{if } a \\geq 0 \\\\\n",
    "0, & \\text{if } a < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The function for the backward pass (i.e., the gradient) is `df/da = 1` if `a >= 0`, and `df/da = 0` if `a < 0`.\n",
    "\n",
    "We are given a function $f(a) = \\max(0, a)$, where $a$ is a tensor. Our task is to find the derivative of this function with respect to $a$.\n",
    "\n",
    "By considering the definition of the ReLU function, we can write $f(a)$ as:\n",
    "\n",
    "$$\n",
    "f(a) = \n",
    "\\begin{cases}\n",
    "a, & \\text{if } a \\geq 0 \\\\\n",
    "0, & \\text{if } a < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Now, let's differentiate $f(a)$ with respect to $a$:\n",
    "\n",
    "$$\n",
    "\\frac{df}{da} = \n",
    "\\begin{cases}\n",
    "1, & \\text{if } a \\geq 0 \\\\\n",
    "0, & \\text{if } a < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Therefore, the gradient of $f(a)$ with respect to $a$ is $1$ if $a \\geq 0$, and $0$ if $a < 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L537){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### relu\n",
       "\n",
       ">      relu (a:minima.autograd.Tensor)\n",
       "\n",
       "Applies the ReLU (Rectified Linear Unit) activation function to the given tensor.\n",
       "\n",
       "Args:\n",
       "- a: The tensor.\n",
       "\n",
       "Returns:\n",
       "The result of applying ReLU to a.\n",
       "\n",
       "Example:\n",
       ">>> a = Tensor([1, -2, 3])\n",
       ">>> result = relu(a)\n",
       ">>> print(result)\n",
       "Tensor([1, 0, 3])"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L537){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### relu\n",
       "\n",
       ">      relu (a:minima.autograd.Tensor)\n",
       "\n",
       "Applies the ReLU (Rectified Linear Unit) activation function to the given tensor.\n",
       "\n",
       "Args:\n",
       "- a: The tensor.\n",
       "\n",
       "Returns:\n",
       "The result of applying ReLU to a.\n",
       "\n",
       "Example:\n",
       ">>> a = Tensor([1, -2, 3])\n",
       ">>> result = relu(a)\n",
       ">>> print(result)\n",
       "Tensor([1, 0, 3])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L499){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ReLU\n",
       "\n",
       ">      ReLU ()\n",
       "\n",
       "Applies the ReLU (Rectified Linear Unit) activation function to the given tensor.\n",
       "\n",
       "Example:\n",
       ">>> a = Tensor([1, -2, 3])\n",
       ">>> op = ReLU()\n",
       ">>> result = op.compute(a)\n",
       ">>> print(result)\n",
       "Tensor([1, 0, 3])"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L499){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ReLU\n",
       "\n",
       ">      ReLU ()\n",
       "\n",
       "Applies the ReLU (Rectified Linear Unit) activation function to the given tensor.\n",
       "\n",
       "Example:\n",
       ">>> a = Tensor([1, -2, 3])\n",
       ">>> op = ReLU()\n",
       ">>> result = op.compute(a)\n",
       ">>> print(result)\n",
       "Tensor([1, 0, 3])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(ReLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9f074a-e23c-4e8a-8ab7-2cfba344c461",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Power Scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d5e875-0981-4243-86b5-d6ca6b117d5e",
   "metadata": {},
   "source": [
    "The derivative of the [`PowerScalar`](https://m0saan.github.io/minima/operators.html#powerscalar) operator:\n",
    "\n",
    "Let's denote the scalar as `n` and `a` as the tensor being raised to the power of the scalar. The operation can be described as `f(a) = a^n`.\n",
    "\n",
    "The function for the backward pass (i.e., the gradient) is `df/da = n * a^(n-1)`.\n",
    "\n",
    "We are given a function $f(a) = a^n$, where $a$ is a tensor and $n$ is a scalar. Our task is to find the derivative of this function with respect to $a$.\n",
    "\n",
    "By differentiating the function $f(a)$ with respect to $a$, we find:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{da} &= \\frac{d}{da} (a^n) \\\\\n",
    "&= n \\cdot a^{n-1}\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, the gradient of $f(a)$ with respect to $a$ is $n \\cdot a^{n-1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L610){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### power_scalar\n",
       "\n",
       ">      power_scalar (a:minima.autograd.Tensor, scalar:int)\n",
       "\n",
       "Raises a tensor to a power.\n",
       "\n",
       "Args:\n",
       "    a (Tensor): The input tensor.\n",
       "    scalar (int): The power to raise the tensor to.\n",
       "\n",
       "Returns:\n",
       "    Tensor: The resulting tensor after the power operation.\n",
       "\n",
       "Example:\n",
       "    >>> import numpy as np\n",
       "    >>> tensor = Tensor(np.array([1, 2, 3]))\n",
       "    >>> result = power_scalar(tensor, 2)\n",
       "    >>> print(result)\n",
       "    Tensor([1, 4, 9])"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L610){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### power_scalar\n",
       "\n",
       ">      power_scalar (a:minima.autograd.Tensor, scalar:int)\n",
       "\n",
       "Raises a tensor to a power.\n",
       "\n",
       "Args:\n",
       "    a (Tensor): The input tensor.\n",
       "    scalar (int): The power to raise the tensor to.\n",
       "\n",
       "Returns:\n",
       "    Tensor: The resulting tensor after the power operation.\n",
       "\n",
       "Example:\n",
       "    >>> import numpy as np\n",
       "    >>> tensor = Tensor(np.array([1, 2, 3]))\n",
       "    >>> result = power_scalar(tensor, 2)\n",
       "    >>> print(result)\n",
       "    Tensor([1, 4, 9])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(power_scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L557){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### PowerScalar\n",
       "\n",
       ">      PowerScalar (scalar:int)\n",
       "\n",
       "The PowerScalar operation raises a tensor to an (integer) power.\n",
       "\n",
       "Attributes:\n",
       "    scalar (int): The power to raise the tensor to.\n",
       "\n",
       "Example:\n",
       "    >>> import numpy as np\n",
       "    >>> tensor = Tensor(np.array([1, 2, 3]))\n",
       "    >>> pow_scalar = PowerScalar(2)\n",
       "    >>> result = pow_scalar.compute(tensor.data)\n",
       "    >>> print(result)\n",
       "    array([1, 4, 9])"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L557){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### PowerScalar\n",
       "\n",
       ">      PowerScalar (scalar:int)\n",
       "\n",
       "The PowerScalar operation raises a tensor to an (integer) power.\n",
       "\n",
       "Attributes:\n",
       "    scalar (int): The power to raise the tensor to.\n",
       "\n",
       "Example:\n",
       "    >>> import numpy as np\n",
       "    >>> tensor = Tensor(np.array([1, 2, 3]))\n",
       "    >>> pow_scalar = PowerScalar(2)\n",
       "    >>> result = pow_scalar.compute(tensor.data)\n",
       "    >>> print(result)\n",
       "    array([1, 4, 9])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(PowerScalar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff76cab1-8050-4df9-87fa-87fc58145f68",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c3d794-0946-4ab1-8e9c-e4483803c4c1",
   "metadata": {},
   "source": [
    "Explanation for the derivative of the `Log` operator:\n",
    "\n",
    "Let's denote `a` as the tensor on which the logarithm is applied. The operation can be described as `f(a) = log(a)`, where `log` represents the natural logarithm.\n",
    "\n",
    "The function for the backward pass (i.e., the gradient) is `df/da = 1/a`.\n",
    "\n",
    "We are given a function $f(a) = \\log(a)$, where $a$ is a tensor. Our task is to find the derivative of this function with respect to $a$.\n",
    "\n",
    "By differentiating the function $f(a)$ with respect to $a$, we find:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{da} &= \\frac{d}{da} (\\log(a)) \\\\\n",
    "&= \\frac{1}{a}\n",
    "\\end{align*}\n",
    "\n",
    "We started by defining the function `f(a) = log(a)`, where `log` represents the natural logarithm. It then explains that when we differentiate `f(a)` with respect to `a`, we find that the derivative is `1/a`. This means that the gradient of `f(a)` with respect to `a` is `1/a`, which represents the behavior of the `Log` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fb52c72-9bb5-4d01-bd08-59cb37acfa3d",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "class Log(TensorOp):\n",
    "    \"\"\"\n",
    "    The Log operation applies the natural logarithm element-wise on the tensor.\n",
    "\n",
    "    Example:\n",
    "        >>> import numpy as np\n",
    "        >>> a = Tensor(np.array([1.0, 2.0, 3.0]))\n",
    "        >>> log_op = Log()\n",
    "        >>> result = log_op.compute(a.data)\n",
    "        >>> print(result)\n",
    "        array([0., 0.69314718, 1.09861229])\n",
    "    \"\"\"\n",
    "\n",
    "    def compute(self, a: NDArray) -> NDArray:\n",
    "        \"\"\"\n",
    "        Applies the natural logarithm to the tensor.\n",
    "\n",
    "        Args:\n",
    "            a (NDArray): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            NDArray: The resulting tensor after applying the natural logarithm.\n",
    "        \"\"\"\n",
    "        return array_api.log(a)\n",
    "\n",
    "    def gradient(self, out_grad: Tensor, node: Tensor) -> Tuple[Tensor, ...]:\n",
    "        \"\"\"\n",
    "        Computes the gradient of the log operation.\n",
    "\n",
    "        Args:\n",
    "            out_grad (Tensor): The gradient of the output tensor.\n",
    "            node (Tensor): The node in the computational graph where the operation was performed.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tensor, ...]: The gradient with respect to the input tensor.\n",
    "        \"\"\"\n",
    "        a = node.children[0]\n",
    "        return (out_grad / a, )\n",
    "\n",
    "def log(a: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Applies the natural logarithm to the tensor.\n",
    "\n",
    "    Args:\n",
    "        a (Tensor): The input tensor.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: The resulting tensor after applying the natural logarithm.\n",
    "\n",
    "    Example:\n",
    "        >>> import numpy as np\n",
    "        >>> a = Tensor(np.array([1.0, 2.0, 3.0]))\n",
    "        >>> result = log(a)\n",
    "        >>> print(result)\n",
    "        Tensor([0., 0.69314718, 1.09861229])\n",
    "    \"\"\"\n",
    "    return Log()(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecaeed3-334f-4d00-9027-5329af5208af",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Transpose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21611651-aa6d-45e0-98a0-a047b73e7cb7",
   "metadata": {},
   "source": [
    "The operation described here is a transposition of a tensor `a`, where the operation can be described as `f(a) = a^T`.\n",
    "\n",
    "We'll compute the derivative of this operation.\n",
    "\n",
    "First, we note that the transpose operation doesn't change the values of the tensor elements but only their positions. This means that the gradient of a transposed tensor is just the transposed gradient of the original tensor.\n",
    "\n",
    "Let's denote the gradient of the transposed tensor as `g = df/da`, where `f(a) = a^T`.\n",
    "\n",
    "Given this, we can derive the following:\n",
    "\n",
    "1. The derivative of `f(a)` with respect to `a` is `df/da = g^T`.\n",
    "\n",
    "This conclusion can be illustrated as follows in Latex:\n",
    "\n",
    "We are given a function $f(a) = a^T$, where $a$ is a tensor and $a^T$ is the transpose of the tensor. Our task is to find the derivative of this function with respect to $a$.\n",
    "\n",
    "Let's compute $\\frac{df}{da}$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{da} &= \\frac{d}{da} (a^T) \\\\\n",
    "&= (g)^T\n",
    "\\end{align*}\n",
    "\n",
    "Here, $g$ is the gradient of the transposed tensor. The derivative of a transposed tensor is the transposed derivative of the original tensor.\n",
    "\n",
    "Now, let's apply this to the [`Transpose`](https://m0saan.github.io/minima/operators.html#transpose) class.\n",
    "\n",
    "The `gradient` method in the [`Transpose`](https://m0saan.github.io/minima/operators.html#transpose) class computes the gradient of the transpose operation. The gradient of the transposed tensor is just the transposed gradient of the original tensor. This is implemented by applying the [`transpose`](https://m0saan.github.io/minima/operators.html#transpose) function to `out_grad`, which is the gradient of the output tensor, and then returning this transposed gradient. The axes used for the transpose operation are the same as the ones used in the forward pass.\n",
    "\n",
    "Therefore, the gradient of the transposition operation with respect to the input tensor `a` is the transpose of the output gradient `out_grad`.\n",
    "\n",
    "In this code, `transpose(out_grad, axes=self.axes)` performs the transposition of `out_grad` along the specified axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L684){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### transpose\n",
       "\n",
       ">      transpose (a:minima.autograd.Tensor, axes:Optional[tuple]=None)\n",
       "\n",
       "Perform the transpose operation on the input tensor along the specified axes.\n",
       "If no axes are specified, it swaps the last two dimensions of the input tensor.\n",
       "\n",
       "Args:\n",
       "    a (Tensor): The input tensor.\n",
       "    axes (Optional[tuple]): The pair of axes that should be swapped. If not provided, the last two axes are swapped.\n",
       "\n",
       "Returns:\n",
       "    Tensor: The transposed tensor.\n",
       "\n",
       "Example:\n",
       "    >>> a = Tensor(np.arange(1, 7).reshape(2, 3))\n",
       "    >>> result = transpose(a)\n",
       "    >>> print(result)\n",
       "    Tensor([[1, 4],\n",
       "            [2, 5],\n",
       "            [3, 6]])"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L684){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### transpose\n",
       "\n",
       ">      transpose (a:minima.autograd.Tensor, axes:Optional[tuple]=None)\n",
       "\n",
       "Perform the transpose operation on the input tensor along the specified axes.\n",
       "If no axes are specified, it swaps the last two dimensions of the input tensor.\n",
       "\n",
       "Args:\n",
       "    a (Tensor): The input tensor.\n",
       "    axes (Optional[tuple]): The pair of axes that should be swapped. If not provided, the last two axes are swapped.\n",
       "\n",
       "Returns:\n",
       "    Tensor: The transposed tensor.\n",
       "\n",
       "Example:\n",
       "    >>> a = Tensor(np.arange(1, 7).reshape(2, 3))\n",
       "    >>> result = transpose(a)\n",
       "    >>> print(result)\n",
       "    Tensor([[1, 4],\n",
       "            [2, 5],\n",
       "            [3, 6]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(transpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L631){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Transpose\n",
       "\n",
       ">      Transpose (axes:Optional[tuple]=None)\n",
       "\n",
       "Tensor operation class that performs transposition of a tensor along specified axes.\n",
       "\n",
       "If no axes are specified, it swaps the last two dimensions of the input tensor.\n",
       "\n",
       "Example:\n",
       "    >>> a = Tensor(np.arange(1, 7).reshape(2, 3))\n",
       "    >>> op = Transpose()\n",
       "    >>> result = op.compute(a.data)\n",
       "    >>> print(result)\n",
       "    array([[1, 4],\n",
       "           [2, 5],\n",
       "           [3, 6]])"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L631){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Transpose\n",
       "\n",
       ">      Transpose (axes:Optional[tuple]=None)\n",
       "\n",
       "Tensor operation class that performs transposition of a tensor along specified axes.\n",
       "\n",
       "If no axes are specified, it swaps the last two dimensions of the input tensor.\n",
       "\n",
       "Example:\n",
       "    >>> a = Tensor(np.arange(1, 7).reshape(2, 3))\n",
       "    >>> op = Transpose()\n",
       "    >>> result = op.compute(a.data)\n",
       "    >>> print(result)\n",
       "    array([[1, 4],\n",
       "           [2, 5],\n",
       "           [3, 6]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(Transpose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6439958d-fd59-4e61-b249-4dd00bfc2cb4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4da8fb7-4f87-47a0-b5b4-468b46c2b14e",
   "metadata": {},
   "source": [
    "The operation described here is a reshaping of a tensor `a`, where the operation can be described as `f(a) = reshape(a, new_shape)`.\n",
    "\n",
    "We'll compute the derivative of this operation.\n",
    "\n",
    "The reshaping operation doesn't change the values of the tensor elements but only rearranges them. This means that the gradient of a reshaped tensor is just the reshaped gradient of the original tensor.\n",
    "\n",
    "Let's denote the gradient of the reshaped tensor as `g = df/da`, where `f(a) = reshape(a, new_shape)`.\n",
    "\n",
    "Given this, we can derive the following:\n",
    "\n",
    "1. The derivative of `f(a)` with respect to `a` is `df/da = reshape(g, original_shape)`.\n",
    "\n",
    "This conclusion can be illustrated as follows in Latex:\n",
    "\n",
    "We are given a function $f(a) = reshape(a, new\\_shape)$, where $a$ is a tensor and `reshape(a, new_shape)` is the reshaped tensor. Our task is to find the derivative of this function with respect to $a$.\n",
    "\n",
    "Let's compute $\\frac{df}{da}$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{da} &= \\frac{d}{da} (reshape(a, new\\_shape)) \\\\\n",
    "&= reshape(g, original\\_shape)\n",
    "\\end{align*}\n",
    "\n",
    "Here, $g$ is the gradient of the reshaped tensor. The derivative of a reshaped tensor is the reshaped derivative of the original tensor. The reshaped derivative has the same shape as the original tensor.\n",
    "\n",
    "Now, let's apply this to the [`Reshape`](https://m0saan.github.io/minima/operators.html#reshape) class.\n",
    "\n",
    "The `gradient` method in the [`Reshape`](https://m0saan.github.io/minima/operators.html#reshape) class computes the gradient of the reshape operation. The gradient of the reshaped tensor is just the reshaped gradient of the original tensor. This is implemented by applying the [`reshape`](https://m0saan.github.io/minima/operators.html#reshape) function to `out_grad`, which is the gradient of the output tensor, and then returning this reshaped gradient. The shape used for the reshaping is the shape of the original tensor, which is obtained from `node.children[0].shape`.\n",
    "\n",
    "Therefore, the gradient of the reshape operation with respect to the input tensor `a` is the reshaping of the output gradient `out_grad` to the shape of the original tensor.\n",
    "\n",
    "Here is the corresponding Python code:\n",
    "\n",
    "```python\n",
    "def gradient(self, out_grad: Tensor, node: Tensor) -> Tuple[Tensor, ...]:\n",
    "    \"\"\"\n",
    "    Compute the gradient of the reshape operation.\n",
    "\n",
    "    Args:\n",
    "        out_grad (Tensor): The gradient of the output tensor.\n",
    "        node (Tensor): The node in the computational graph where the operation was performed.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Tensor, ...]: The gradient with respect to the input tensor.\n",
    "    \"\"\"\n",
    "    input_shape = node.children[0].shape\n",
    "    return reshape(out_grad, input_shape),\n",
    "```\n",
    "\n",
    "In this code, `reshape(out_grad, input_shape)` performs the reshaping of `out_grad` to the shape of the original tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L755){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### reshape\n",
       "\n",
       ">      reshape (a:minima.autograd.Tensor, shape:Tuple[int,...])\n",
       "\n",
       "Reshape the input tensor to the specified shape.\n",
       "\n",
       "Args:\n",
       "    a (Tensor): The input tensor.\n",
       "    shape (Tuple[int, ...]): The desired shape of the output tensor.\n",
       "\n",
       "Returns:\n",
       "    Tensor: The reshaped tensor.\n",
       "\n",
       "Example:\n",
       "    >>> a = Tensor([1, 2, 3, 4, 5, 6])\n",
       "    >>> result = reshape(a, (2, 3))\n",
       "    >>> print(result)\n",
       "    Tensor([[1, 2, 3],\n",
       "             [4, 5, 6]])"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L755){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### reshape\n",
       "\n",
       ">      reshape (a:minima.autograd.Tensor, shape:Tuple[int,...])\n",
       "\n",
       "Reshape the input tensor to the specified shape.\n",
       "\n",
       "Args:\n",
       "    a (Tensor): The input tensor.\n",
       "    shape (Tuple[int, ...]): The desired shape of the output tensor.\n",
       "\n",
       "Returns:\n",
       "    Tensor: The reshaped tensor.\n",
       "\n",
       "Example:\n",
       "    >>> a = Tensor([1, 2, 3, 4, 5, 6])\n",
       "    >>> result = reshape(a, (2, 3))\n",
       "    >>> print(result)\n",
       "    Tensor([[1, 2, 3],\n",
       "             [4, 5, 6]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L708){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Reshape\n",
       "\n",
       ">      Reshape (shape:Tuple[int,...])\n",
       "\n",
       "Tensor operation class that reshapes a tensor.\n",
       "\n",
       "Example:\n",
       "    >>> a = Tensor([1, 2, 3, 4, 5, 6])\n",
       "    >>> op = Reshape((2, 3))\n",
       "    >>> result = op.compute(a)\n",
       "    >>> print(result)\n",
       "    Tensor([[1, 2, 3],\n",
       "             [4, 5, 6]])"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L708){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Reshape\n",
       "\n",
       ">      Reshape (shape:Tuple[int,...])\n",
       "\n",
       "Tensor operation class that reshapes a tensor.\n",
       "\n",
       "Example:\n",
       "    >>> a = Tensor([1, 2, 3, 4, 5, 6])\n",
       "    >>> op = Reshape((2, 3))\n",
       "    >>> result = op.compute(a)\n",
       "    >>> print(result)\n",
       "    Tensor([[1, 2, 3],\n",
       "             [4, 5, 6]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(Reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9fc235f7-c1f4-42b5-b3a1-d9cb909617d9",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
