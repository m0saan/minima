{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: The `operators` module in this framework provides a collection of tensor\n",
    "  operations for building computational graphs in deep learning. Each class in this\n",
    "  module represents a different type of operation that can be performed on tensors,\n",
    "  such as element-wise addition, scalar multiplication, division, exponentiation,\n",
    "  etc.\n",
    "output-file: operators.html\n",
    "title: operators\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9a4956-5575-4950-9365-560225c3a715",
   "metadata": {},
   "source": [
    "The `out_grad` parameter refers to the gradient of the loss function with respect to the output of the node. Multiplying this with the local gradient gives the gradient of the loss with respect to the input to the node, according to the chain rule of calculus, which is the basis for backpropagation in neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c0867e-7744-4c76-95b6-da3868dc8625",
   "metadata": {},
   "source": [
    "The chain rule is a fundamental concept in calculus that provides a method to compute the derivative of composite functions. In simple terms, the chain rule states that the derivative of a composite function is the derivative of the outer function multiplied by the derivative of the inner function.\n",
    "\n",
    "Given a composite function that is the composition of two functions, say, $f(g(x))$, the chain rule can be stated as follows:\n",
    "\n",
    "$$\\frac{df}{dx} = \\frac{df}{dg} \\cdot \\frac{dg}{dx}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\frac{df}{dx}$ is the derivative of the composite function $f(g(x))$ with respect to $x$,\n",
    "- $\\frac{df}{dg}$ is the derivative of the outer function $f$ with respect to its argument $g(x)$, and\n",
    "- $\\frac{dg}{dx}$ is the derivative of the inner function $g(x)$ with respect to $x$.\n",
    "\n",
    "The chain rule can be extended to the case where we have more than two composite functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cb2320-c471-426d-ad48-0197b1daecaa",
   "metadata": {},
   "source": [
    "## Element Wise Addition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feadcd15-44d4-4a3d-aef9-39b3b7f6fcd1",
   "metadata": {},
   "source": [
    "Let's walk through the step-by-step derivative calculation for the [`EWiseAdd`](https://m0saan.github.io/minima/operators.html#ewiseadd) operation:\n",
    "\n",
    "We have the function `f(a, b) = a + b`, where `a` and `b` are tensors. Our goal is to compute the partial derivatives with respect to `a` and `b`.\n",
    "\n",
    "Let's start by calculating the derivative of `f` with respect to `a`, denoted as `df/da`:\n",
    "\n",
    "Step 1: Compute the derivative of `f` with respect to `a`.\n",
    "\n",
    "$\\frac{{\\partial f}}{{\\partial a}} = \\frac{{\\partial}}{{\\partial a}} (a + b)$\n",
    "\n",
    "Since `a` is the variable we are differentiating with respect to, the derivative of `a` with respect to itself is 1:\n",
    "\n",
    "$$\\frac{{\\partial f}}{{\\partial a}} = 1$$\n",
    "\n",
    "Therefore, $$\\frac{{\\partial f}}{{\\partial a}} = 1.$$\n",
    "\n",
    "Step 2: Compute the derivative of `f` with respect to `b`.\n",
    "\n",
    "$$\\frac{{\\partial f}}{{\\partial b}} = \\frac{{\\partial}}{{\\partial b}} (a + b)$$\n",
    "\n",
    "Again, since `b` is the variable we are differentiating with respect to, the derivative of `b` with respect to itself is 1:\n",
    "\n",
    "$$\\frac{{\\partial f}}{{\\partial b}} = 1$$\n",
    "\n",
    "Therefore, $$\\frac{{\\partial f}}{{\\partial b}} = 1$$\n",
    "\n",
    "Hence, the partial derivatives of `f(a, b) = a + b` with respect to `a` and `b` are both equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L61){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### add\n",
       "\n",
       ">      add (a:minima.autograd.Tensor, b:minima.autograd.Tensor)\n",
       "\n",
       "Adds two tensors element-wise.\n",
       "\n",
       "Args:\n",
       "- a: The first tensor.\n",
       "- b: The second tensor.\n",
       "\n",
       "Returns:\n",
       "The element-wise sum of a and b."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L61){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### add\n",
       "\n",
       ">      add (a:minima.autograd.Tensor, b:minima.autograd.Tensor)\n",
       "\n",
       "Adds two tensors element-wise.\n",
       "\n",
       "Args:\n",
       "- a: The first tensor.\n",
       "- b: The second tensor.\n",
       "\n",
       "Returns:\n",
       "The element-wise sum of a and b."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L22){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### EWiseAdd\n",
       "\n",
       ">      EWiseAdd ()\n",
       "\n",
       "Performs element-wise addition of two tensors.\n",
       "\n",
       "Example:\n",
       ">>> a = Tensor([1, 2, 3])\n",
       ">>> b = Tensor([4, 5, 6])\n",
       ">>> op = EWiseAdd()\n",
       ">>> result = op.compute(a, b)\n",
       ">>> print(result)\n",
       "Tensor([5, 7, 9])"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L22){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### EWiseAdd\n",
       "\n",
       ">      EWiseAdd ()\n",
       "\n",
       "Performs element-wise addition of two tensors.\n",
       "\n",
       "Example:\n",
       ">>> a = Tensor([1, 2, 3])\n",
       ">>> b = Tensor([4, 5, 6])\n",
       ">>> op = EWiseAdd()\n",
       ">>> result = op.compute(a, b)\n",
       ">>> print(result)\n",
       "Tensor([5, 7, 9])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(EWiseAdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd56c389-9132-4744-bcb6-cd85dc084a2d",
   "metadata": {},
   "source": [
    "Create two 1-D tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358fcf64-9c44-4d44-8374-a0ef11668d6e",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "a = Tensor([1, 2, 3])\n",
    "b = Tensor([4, 5, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cddb288-2e63-4d19-ad07-f708fa7c2dc9",
   "metadata": {},
   "source": [
    "Create an EWiseAdd operation instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81b16cf-a61b-4771-8799-259daefa971f",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "op = EWiseAdd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b5a98f-cb3f-460b-9618-5457c26af297",
   "metadata": {},
   "source": [
    "Compute the element-wise sum of a and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349ecf89-aa8f-47e6-8088-24175baa0819",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 7 9]\n"
     ]
    }
   ],
   "source": [
    "result = op.compute(a, b)\n",
    "print(result) # Output: Tensor([5, 7, 9])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df6a9f8-8f04-4098-87dc-b497137f73e9",
   "metadata": {},
   "source": [
    "Alternatively, you can use the add function directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6db6072-65fb-4cad-af75-e87236dbc921",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "minima.Tensor([5 7 9])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = add(a, b)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c4c99b-535c-4049-aa09-ae2b2b26c03d",
   "metadata": {},
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba584690-7fe5-4d93-95d7-2cd7e373811b",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "minima.Tensor([5 7 9])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dc5f72-9776-45d4-8d47-3e0313c75885",
   "metadata": {},
   "source": [
    "For 2-D tensors, we can compute the element-wise sum of a and b in the same way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbbc80e-a2f0-46d2-97df-35344197da88",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "minima.Tensor([[ 8 10 12]\n",
       " [14 16 18]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Tensor([[1, 2, 3], [4, 5, 6]])\n",
    "b = Tensor([[7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "result = op.compute(a, b)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd371e81-b6e4-43da-9987-30057c5c038a",
   "metadata": {},
   "source": [
    "## Scalar Addition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dbc8dc-25ae-4793-9cc8-cdbef59a8400",
   "metadata": {},
   "source": [
    "Explanation for the derivative of the [`AddScalar`](https://m0saan.github.io/minima/operators.html#addscalar) operator:\n",
    "\n",
    "Let's denote the scalar as `c` and `a` as the tensor being added by the scalar. The operation can be described as `f(a) = a + c`.\n",
    "\n",
    "The function for the backward pass (i.e., the gradient) is `df/da = 1`, which means the derivative of `f(a)` with respect to `a` is simply `1`.\n",
    "\n",
    "We are given a function $f(a) = a + c$, where $a$ is a tensor and $c$ is a scalar. Our task is to find the derivative of this function with respect to $a$.\n",
    "\n",
    "By differentiating the function $f(a)$ with respect to $a$, we find:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{da} &= \\frac{d}{da} (a + c) \\\\\n",
    "&= 1\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, the gradient of $f(a)$ with respect to $a$ is $1$.\n",
    "\n",
    "\n",
    "We starts by defining the function `f(a) = a + c`. It then explains that when we differentiate `f(a)` with respect to `a`, we find that the derivative is `1`. This means that the gradient of `f(a)` with respect to `a` is `1`, which matches the behavior of the [`AddScalar`](https://m0saan.github.io/minima/operators.html#addscalar) operator as provided in the `gradient` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L120){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### add_scalar\n",
       "\n",
       ">      add_scalar (a:minima.autograd.Tensor, scalar:Union[int,float])\n",
       "\n",
       "Adds a scalar to a tensor.\n",
       "\n",
       "Args:\n",
       "- a: The tensor.\n",
       "- scalar: The scalar to add.\n",
       "\n",
       "Returns:\n",
       "The sum of a and the scalar."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L120){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### add_scalar\n",
       "\n",
       ">      add_scalar (a:minima.autograd.Tensor, scalar:Union[int,float])\n",
       "\n",
       "Adds a scalar to a tensor.\n",
       "\n",
       "Args:\n",
       "- a: The tensor.\n",
       "- scalar: The scalar to add.\n",
       "\n",
       "Returns:\n",
       "The sum of a and the scalar."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(add_scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L75){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AddScalar\n",
       "\n",
       ">      AddScalar (scalar:Union[int,float])\n",
       "\n",
       "Performs addition of a tensor and a scalar.\n",
       "\n",
       "Example:\n",
       ">>> a = Tensor([1, 2, 3])\n",
       ">>> op = AddScalar(5)\n",
       ">>> result = op.compute(a)\n",
       ">>> print(result)\n",
       "Tensor([6, 7, 8])"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L75){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AddScalar\n",
       "\n",
       ">      AddScalar (scalar:Union[int,float])\n",
       "\n",
       "Performs addition of a tensor and a scalar.\n",
       "\n",
       "Example:\n",
       ">>> a = Tensor([1, 2, 3])\n",
       ">>> op = AddScalar(5)\n",
       ">>> result = op.compute(a)\n",
       ">>> print(result)\n",
       "Tensor([6, 7, 8])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(AddScalar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26ed99f-b3f2-4df1-9918-ff23fc99be74",
   "metadata": {},
   "source": [
    "## Element Wise Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf5a7d0-8e8a-47cd-a4b5-cdc12a03649c",
   "metadata": {},
   "source": [
    "Explanation for the derivative of the [`EWiseMul`](https://m0saan.github.io/minima/operators.html#ewisemul) (element-wise multiplication) operator:\n",
    "\n",
    "Let's denote the two input tensors as `a` and `b`. The operation can be described as `f(a, b) = a * b`, where `*` represents element-wise multiplication.\n",
    "\n",
    "The function for the backward pass (i.e., the gradient) is `df/da = b` and `df/db = a`. This means that the derivative of `f(a, b)` with respect to `a` is `b`, and the derivative with respect to `b` is `a`.\n",
    "\n",
    "\n",
    "We are given a function $f(a, b) = a \\odot b$, where $a$ and $b$ are tensors, and $\\odot$ represents element-wise multiplication. Our task is to find the derivatives of this function with respect to $a$ and $b$.\n",
    "\n",
    "By differentiating the function $f(a, b)$ with respect to $a$, we find:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{da} &= \\frac{d}{da} (a \\odot b) \\\\\n",
    "&= b\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, the gradient of $f(a, b)$ with respect to $a$ is $b$.\n",
    "\n",
    "Similarly, by differentiating the function $f(a, b)$ with respect to $b$, we find:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{db} &= \\frac{d}{db} (a \\odot b) \\\\\n",
    "&= a\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, the gradient of $f(a, b)$ with respect to $b$ is $a$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L173){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### multiply\n",
       "\n",
       ">      multiply (a:minima.autograd.Tensor, b:minima.autograd.Tensor)\n",
       "\n",
       "Multiplies two tensors element-wise.\n",
       "\n",
       "Args:\n",
       "- a: The first tensor.\n",
       "- b: The second tensor.\n",
       "\n",
       "Returns:\n",
       "The element-wise product of a and b."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L173){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### multiply\n",
       "\n",
       ">      multiply (a:minima.autograd.Tensor, b:minima.autograd.Tensor)\n",
       "\n",
       "Multiplies two tensors element-wise.\n",
       "\n",
       "Args:\n",
       "- a: The first tensor.\n",
       "- b: The second tensor.\n",
       "\n",
       "Returns:\n",
       "The element-wise product of a and b."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(multiply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L134){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### EWiseMul\n",
       "\n",
       ">      EWiseMul ()\n",
       "\n",
       "Performs element-wise multiplication of two tensors.\n",
       "\n",
       "Example:\n",
       ">>> a = Tensor([1, 2, 3])\n",
       ">>> b = Tensor([4, 5, 6])\n",
       ">>> op = EWiseMul()\n",
       ">>> result = op.compute(a, b)\n",
       ">>> print(result)\n",
       "Tensor([4, 10, 18])"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L134){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### EWiseMul\n",
       "\n",
       ">      EWiseMul ()\n",
       "\n",
       "Performs element-wise multiplication of two tensors.\n",
       "\n",
       "Example:\n",
       ">>> a = Tensor([1, 2, 3])\n",
       ">>> b = Tensor([4, 5, 6])\n",
       ">>> op = EWiseMul()\n",
       ">>> result = op.compute(a, b)\n",
       ">>> print(result)\n",
       "Tensor([4, 10, 18])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(EWiseMul)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb531c5-f22c-40c9-901e-e373b837a846",
   "metadata": {},
   "source": [
    "## Scalar Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0246b9-ce8f-4fab-991d-7ec43745c2ea",
   "metadata": {},
   "source": [
    "Let's denote the scalar as `c` and `a` as the tensor being multiplied by the scalar. The operation can be described as `f(a) = a * c`.\n",
    "\n",
    "The function for the backward pass (i.e., the gradient) is `df/da = c`, which means the derivative of `f(a)` with respect to `a` is `c`.\n",
    "\n",
    "The LaTeX document will look as follows:\n",
    "\n",
    "We are given a function $f(a) = a \\cdot c$, where $a$ is a tensor and $c$ is a scalar. Our task is to find the derivative of this function with respect to $a$.\n",
    "\n",
    "By differentiating the function $f(a)$ with respect to $a$, we find:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{da} &= \\frac{d}{da} (a \\cdot c) \\\\\n",
    "&= c\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, the gradient of $f(a)$ with respect to $a$ is $c$.\n",
    "\n",
    "We starts by defining the function `f(a) = a * c`. It then explains that when we differentiate `f(a)` with respect to `a`, we find that the derivative is `c`. This means that the gradient of `f(a)` with respect to `a` is `c`, which matches the behavior of the [`MulScalar`](https://m0saan.github.io/minima/operators.html#mulscalar) operator as provided in the `gradient` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L232){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### mul_scalar\n",
       "\n",
       ">      mul_scalar (a:minima.autograd.Tensor, scalar:Union[int,float])\n",
       "\n",
       "Multiplies a tensor by a scalar.\n",
       "\n",
       "Args:\n",
       "- a: The tensor.\n",
       "- scalar: The scalar to multiply.\n",
       "\n",
       "Returns:\n",
       "The product of a and the scalar."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L232){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### mul_scalar\n",
       "\n",
       ">      mul_scalar (a:minima.autograd.Tensor, scalar:Union[int,float])\n",
       "\n",
       "Multiplies a tensor by a scalar.\n",
       "\n",
       "Args:\n",
       "- a: The tensor.\n",
       "- scalar: The scalar to multiply.\n",
       "\n",
       "Returns:\n",
       "The product of a and the scalar."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(mul_scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L187){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MulScalar\n",
       "\n",
       ">      MulScalar (scalar:Union[int,float])\n",
       "\n",
       "Performs multiplication of a tensor and a scalar.\n",
       "\n",
       "Example:\n",
       ">>> a = Tensor([1, 2, 3])\n",
       ">>> op = MulScalar(5)\n",
       ">>> result = op.compute(a)\n",
       ">>> print(result)\n",
       "Tensor([5, 10, 15])"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/operators.py#L187){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MulScalar\n",
       "\n",
       ">      MulScalar (scalar:Union[int,float])\n",
       "\n",
       "Performs multiplication of a tensor and a scalar.\n",
       "\n",
       "Example:\n",
       ">>> a = Tensor([1, 2, 3])\n",
       ">>> op = MulScalar(5)\n",
       ">>> result = op.compute(a)\n",
       ">>> print(result)\n",
       "Tensor([5, 10, 15])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(MulScalar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6497d8ab-3003-4784-a330-ad5f862b9ca5",
   "metadata": {},
   "source": [
    "## Negation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04085053-1446-4343-b720-17e04a1c4ee1",
   "metadata": {},
   "source": [
    "Let's denote `a` as the tensor being negated. The operation can be described as `f(a) = -a`.\n",
    "\n",
    "The function for the backward pass (i.e., the gradient) is `df/da = -1`.\n",
    "\n",
    "We are given a function $f(a) = -a$, where $a$ is a tensor. Our task is to find the derivative of this function with respect to $a$.\n",
    "\n",
    "By differentiating the function $f(a)$ with respect to $a$, we find:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{da} &= \\frac{d}{da} (-a) \\\\\n",
    "&= -1\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, the gradient of $f(a)$ with respect to $a$ is $-1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### negate\n",
       "\n",
       ">      negate (a:minima.autograd.Tensor)\n",
       "\n",
       "Negates the given tensor.\n",
       "\n",
       "Args:\n",
       "- a: The tensor to negate.\n",
       "\n",
       "Returns:\n",
       "The negation of a.\n",
       "\n",
       "Example:\n",
       ">>> a = Tensor([1, -2, 3])\n",
       ">>> result = negate(a)\n",
       ">>> print(result)\n",
       "Tensor([-1, 2, -3])"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### negate\n",
       "\n",
       ">      negate (a:minima.autograd.Tensor)\n",
       "\n",
       "Negates the given tensor.\n",
       "\n",
       "Args:\n",
       "- a: The tensor to negate.\n",
       "\n",
       "Returns:\n",
       "The negation of a.\n",
       "\n",
       "Example:\n",
       ">>> a = Tensor([1, -2, 3])\n",
       ">>> result = negate(a)\n",
       ">>> print(result)\n",
       "Tensor([-1, 2, -3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(negate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### Negate\n",
       "\n",
       ">      Negate ()\n",
       "\n",
       "Negates the given tensor.\n",
       "\n",
       "Example:\n",
       ">>> a = Tensor([1, -2, 3])\n",
       ">>> op = Negate()\n",
       ">>> result = op.compute(a)\n",
       ">>> print(result)\n",
       "Tensor([-1, 2, -3])"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### Negate\n",
       "\n",
       ">      Negate ()\n",
       "\n",
       "Negates the given tensor.\n",
       "\n",
       "Example:\n",
       ">>> a = Tensor([1, -2, 3])\n",
       ">>> op = Negate()\n",
       ">>> result = op.compute(a)\n",
       ">>> print(result)\n",
       "Tensor([-1, 2, -3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(Negate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e28e521-a653-45e0-a567-46c7b800d281",
   "metadata": {},
   "source": [
    "## Exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8bf43a-9a2a-4077-a54f-54df0c6c955d",
   "metadata": {},
   "source": [
    "Explanation for the derivative of the `Exp` operator:\n",
    "\n",
    "Let's denote `a` as the tensor on which the exponential function is applied. The operation can be described as `f(a) = exp(a)`, where `exp` represents the exponential function.\n",
    "\n",
    "The function for the backward pass (i.e., the gradient) is `df/da = exp(a)`.\n",
    "\n",
    "We are given a function $f(a) = \\exp(a)$, where $a$ is a tensor. Our task is to find the derivative of this function with respect to $a$.\n",
    "\n",
    "By differentiating the function $f(a)$ with respect to $a$, we find:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{da} &= \\frac{d}{da} (\\exp(a)) \\\\\n",
    "&= \\exp(a)\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, the gradient of $f(a)$ with respect to $a$ is $\\exp(a)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### exp\n",
       "\n",
       ">      exp (a:minima.autograd.Tensor)\n",
       "\n",
       "Calculates the exponential of the given tensor.\n",
       "\n",
       "Args:\n",
       "- a: The tensor.\n",
       "\n",
       "Returns:\n",
       "The exponential of a.\n",
       "\n",
       "Example:\n",
       ">>> a = Tensor([1, 2, 3])\n",
       ">>> result = exp(a)\n",
       ">>> print(result)\n",
       "Tensor([2.71828183, 7.3890561, 20.08553692])"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### exp\n",
       "\n",
       ">      exp (a:minima.autograd.Tensor)\n",
       "\n",
       "Calculates the exponential of the given tensor.\n",
       "\n",
       "Args:\n",
       "- a: The tensor.\n",
       "\n",
       "Returns:\n",
       "The exponential of a.\n",
       "\n",
       "Example:\n",
       ">>> a = Tensor([1, 2, 3])\n",
       ">>> result = exp(a)\n",
       ">>> print(result)\n",
       "Tensor([2.71828183, 7.3890561, 20.08553692])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### Exp\n",
       "\n",
       ">      Exp ()\n",
       "\n",
       "Calculates the exponential of the given tensor.\n",
       "\n",
       "Example:\n",
       ">>> a = Tensor([1, 2, 3])\n",
       ">>> op = Exp()\n",
       ">>> result = op.compute(a)\n",
       ">>> print(result)\n",
       "Tensor([2.71828183, 7.3890561, 20.08553692])"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### Exp\n",
       "\n",
       ">      Exp ()\n",
       "\n",
       "Calculates the exponential of the given tensor.\n",
       "\n",
       "Example:\n",
       ">>> a = Tensor([1, 2, 3])\n",
       ">>> op = Exp()\n",
       ">>> result = op.compute(a)\n",
       ">>> print(result)\n",
       "Tensor([2.71828183, 7.3890561, 20.08553692])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(Exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45592e37-9a6d-42ec-8458-167a94394cc1",
   "metadata": {},
   "source": [
    "## ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8aefd6-23e2-4df7-8627-f74813b8f0bc",
   "metadata": {},
   "source": [
    "The derivative of the `ReLU` (Rectified Linear Unit) operator:\n",
    "\n",
    "Let's denote `a` as the tensor on which the ReLU function is applied. The ReLU function is defined as follows: \n",
    "\n",
    "$$\n",
    "f(a) = \n",
    "\\begin{cases}\n",
    "a, & \\text{if } a \\geq 0 \\\\\n",
    "0, & \\text{if } a < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The function for the backward pass (i.e., the gradient) is `df/da = 1` if `a >= 0`, and `df/da = 0` if `a < 0`.\n",
    "\n",
    "We are given a function $f(a) = \\max(0, a)$, where $a$ is a tensor. Our task is to find the derivative of this function with respect to $a$.\n",
    "\n",
    "By considering the definition of the ReLU function, we can write $f(a)$ as:\n",
    "\n",
    "$$\n",
    "f(a) = \n",
    "\\begin{cases}\n",
    "a, & \\text{if } a \\geq 0 \\\\\n",
    "0, & \\text{if } a < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Now, let's differentiate $f(a)$ with respect to $a$:\n",
    "\n",
    "$$\n",
    "\\frac{df}{da} = \n",
    "\\begin{cases}\n",
    "1, & \\text{if } a \\geq 0 \\\\\n",
    "0, & \\text{if } a < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Therefore, the gradient of $f(a)$ with respect to $a$ is $1$ if $a \\geq 0$, and $0$ if $a < 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### relu\n",
       "\n",
       ">      relu (a:minima.autograd.Tensor)\n",
       "\n",
       "Applies the ReLU (Rectified Linear Unit) activation function to the given tensor.\n",
       "\n",
       "Args:\n",
       "- a: The tensor.\n",
       "\n",
       "Returns:\n",
       "The result of applying ReLU to a.\n",
       "\n",
       "Example:\n",
       ">>> a = Tensor([1, -2, 3])\n",
       ">>> result = relu(a)\n",
       ">>> print(result)\n",
       "Tensor([1, 0, 3])"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### relu\n",
       "\n",
       ">      relu (a:minima.autograd.Tensor)\n",
       "\n",
       "Applies the ReLU (Rectified Linear Unit) activation function to the given tensor.\n",
       "\n",
       "Args:\n",
       "- a: The tensor.\n",
       "\n",
       "Returns:\n",
       "The result of applying ReLU to a.\n",
       "\n",
       "Example:\n",
       ">>> a = Tensor([1, -2, 3])\n",
       ">>> result = relu(a)\n",
       ">>> print(result)\n",
       "Tensor([1, 0, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### ReLU\n",
       "\n",
       ">      ReLU ()\n",
       "\n",
       "Applies the ReLU (Rectified Linear Unit) activation function to the given tensor.\n",
       "\n",
       "Example:\n",
       ">>> a = Tensor([1, -2, 3])\n",
       ">>> op = ReLU()\n",
       ">>> result = op.compute(a)\n",
       ">>> print(result)\n",
       "Tensor([1, 0, 3])"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### ReLU\n",
       "\n",
       ">      ReLU ()\n",
       "\n",
       "Applies the ReLU (Rectified Linear Unit) activation function to the given tensor.\n",
       "\n",
       "Example:\n",
       ">>> a = Tensor([1, -2, 3])\n",
       ">>> op = ReLU()\n",
       ">>> result = op.compute(a)\n",
       ">>> print(result)\n",
       "Tensor([1, 0, 3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(ReLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9f074a-e23c-4e8a-8ab7-2cfba344c461",
   "metadata": {},
   "source": [
    "## Power Scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d5e875-0981-4243-86b5-d6ca6b117d5e",
   "metadata": {},
   "source": [
    "The derivative of the `PowerScalar` operator:\n",
    "\n",
    "Let's denote the scalar as `n` and `a` as the tensor being raised to the power of the scalar. The operation can be described as `f(a) = a^n`.\n",
    "\n",
    "The function for the backward pass (i.e., the gradient) is `df/da = n * a^(n-1)`.\n",
    "\n",
    "We are given a function $f(a) = a^n$, where $a$ is a tensor and $n$ is a scalar. Our task is to find the derivative of this function with respect to $a$.\n",
    "\n",
    "By differentiating the function $f(a)$ with respect to $a$, we find:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{da} &= \\frac{d}{da} (a^n) \\\\\n",
    "&= n \\cdot a^{n-1}\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, the gradient of $f(a)$ with respect to $a$ is $n \\cdot a^{n-1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### power_scalar\n",
       "\n",
       ">      power_scalar (a:minima.autograd.Tensor, scalar:int)\n",
       "\n",
       "Raises a tensor to a power.\n",
       "\n",
       "Args:\n",
       "    a (Tensor): The input tensor.\n",
       "    scalar (int): The power to raise the tensor to.\n",
       "\n",
       "Returns:\n",
       "    Tensor: The resulting tensor after the power operation.\n",
       "\n",
       "Example:\n",
       "    >>> import numpy as np\n",
       "    >>> tensor = Tensor(np.array([1, 2, 3]))\n",
       "    >>> result = power_scalar(tensor, 2)\n",
       "    >>> print(result)\n",
       "    Tensor([1, 4, 9])"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### power_scalar\n",
       "\n",
       ">      power_scalar (a:minima.autograd.Tensor, scalar:int)\n",
       "\n",
       "Raises a tensor to a power.\n",
       "\n",
       "Args:\n",
       "    a (Tensor): The input tensor.\n",
       "    scalar (int): The power to raise the tensor to.\n",
       "\n",
       "Returns:\n",
       "    Tensor: The resulting tensor after the power operation.\n",
       "\n",
       "Example:\n",
       "    >>> import numpy as np\n",
       "    >>> tensor = Tensor(np.array([1, 2, 3]))\n",
       "    >>> result = power_scalar(tensor, 2)\n",
       "    >>> print(result)\n",
       "    Tensor([1, 4, 9])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(power_scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### PowerScalar\n",
       "\n",
       ">      PowerScalar (scalar:int)\n",
       "\n",
       "The PowerScalar operation raises a tensor to an (integer) power.\n",
       "\n",
       "Attributes:\n",
       "    scalar (int): The power to raise the tensor to.\n",
       "\n",
       "Example:\n",
       "    >>> import numpy as np\n",
       "    >>> tensor = Tensor(np.array([1, 2, 3]))\n",
       "    >>> pow_scalar = PowerScalar(2)\n",
       "    >>> result = pow_scalar.compute(tensor.data)\n",
       "    >>> print(result)\n",
       "    array([1, 4, 9])"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### PowerScalar\n",
       "\n",
       ">      PowerScalar (scalar:int)\n",
       "\n",
       "The PowerScalar operation raises a tensor to an (integer) power.\n",
       "\n",
       "Attributes:\n",
       "    scalar (int): The power to raise the tensor to.\n",
       "\n",
       "Example:\n",
       "    >>> import numpy as np\n",
       "    >>> tensor = Tensor(np.array([1, 2, 3]))\n",
       "    >>> pow_scalar = PowerScalar(2)\n",
       "    >>> result = pow_scalar.compute(tensor.data)\n",
       "    >>> print(result)\n",
       "    array([1, 4, 9])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(PowerScalar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbb40d3-1ed4-4a7f-9d74-b39b70187860",
   "metadata": {},
   "source": [
    "## Element Wise Divide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a7c7e2-d71d-49c0-9c45-ffc7f0830334",
   "metadata": {},
   "source": [
    "The operation described here is an element-wise division of two tensors, `a` and `b`, where the operation can be described as `f(a, b) = a / b`. \n",
    "\n",
    "We'll compute the partial derivatives with respect to `a` and `b`:\n",
    "\n",
    "1. The partial derivative of `f(a, b)` with respect to `a` (`df/da`) is `1/b`.\n",
    "\n",
    "2. The partial derivative of `f(a, b)` with respect to `b` (`df/db`) is `-a / b^2`.\n",
    "\n",
    "We are given a function $f(a, b) = \\frac{a}{b}$, where $a$ and $b$ are tensors. Our task is to find the partial derivatives of this function with respect to $a$ and $b$.\n",
    "\n",
    "Let's start with $\\frac{\\partial f}{\\partial a}$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial f}{\\partial a} &= \\frac{\\partial}{\\partial a} \\left(\\frac{a}{b}\\right) \\\\\n",
    "&= \\frac{1}{b}\n",
    "\\end{align*}\n",
    "\n",
    "Now, let's compute $\\frac{\\partial f}{\\partial b}$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial f}{\\partial b} &= \\frac{\\partial}{\\partial b} \\left(\\frac{a}{b}\\right) \\\\\n",
    "&= - \\frac{a}{b^{2}}\n",
    "\\end{align*}\n",
    "\n",
    "Here is a detailed derivative:\n",
    "\n",
    "Given a function of the form $y = \\frac{u}{v}$, where both $u$ and $v$ are functions of $x$, the quotient rule of differentiation states:\n",
    "\n",
    "$$\\frac{dy}{dx} = \\frac{v \\cdot \\frac{du}{dx} - u \\cdot \\frac{dv}{dx}}{v^2}$$\n",
    "\n",
    "In our case, we're looking at the function $y = \\frac{a}{b}$, where $a$ and $b$ are tensors. We want to find the derivative with respect to $b$ (instead of $x$ in our general formula). So we have:\n",
    "\n",
    "$$\\frac{dy}{db} = \\frac{b \\cdot \\frac{da}{db} - a \\cdot \\frac{db}{db}}{b^2}$$\n",
    "\n",
    "Since $a$ does not depend on $b$, $\\frac{da}{db} = 0$, and since any variable is equal to itself, $\\frac{db}{db} = 1$. \n",
    "\n",
    "So the derivative $\\frac{dy}{db}$ simplifies to:\n",
    "\n",
    "$$\\frac{dy}{db} = \\frac{b \\cdot 0 - a \\cdot 1}{b^2}$$\n",
    "\n",
    "Therefore, the derivative of $y$ with respect to $b$ is $-\\frac{a}{b^2}$.\n",
    "\n",
    "Therefore, the gradient of $f(a, b)$ with respect to $a$ is $\\frac{1}{b}$, and the gradient of $f(a, b)$ with respect to $b$ is $- \\frac{a}{b^{2}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### divide\n",
       "\n",
       ">      divide (a:minima.autograd.Tensor, b:minima.autograd.Tensor)\n",
       "\n",
       "Divides two tensors element-wise.\n",
       "\n",
       "Args:\n",
       "    a (Tensor): The dividend tensor.\n",
       "    b (Tensor): The divisor tensor.\n",
       "\n",
       "Returns:\n",
       "    Tensor: The resulting tensor after element-wise division.\n",
       "\n",
       "Example:\n",
       "    >>> import numpy as np\n",
       "    >>> a = Tensor(np.array([1, 2, 3]))\n",
       "    >>> b = Tensor(np.array([4, 5, 6]))\n",
       "    >>> result = divide(a, b)\n",
       "    >>> print(result)\n",
       "    Tensor([0.25, 0.4, 0.5])"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### divide\n",
       "\n",
       ">      divide (a:minima.autograd.Tensor, b:minima.autograd.Tensor)\n",
       "\n",
       "Divides two tensors element-wise.\n",
       "\n",
       "Args:\n",
       "    a (Tensor): The dividend tensor.\n",
       "    b (Tensor): The divisor tensor.\n",
       "\n",
       "Returns:\n",
       "    Tensor: The resulting tensor after element-wise division.\n",
       "\n",
       "Example:\n",
       "    >>> import numpy as np\n",
       "    >>> a = Tensor(np.array([1, 2, 3]))\n",
       "    >>> b = Tensor(np.array([4, 5, 6]))\n",
       "    >>> result = divide(a, b)\n",
       "    >>> print(result)\n",
       "    Tensor([0.25, 0.4, 0.5])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(divide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### EWiseDiv\n",
       "\n",
       ">      EWiseDiv ()\n",
       "\n",
       "The EWiseDiv operation divides two tensors element-wise.\n",
       "\n",
       "Example:\n",
       "    >>> import numpy as np\n",
       "    >>> a = Tensor(np.array([1, 2, 3]))\n",
       "    >>> b = Tensor(np.array([4, 5, 6]))\n",
       "    >>> div = EWiseDiv()\n",
       "    >>> result = div.compute(a.data, b.data)\n",
       "    >>> print(result)\n",
       "    array([0.25, 0.4, 0.5])"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### EWiseDiv\n",
       "\n",
       ">      EWiseDiv ()\n",
       "\n",
       "The EWiseDiv operation divides two tensors element-wise.\n",
       "\n",
       "Example:\n",
       "    >>> import numpy as np\n",
       "    >>> a = Tensor(np.array([1, 2, 3]))\n",
       "    >>> b = Tensor(np.array([4, 5, 6]))\n",
       "    >>> div = EWiseDiv()\n",
       "    >>> result = div.compute(a.data, b.data)\n",
       "    >>> print(result)\n",
       "    array([0.25, 0.4, 0.5])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(EWiseDiv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7daf46-5e14-4bf7-9f7f-81947e7e7cd3",
   "metadata": {},
   "source": [
    "## Divide Scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d98d2f-34a5-4744-994a-9c9264bfb4a9",
   "metadata": {},
   "source": [
    "Let's denote the scalar as `c`, and `a` as the tensor being divided by the scalar. The operation can be described as `f(a) = a / c`.\n",
    "\n",
    "The function for the backward pass (i.e., the gradient) is `df/da = 1/c`.\n",
    "\n",
    "This is the derivative of `f(a)` with respect to `a`.\n",
    "\n",
    "We are given a function $f(a) = \\frac{a}{c}$, where $a$ is a tensor and $c$ is a scalar. Our task is to find the derivative of this function with respect to $a$.\n",
    "\n",
    "By using the power rule of differentiation, where the derivative of $a^n$ is $n \\cdot a^{n-1}$, we can rewrite $f(a)$ as $f(a) = c^{-1}a$. \n",
    "\n",
    "Now, we can differentiate this with respect to $a$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{da} &= \\frac{d}{da} (c^{-1}a) \\\\\n",
    "&= c^{-1} \\frac{d}{da} (a) \\\\\n",
    "&= c^{-1} \\\\\n",
    "&= \\frac{1}{c}\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, the gradient of $f(a)$ with respect to $a$ is $\\frac{1}{c}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### divide_scalar\n",
       "\n",
       ">      divide_scalar (a:minima.autograd.Tensor, scalar:Union[int,float])\n",
       "\n",
       "Divides a tensor by a scalar.\n",
       "\n",
       "Args:\n",
       "    a (Tensor): The tensor to divide.\n",
       "    scalar (int, float): The scalar to divide the tensor by.\n",
       "\n",
       "Returns:\n",
       "    Tensor: The resulting tensor after division.\n",
       "\n",
       "Example:\n",
       "    >>> import numpy as np\n",
       "    >>> a = Tensor(np.array([1, 2, 3]))\n",
       "    >>> scalar = 2\n",
       "    >>> result = divide_scalar(a, scalar)\n",
       "    >>> print(result)\n",
       "    Tensor([0.5, 1.0, 1.5])"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### divide_scalar\n",
       "\n",
       ">      divide_scalar (a:minima.autograd.Tensor, scalar:Union[int,float])\n",
       "\n",
       "Divides a tensor by a scalar.\n",
       "\n",
       "Args:\n",
       "    a (Tensor): The tensor to divide.\n",
       "    scalar (int, float): The scalar to divide the tensor by.\n",
       "\n",
       "Returns:\n",
       "    Tensor: The resulting tensor after division.\n",
       "\n",
       "Example:\n",
       "    >>> import numpy as np\n",
       "    >>> a = Tensor(np.array([1, 2, 3]))\n",
       "    >>> scalar = 2\n",
       "    >>> result = divide_scalar(a, scalar)\n",
       "    >>> print(result)\n",
       "    Tensor([0.5, 1.0, 1.5])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(divide_scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### DivScalar\n",
       "\n",
       ">      DivScalar (scalar:Union[int,float])\n",
       "\n",
       "The DivScalar operation divides a tensor by a scalar.\n",
       "\n",
       "Example:\n",
       "    >>> import numpy as np\n",
       "    >>> a = Tensor(np.array([1, 2, 3]))\n",
       "    >>> scalar = 2\n",
       "    >>> div_scalar = DivScalar(scalar)\n",
       "    >>> result = div_scalar.compute(a.data)\n",
       "    >>> print(result)\n",
       "    array([0.5, 1.0, 1.5])"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### DivScalar\n",
       "\n",
       ">      DivScalar (scalar:Union[int,float])\n",
       "\n",
       "The DivScalar operation divides a tensor by a scalar.\n",
       "\n",
       "Example:\n",
       "    >>> import numpy as np\n",
       "    >>> a = Tensor(np.array([1, 2, 3]))\n",
       "    >>> scalar = 2\n",
       "    >>> div_scalar = DivScalar(scalar)\n",
       "    >>> result = div_scalar.compute(a.data)\n",
       "    >>> print(result)\n",
       "    array([0.5, 1.0, 1.5])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(DivScalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc235f7-c1f4-42b5-b3a1-d9cb909617d9",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6c30be-1222-4b58-947d-019db44d20f8",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
