{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: The `Optim` module in minima is a flexible and powerful toolbox for optimizing\n",
    "  the parameters of your deep learning models. Built on a high-level,\n",
    "output-file: optim.html\n",
    "title: optim\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/m0saan/mambaforge/lib/python3.10/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Raises\n",
      "  else: warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/optim.py#L14){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Optimizer\n",
       "\n",
       ">      Optimizer (params)\n",
       "\n",
       "Base class for all optimizers. Not meant to be instantiated directly.\n",
       "\n",
       "This class represents the abstract concept of an optimizer, and contains methods that \n",
       "all concrete optimizer classes must implement. It is designed to handle the parameters \n",
       "of a machine learning model, providing functionality to perform a step of optimization \n",
       "and to zero out gradients.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| params | Iterable | The parameters of the model to be optimized. |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/optim.py#L14){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Optimizer\n",
       "\n",
       ">      Optimizer (params)\n",
       "\n",
       "Base class for all optimizers. Not meant to be instantiated directly.\n",
       "\n",
       "This class represents the abstract concept of an optimizer, and contains methods that \n",
       "all concrete optimizer classes must implement. It is designed to handle the parameters \n",
       "of a machine learning model, providing functionality to perform a step of optimization \n",
       "and to zero out gradients.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| params | Iterable | The parameters of the model to be optimized. |"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(Optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a408375e-08e1-4eba-ac22-20d04edefc8f",
   "metadata": {},
   "source": [
    "## SGD Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685e3697-2668-4806-992f-8fc1fbb6dd5a",
   "metadata": {},
   "source": [
    "This is a PyTorch-style implementation of the classic optimizer Stochastic Gradient Descent (SGD).\n",
    "\n",
    "SGD update is,\n",
    "\n",
    "$$\n",
    "\\theta_{t} = \\theta_{t-1} - \\alpha \\cdot g_{t}\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is the learning rate, and $g_{t}$ is the gradient at time step $t$. $Î¸_{t}$ represents the model parameters at time step $t$.\n",
    "\n",
    "The learning rate $\\alpha$ is a scalar hyperparameter that controls the size of the update at each iteration.\n",
    "\n",
    "An optional momentum term can be added to the update rule:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "v_{t} & \\leftarrow \\mu v_{t-1} + (1-\\mu) \\cdot g_t \\\\\n",
    "\\theta_{t} & \\leftarrow \\theta_{t-1} - \\alpha \\cdot v_t \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $v_{t}$ is the momentum term at time step $t$, and $\\mu$ is the momentum factor. The momentum term increases for dimensions whose gradients point in the same   \n",
    "direction and reduces updates for dimensions whose gradients change direction, thereby adding a form of preconditioning.  \n",
    "\n",
    "A weight decay term can also be included, which adds a regularization effect:\n",
    "\n",
    "$$\n",
    "\\theta_{t} = (1 - \\alpha \\cdot \\lambda) \\cdot \\theta_{t-1} - \\alpha \\cdot g_t\n",
    "$$\n",
    "\n",
    "where $\\lambda$ is the weight decay factor. This results in the model weights shrinking at each time step, which can prevent overfitting by keeping the model complexity in check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/optim.py#L63){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGD\n",
       "\n",
       ">      SGD (params, lr=0.01, momentum=0.0, wd=0.0)\n",
       "\n",
       "Implements stochastic gradient descent (optionally with momentum).\n",
       "\n",
       "This is a basic optimizer that's suitable for many machine learning models, and is often\n",
       "used as a baseline for comparing other optimizers' performance.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| params | Iterable |  | The parameters of the model to be optimized. |\n",
       "| lr | float | 0.01 | The learning rate. |\n",
       "| momentum | float | 0.0 | The momentum factor. |\n",
       "| wd | float | 0.0 | The weight decay (L2 regularization). |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/optim.py#L63){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SGD\n",
       "\n",
       ">      SGD (params, lr=0.01, momentum=0.0, wd=0.0)\n",
       "\n",
       "Implements stochastic gradient descent (optionally with momentum).\n",
       "\n",
       "This is a basic optimizer that's suitable for many machine learning models, and is often\n",
       "used as a baseline for comparing other optimizers' performance.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| params | Iterable |  | The parameters of the model to be optimized. |\n",
       "| lr | float | 0.01 | The learning rate. |\n",
       "| momentum | float | 0.0 | The momentum factor. |\n",
       "| wd | float | 0.0 | The weight decay (L2 regularization). |"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(SGD)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7aa00356-7cb1-4aa3-9bf6-70e5967cd2a3",
   "metadata": {},
   "source": [
    "## Adam Optimizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54a810ab-455b-41a8-bba0-edb974d39f4d",
   "metadata": {},
   "source": [
    "This is a PyTorch-like implementation of popular optimizer *Adam* from paper\n",
    " [Adam: A Method for Stochastic Optimization](https://papers.labml.ai/paper/1412.6980).\n",
    "\n",
    "*Adam* update is,\n",
    "$$\n",
    "\\begin{align}\n",
    "m_t &\\leftarrow \\beta_1 m_{t-1} + (1 - \\beta_1) \\cdot g_t \\\\\n",
    "v_t &\\leftarrow \\beta_2 v_{t-1} + (1 - \\beta_2) \\cdot g_t^2 \\\\\n",
    "\\hat{m}_t &\\leftarrow \\frac{m_t}{1-\\beta_1^t} \\\\\n",
    "\\hat{v}_t &\\leftarrow \\frac{v_t}{1-\\beta_2^t} \\\\\n",
    "\\theta_t &\\leftarrow \\theta_{t-1} - \\alpha \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\alpha$, $\\beta_1$, $\\beta_2$ and $\\epsilon$ are scalar hyper parameters.\n",
    "$m_t$ and $v_t$ are first and second order moments.\n",
    "$\\hat{m}_t$  and $\\hat{v}_t$ are biased corrected moments.\n",
    "$\\epsilon$ is used as a fix for division by zero error, but also acts as a form of a hyper-parameter\n",
    "that acts against variance in gradients.\n",
    "\n",
    "Effective step taken assuming $\\epsilon = 0$ is,\n",
    "$$\\Delta t = \\alpha \\cdot \\frac{\\hat{m}_t}{\\hat{v}_t}$$\n",
    "This is bounded by,\n",
    "$$\\vert \\Delta t \\vert \\le \\alpha \\cdot \\frac{1 - \\beta_1}{\\sqrt{1-\\beta_2}}$$\n",
    "when $1-\\beta_1 \\gt \\sqrt{1-\\beta_2}$\n",
    "and\n",
    "$$\\vert \\Delta t\\vert  \\le \\alpha$$\n",
    "otherwise.\n",
    "And in most common scenarios,\n",
    "$$\\vert \\Delta t \\vert \\approx \\alpha$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/m0saan/mambaforge/lib/python3.10/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Attributes\n",
      "  else: warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/optim.py#L126){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Adam\n",
       "\n",
       ">      Adam (params, lr=0.01, beta1=0.9, beta2=0.999, eps=1e-08,\n",
       ">            weight_decay=0.0)\n",
       "\n",
       "Implements the Adam optimization algorithm.\n",
       "\n",
       "Adam is an adaptive learning rate optimization algorithm that has been designed specifically for training \n",
       "deep neural networks. It leverages the power of adaptive learning rates methods to find individual learning \n",
       "rates for each parameter.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| params | Iterable |  | `params` is the list of parameters |\n",
       "| lr | float | 0.01 | `lr` is the learning rate $\\alpha$ |\n",
       "| beta1 | float | 0.9 | The exponential decay rate for the first moment estimates. Default is 0.9. |\n",
       "| beta2 | float | 0.999 | The exponential decay rate for the second moment estimates. Default is 0.999. |\n",
       "| eps | float | 1e-08 | `eps` is $\\hat{\\epsilon}$ or $\\epsilon$ based on `optimized_update` |\n",
       "| weight_decay | float | 0.0 | is an instance of class `WeightDecay` defined in [`__init__.py`](index.html) |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/m0saan/minima/blob/main/minima/optim.py#L126){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Adam\n",
       "\n",
       ">      Adam (params, lr=0.01, beta1=0.9, beta2=0.999, eps=1e-08,\n",
       ">            weight_decay=0.0)\n",
       "\n",
       "Implements the Adam optimization algorithm.\n",
       "\n",
       "Adam is an adaptive learning rate optimization algorithm that has been designed specifically for training \n",
       "deep neural networks. It leverages the power of adaptive learning rates methods to find individual learning \n",
       "rates for each parameter.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| params | Iterable |  | `params` is the list of parameters |\n",
       "| lr | float | 0.01 | `lr` is the learning rate $\\alpha$ |\n",
       "| beta1 | float | 0.9 | The exponential decay rate for the first moment estimates. Default is 0.9. |\n",
       "| beta2 | float | 0.999 | The exponential decay rate for the second moment estimates. Default is 0.999. |\n",
       "| eps | float | 1e-08 | `eps` is $\\hat{\\epsilon}$ or $\\epsilon$ based on `optimized_update` |\n",
       "| weight_decay | float | 0.0 | is an instance of class `WeightDecay` defined in [`__init__.py`](index.html) |"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae20858-2009-470d-a863-ac0bd45ed6ab",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
