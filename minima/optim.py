# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/04_optim.ipynb.

# %% auto 0
__all__ = ['Optimizer', 'SGD', 'AdaGrad', 'RMSProp', 'Adam']

# %% ../nbs/04_optim.ipynb 2
import minima as mi
from .nn import Parameter
from .autograd import Tensor
from . import init
import numpy as np

# %% ../nbs/04_optim.ipynb 3
class Optimizer:
    """
    Base class for all optimizers. Not meant to be instantiated directly.

    This class represents the abstract concept of an optimizer, and contains methods that 
    all concrete optimizer classes must implement. It is designed to handle the parameters 
    of a machine learning model, providing functionality to perform a step of optimization 
    and to zero out gradients.
    
    Parameters
    ----------
    params : Iterable
        The parameters of the model to be optimized.

    Raises
    ------
    NotImplementedError
        If the `step` method is not implemented in a subclass.
    """
    def __init__(
        self,
        params # The parameters of the model to be optimized.
    ):
        self.params = params

    def step(self):
        """
        Performs a single optimization step.

        This method must be overridden by any subclass to provide the specific optimization logic.
        
        Raises
        ------
        NotImplementedError
            If the method is not implemented in a subclass.
        """
        raise NotImplementedError()

    def zero_grad(self):
        """
        Zeros out all gradients in `params`.

        This method is typically used before backpropagation to ensure that gradients 
        are not being accumulated from multiple passes.
        """
        for p in self.params:
            p.grad = None

# %% ../nbs/04_optim.ipynb 6
class SGD(Optimizer):
    """
    Implements stochastic gradient descent (optionally with momentum).

    This is a basic optimizer that's suitable for many machine learning models, and is often
    used as a baseline for comparing other optimizers' performance.

    Parameters
    ----------
    params : Iterable
        The parameters of the model to be optimized.
    lr : float, optional
        The learning rate.
    momentum : float, optional
        The momentum factor.
    wd : float, optional
        The weight decay (L2 regularization).
    """
    def __init__(
        self,
        params, # The parameters of the model to be optimized.
        lr=0.01, # The learning rate.
        momentum=0.0, # The momentum factor.
        wd=0.0 # The weight decay (L2 regularization).
    ):
        super().__init__(params)

        self.lr = lr
        self.momentum = momentum
        self.u = {}
        self.wd = wd

    def step(self):
        """
        Performs a single optimization step.

        This method uses the current gradients to adjust the parameters using stochastic gradient descent.
        """
        for self.idx, p in enumerate(self.params):
            self._reg_step(p)
            self._opt_step(p)

    def _opt_step(self, p):
        """
        Performs the optimization step for a single parameter tensor.

        If momentum is set, it applies momentum by using a running average of the previous gradients.
        """
        if self.idx not in self.u:
            self.u[self.idx] = init.zeros(*p.shape)
        self.u[self.idx] = self.momentum * self.u[self.idx] + (1 - self.momentum) * p.grad.data
        p.data = p.data - self.lr * self.u[self.idx]

    def _reg_step(self, p):
        """
        Applies weight decay for a single parameter tensor.

        This form of L2 regularization can help prevent overfitting.
        """
        if self.wd != 0:
            p.data *= (1 - self.lr * self.wd)

# %% ../nbs/04_optim.ipynb 10
class AdaGrad(Optimizer):
    """
    Implements AdaGrad optimization algorithm.

    AdaGrad is an optimizer with parameter-wise learning rates, which adapts the learning rate
    based on how frequently a parameter gets updated during training. It's particularly useful
    for sparse data.

    Parameters
    ----------
    params : Iterable
        The parameters of the model to be optimized.
    lr : float, optional
        The initial learning rate.
    wd : float, optional
        The weight decay (L2 regularization).
    eps : float, optional
        A small constant for numerical stability.
    """
    def __init__(
        self,
        params,  # The parameters of the model to be optimized.
        lr=0.001,  # The initial learning rate.
        wd=0.0,  # The weight decay (L2 regularization).
        eps=1e-7,  # A small constant for numerical stability.
    ):
        super().__init__(params)

        self.lr = lr
        self.cache = {}
        self.wd = wd
        self.eps = eps

    def step(self):
        """
        Performs a single optimization step.

        This method uses the current gradients to adjust the parameters using AdaGrad algorithm.
        """
        for self.idx, p in enumerate(self.params):
            self._reg_step(p)
            self._opt_step(p)

    def _opt_step(self, p):
        """
        Performs the optimization step for a single parameter tensor.

        It computes parameter-wise learning rates and updates the parameters accordingly.
        """
        if self.idx not in self.cache:
            self.cache[self.idx] = init.zeros(*p.shape)
        self.cache[self.idx] += p.grad.data ** 2
        p.data = p.data - (self.lr / (self.cache[self.idx] + self.eps) ** 0.5 ) * p.grad.data

    def _reg_step(self, p):
        """
        Applies weight decay for a single parameter tensor.

        This form of L2 regularization can help prevent overfitting.
        """
        if self.wd != 0:
            p.data *= (1 - self.lr * self.wd)

# %% ../nbs/04_optim.ipynb 13
class RMSProp(Optimizer):
    """
    Implements RMSProp optimization algorithm.

    RMSProp is an optimizer with parameter-wise adaptive learning rates, which adapt the learning rate
    for each parameter individually, making it suitable for dealing with sparse or multi-scale data.

    Parameters
    ----------
    params : Iterable
        The parameters of the model to be optimized.
    lr : float, optional
        The initial learning rate.
    wd : float, optional
        The weight decay (L2 regularization).
    eps : float, optional
        A small constant for numerical stability.
    rho : float, optional
        The decay rate for the moving average of squared gradients.
    """
    def __init__(
        self,
        params,  # The parameters of the model to be optimized.
        lr=0.001,  # The initial learning rate.
        wd=0.0,  # The weight decay (L2 regularization).
        eps=1e-7,  # A small constant for numerical stability.
        rho=0.9, # The decay rate for the moving average of squared gradients.
    ):
        super().__init__(params)

        self.lr = lr
        self.cache = {}
        self.wd = wd
        self.eps = eps
        self.rho = rho

    def step(self):
        """
        Performs a single optimization step.

        This method uses the current gradients to adjust the parameters using RMSProp algorithm.
        """
        for self.idx, p in enumerate(self.params):
            self._reg_step(p)
            self._opt_step(p)

    def _opt_step(self, p):
        """
        Performs the optimization step for a single parameter tensor.

        It computes parameter-wise learning rates and updates the parameters accordingly.
        """
        if self.idx not in self.cache:
            self.cache[self.idx] = init.zeros(*p.shape)
        self.cache[self.idx] = self.rho * self.cache[self.idx] + (1 - self.rho) * p.grad.data ** 2
        p.data = p.data - (self.lr / (self.cache[self.idx] + self.eps) ** 0.5 ) * p.grad.data

    def _reg_step(self, p):
        """
        Applies weight decay for a single parameter tensor.

        This form of L2 regularization can help prevent overfitting.
        """
        if self.wd != 0:
            p.data *= (1 - self.lr * self.wd)

# %% ../nbs/04_optim.ipynb 16
class Adam(Optimizer):
    """
    Implements the Adam optimization algorithm.

    Adam is an adaptive learning rate optimization algorithm that has been designed specifically for training 
    deep neural networks. It leverages the power of adaptive learning rates methods to find individual learning 
    rates for each parameter.

    Parameters
    ----------
    params : Iterable
        The parameters of the model to be optimized.
    lr : float, optional
        The learning rate. Default is 0.01.
    beta1 : float, optional
        The exponential decay rate for the first moment estimates. Default is 0.9.
    beta2 : float, optional
        The exponential decay rate for the second moment estimates. Default is 0.999.
    eps : float, optional
        A small constant for numerical stability. Default is 1e-8.
    weight_decay : float, optional
        Weight decay (L2 penalty). Default is 0.

    Attributes
    ----------
    t : int
        The time step for the Adam optimizer.
    exp_avg : dict
        The dictionary to store the exponential moving average of gradient values.
    exp_avg_sq : dict
        The dictionary to store the exponential moving average of squared gradient values.
    """
    def __init__(
        self,
        params, # `params` is the list of parameters
        lr=1e-5, # `lr` is the learning rate $\alpha$
        beta1=0.9, # The exponential decay rate for the first moment estimates. Default is 0.9.
        beta2=0.999, # The exponential decay rate for the second moment estimates. Default is 0.999.
        eps=1e-8, # `eps` is $\hat{\epsilon}$ or $\epsilon$ based on `optimized_update`
        weight_decay=0.0, # is an instance of class `WeightDecay` defined in [`__init__.py`](index.html)
    ):
        super().__init__(params)
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
        self.eps = eps
        self.wd = weight_decay
        self.t = 0

        self.exp_avg = {}
        self.exp_avg_sq = {}

    def step(self):
        """
        Performs a single optimization step.

        This method updates the parameters based on the current gradient.
        """
        for self.idx, p in enumerate(self.params):
            self._reg_step(p)
            self._opt_step(p)

    def _opt_step(self, p):
        """
        Performs the optimization step for a single parameter tensor.

        The method updates the moving averages of the gradient (m) and the squared gradient (v), and then 
        computes the bias-corrected estimates of these two variables. These bias-corrected estimates are 
        then used to update the parameter.
        """
        if self.idx not in self.exp_avg:
            self.exp_avg[self.idx] = init.zeros(*p.shape)
            self.exp_avg_sq[self.idx] = init.zeros(*p.shape)
        
        # Update biased first and second moment estimates
        self.exp_avg[self.idx] = self.beta1 * self.exp_avg[self.idx] + (1 - self.beta1) * p.grad.data
        self.exp_avg_sq[self.idx] = self.beta2 * self.exp_avg_sq[self.idx] + (1 - self.beta2) * p.grad.data**2
        
        # Compute bias-corrected first and second moment estimates
        exp_avg_hat = self.exp_avg[self.idx] / (1 - self.beta1 ** (self.idx + 1))
        exp_avg_sq_hat = self.exp_avg_sq[self.idx] / (1 - self.beta2 ** (self.idx + 1))
        p.data = p.data - self.lr * exp_avg_hat / (exp_avg_sq_hat ** 0.5 + self.eps)

    def _reg_step(self, p):
        """
        Applies weight decay for a single parameter tensor.

        This form of L2 regularization can help prevent overfitting. It adjusts the parameter by 
        a small factor of its current value.
        """
        if self.wd != 0:
            p.data *= (1 - self.lr * self.wd)
        # all same :3
        # p.data *= (1 - self.lr * self.weight_decay)
        # p.data = p.data - self.lr * self.weight_decay * p.data
        # p.data -= self.lr * self.weight_decay * p.data
