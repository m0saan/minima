{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6274abbb-d877-4da6-87a9-196ec41bcce5",
   "metadata": {},
   "source": [
    "# operators\n",
    "\n",
    "> The `operators` module in this framework provides a collection of tensor operations for building computational graphs in deep learning. Each class in this module represents a different type of operation that can be performed on tensors, such as element-wise addition, scalar multiplication, division, exponentiation, etc. \n",
    "\n",
    ">These operations are the building blocks that allow complex mathematical functions to be expressed as a graph of simpler operations, facilitating automatic differentiation and gradient descent optimization. \n",
    "\n",
    "> Each operation also implements a method for computing its gradient, enabling the backpropagation of gradients through the computational graph for training models. The consistent interface provided by these classes allows for flexibility and modularity in defining and manipulating computational graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cf77c9-d5dc-473d-9624-6679191a3c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5485133d-9a30-4362-af02-d6724482f459",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\"\"\"Operator implementations.\"\"\"\n",
    "\n",
    "from numbers import Number\n",
    "from typing import Optional, List\n",
    "from minima.autograd import NDArray\n",
    "from minima.autograd import Operator, Tensor, Value, TensorOp, Tuple, Union\n",
    "from collections import namedtuple\n",
    "from typing import NamedTuple\n",
    "import numpy\n",
    "import torch\n",
    "\n",
    "# NOTE: we will import numpy as the array_api\n",
    "# as the backend for our computations, this line will change in later homeworks\n",
    "import numpy as ARRAY_API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c758cd9-3451-4fb7-a7e1-68fa9c4933d6",
   "metadata": {},
   "source": [
    "## Note about the `out_grad` parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9a4956-5575-4950-9365-560225c3a715",
   "metadata": {},
   "source": [
    "During backpropagation in a neural network, we compute gradients starting from the output layer and propagate them back towards the input layer. The key idea here is that each layer receives the gradient of the loss with respect to its output (let's call this `out_grad`), and it needs to compute and pass back the gradient of the loss with respect to its input (let's call this `in_grad`). This is needed so that the parameters of each layer can be updated correctly during gradient descent.\n",
    "\n",
    "The `out_grad` parameter refers to the gradient of the loss function with respect to the output of the node. Multiplying this with the local gradient gives the gradient of the loss with respect to the input to the node, according to the chain rule of calculus, which is the basis for backpropagation in neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c0867e-7744-4c76-95b6-da3868dc8625",
   "metadata": {},
   "source": [
    "The chain rule is a fundamental concept in calculus that provides a method to compute the derivative of composite functions. In simple terms, the chain rule states that the derivative of a composite function is the derivative of the outer function multiplied by the derivative of the inner function.\n",
    "\n",
    "Given a composite function that is the composition of two functions, say, $f(g(x))$, the chain rule can be stated as follows:\n",
    "\n",
    "$$\\frac{df}{dx} = \\frac{df}{dg} \\cdot \\frac{dg}{dx}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\frac{df}{dx}$ is the derivative of the composite function $f(g(x))$ with respect to $x$,\n",
    "- $\\frac{df}{dg}$ is the derivative of the outer function $f$ with respect to its argument $g(x)$, and\n",
    "- $\\frac{dg}{dx}$ is the derivative of the inner function $g(x)$ with respect to $x$.\n",
    "\n",
    "The chain rule can be extended to the case where we have more than two composite functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cb2320-c471-426d-ad48-0197b1daecaa",
   "metadata": {},
   "source": [
    "## Element Wise Addition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feadcd15-44d4-4a3d-aef9-39b3b7f6fcd1",
   "metadata": {},
   "source": [
    "Let's walk through the step-by-step derivative calculation for the `EWiseAdd` operation:\n",
    "\n",
    "We have the function `f(a, b) = a + b`, where `a` and `b` are tensors. Our goal is to compute the partial derivatives with respect to `a` and `b`.\n",
    "\n",
    "Let's start by calculating the derivative of `f` with respect to `a`, denoted as `df/da`:\n",
    "\n",
    "Step 1: Compute the derivative of `f` with respect to `a`.\n",
    "\n",
    "$\\frac{{\\partial f}}{{\\partial a}} = \\frac{{\\partial}}{{\\partial a}} (a + b)$\n",
    "\n",
    "Since `a` is the variable we are differentiating with respect to, the derivative of `a` with respect to itself is 1:\n",
    "\n",
    "$$\\frac{{\\partial f}}{{\\partial a}} = 1$$\n",
    "\n",
    "Therefore, $$\\frac{{\\partial f}}{{\\partial a}} = 1.$$\n",
    "\n",
    "Step 2: Compute the derivative of `f` with respect to `b`.\n",
    "\n",
    "$$\\frac{{\\partial f}}{{\\partial b}} = \\frac{{\\partial}}{{\\partial b}} (a + b)$$\n",
    "\n",
    "Again, since `b` is the variable we are differentiating with respect to, the derivative of `b` with respect to itself is 1:\n",
    "\n",
    "$$\\frac{{\\partial f}}{{\\partial b}} = 1$$\n",
    "\n",
    "Therefore, $$\\frac{{\\partial f}}{{\\partial b}} = 1$$\n",
    "\n",
    "Hence, the partial derivatives of `f(a, b) = a + b` with respect to `a` and `b` are both equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4e9796-2da9-4668-ae19-cf547e25ddd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class EWiseAdd(TensorOp):\n",
    "    \"\"\"\n",
    "    Performs element-wise addition of two tensors.\n",
    "\n",
    "    Example:\n",
    "    >>> a = Tensor([1, 2, 3])\n",
    "    >>> b = Tensor([4, 5, 6])\n",
    "    >>> op = EWiseAdd()\n",
    "    >>> result = op.compute(a, b)\n",
    "    >>> print(result)\n",
    "    Tensor([5, 7, 9])\n",
    "    \"\"\"\n",
    "    \n",
    "    def compute(self, a: NDArray, b: NDArray) -> NDArray:\n",
    "        \"\"\"\n",
    "        Computes the element-wise sum of two tensors.\n",
    "\n",
    "        Args:\n",
    "        - a: The first tensor.\n",
    "        - b: The second tensor.\n",
    "\n",
    "        Returns:\n",
    "        The element-wise sum of a and b.\n",
    "        \"\"\"\n",
    "        return a + b\n",
    "\n",
    "    def gradient(self, out_grad: Tensor, node: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Computes the gradient of the element-wise addition operation.\n",
    "\n",
    "        Args:\n",
    "        - out_grad: The gradient of the output of the operation.\n",
    "        - node: The node in the computational graph where the operation was performed.\n",
    "\n",
    "        Returns:\n",
    "        The gradients with respect to the inputs.\n",
    "        \"\"\"\n",
    "        return (out_grad, out_grad)\n",
    "\n",
    "def add(a: Tensor, b: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Adds two tensors element-wise.\n",
    "\n",
    "    Args:\n",
    "    - a: The first tensor.\n",
    "    - b: The second tensor.\n",
    "\n",
    "    Returns:\n",
    "    The element-wise sum of a and b.\n",
    "    \"\"\"\n",
    "    return EWiseAdd()(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd56c389-9132-4744-bcb6-cd85dc084a2d",
   "metadata": {},
   "source": [
    "Create two 1-D tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358fcf64-9c44-4d44-8374-a0ef11668d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Tensor([1, 2, 3])\n",
    "b = Tensor([4, 5, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cddb288-2e63-4d19-ad07-f708fa7c2dc9",
   "metadata": {},
   "source": [
    "Create an EWiseAdd operation instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81b16cf-a61b-4771-8799-259daefa971f",
   "metadata": {},
   "outputs": [],
   "source": [
    "op = EWiseAdd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b5a98f-cb3f-460b-9618-5457c26af297",
   "metadata": {},
   "source": [
    "Compute the element-wise sum of a and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349ecf89-aa8f-47e6-8088-24175baa0819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "minima.Tensor([5 7 9])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = op.compute(a, b)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df6a9f8-8f04-4098-87dc-b497137f73e9",
   "metadata": {},
   "source": [
    "Alternatively, you can use the add function directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6db6072-65fb-4cad-af75-e87236dbc921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "minima.Tensor([5 7 9])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = add(a, b)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c4c99b-535c-4049-aa09-ae2b2b26c03d",
   "metadata": {},
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba584690-7fe5-4d93-95d7-2cd7e373811b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "minima.Tensor([5 7 9])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dc5f72-9776-45d4-8d47-3e0313c75885",
   "metadata": {},
   "source": [
    "For 2-D tensors, we can compute the element-wise sum of a and b in the same way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbbc80e-a2f0-46d2-97df-35344197da88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "minima.Tensor([[ 8 10 12]\n",
       " [14 16 18]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Tensor([[1, 2, 3], [4, 5, 6]])\n",
    "b = Tensor([[7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "result = op.compute(a, b)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd371e81-b6e4-43da-9987-30057c5c038a",
   "metadata": {},
   "source": [
    "## Scalar Addition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dbc8dc-25ae-4793-9cc8-cdbef59a8400",
   "metadata": {},
   "source": [
    "Explanation for the derivative of the `AddScalar` operator:\n",
    "\n",
    "Let's denote the scalar as `c` and `a` as the tensor being added by the scalar. The operation can be described as `f(a) = a + c`.\n",
    "\n",
    "The function for the backward pass (i.e., the gradient) is `df/da = 1`, which means the derivative of `f(a)` with respect to `a` is simply `1`.\n",
    "\n",
    "We are given a function $f(a) = a + c$, where $a$ is a tensor and $c$ is a scalar. Our task is to find the derivative of this function with respect to $a$.\n",
    "\n",
    "By differentiating the function $f(a)$ with respect to $a$, we find:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{da} &= \\frac{d}{da} (a + c) \\\\\n",
    "&= 1\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, the gradient of $f(a)$ with respect to $a$ is $1$.\n",
    "\n",
    "\n",
    "We starts by defining the function `f(a) = a + c`. It then explains that when we differentiate `f(a)` with respect to `a`, we find that the derivative is `1`. This means that the gradient of `f(a)` with respect to `a` is `1`, which matches the behavior of the `AddScalar` operator as provided in the `gradient` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebbb12f-03ea-4d3e-beb4-e15c0eb7fc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AddScalar(TensorOp):\n",
    "    \"\"\"\n",
    "    Performs addition of a tensor and a scalar.\n",
    "\n",
    "    Example:\n",
    "    >>> a = Tensor([1, 2, 3])\n",
    "    >>> op = AddScalar(5)\n",
    "    >>> result = op.compute(a)\n",
    "    >>> print(result)\n",
    "    Tensor([6, 7, 8])\n",
    "    \"\"\"\n",
    "    def __init__(self, scalar: Union[int, float]):\n",
    "        \"\"\"\n",
    "        Initializes the operation with a scalar.\n",
    "\n",
    "        Args:\n",
    "        - scalar: The scalar to add to the tensor.\n",
    "        \"\"\"\n",
    "        self.scalar = scalar\n",
    "\n",
    "    def compute(self, a: NDArray) -> NDArray:\n",
    "        \"\"\"\n",
    "        Computes the sum of a tensor and a scalar.\n",
    "\n",
    "        Args:\n",
    "        - a: The tensor.\n",
    "\n",
    "        Returns:\n",
    "        The sum of a and the scalar.\n",
    "        \"\"\"\n",
    "        return a + self.scalar\n",
    "\n",
    "    def gradient(self, out_grad: Tensor, node: Tensor) -> Tuple[Tensor]:\n",
    "        \"\"\"\n",
    "        Computes the gradient of the addition operation.\n",
    "\n",
    "        Args:\n",
    "        - out_grad: The gradient of the output of the operation.\n",
    "        - node: The node in the computational graph where the operation was performed.\n",
    "\n",
    "        Returns:\n",
    "        The gradient with respect to the input.\n",
    "        \"\"\"\n",
    "        return (out_grad, )\n",
    "\n",
    "def add_scalar(a: Tensor, scalar: Union[int, float]) -> Tensor:\n",
    "    \"\"\"\n",
    "    Adds a scalar to a tensor.\n",
    "\n",
    "    Args:\n",
    "    - a: The tensor.\n",
    "    - scalar: The scalar to add.\n",
    "\n",
    "    Returns:\n",
    "    The sum of a and the scalar.\n",
    "    \"\"\"\n",
    "    return AddScalar(scalar)(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26ed99f-b3f2-4df1-9918-ff23fc99be74",
   "metadata": {},
   "source": [
    "## Element Wise Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf5a7d0-8e8a-47cd-a4b5-cdc12a03649c",
   "metadata": {},
   "source": [
    "Explanation for the derivative of the `EWiseMul` (element-wise multiplication) operator:\n",
    "\n",
    "Let's denote the two input tensors as `a` and `b`. The operation can be described as `f(a, b) = a * b`, where `*` represents element-wise multiplication.\n",
    "\n",
    "The function for the backward pass (i.e., the gradient) is `df/da = b` and `df/db = a`. This means that the derivative of `f(a, b)` with respect to `a` is `b`, and the derivative with respect to `b` is `a`.\n",
    "\n",
    "\n",
    "We are given a function $f(a, b) = a \\odot b$, where $a$ and $b$ are tensors, and $\\odot$ represents element-wise multiplication. Our task is to find the derivatives of this function with respect to $a$ and $b$.\n",
    "\n",
    "By differentiating the function $f(a, b)$ with respect to $a$, we find:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{da} &= \\frac{d}{da} (a \\odot b) \\\\\n",
    "&= b\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, the gradient of $f(a, b)$ with respect to $a$ is $b$.\n",
    "\n",
    "Similarly, by differentiating the function $f(a, b)$ with respect to $b$, we find:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{db} &= \\frac{d}{db} (a \\odot b) \\\\\n",
    "&= a\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, the gradient of $f(a, b)$ with respect to $b$ is $a$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e4faab-b6f8-4332-ba80-06564920e53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class EWiseMul(TensorOp):\n",
    "    \"\"\"\n",
    "    Performs element-wise multiplication of two tensors.\n",
    "\n",
    "    Example:\n",
    "    >>> a = Tensor([1, 2, 3])\n",
    "    >>> b = Tensor([4, 5, 6])\n",
    "    >>> op = EWiseMul()\n",
    "    >>> result = op.compute(a, b)\n",
    "    >>> print(result)\n",
    "    Tensor([4, 10, 18])\n",
    "    \"\"\"\n",
    "    def compute(self, a: NDArray, b: NDArray) -> NDArray:\n",
    "        \"\"\"\n",
    "        Computes the element-wise product of two tensors.\n",
    "\n",
    "        Args:\n",
    "        - a: The first tensor.\n",
    "        - b: The second tensor.\n",
    "\n",
    "        Returns:\n",
    "        The element-wise product of a and b.\n",
    "        \"\"\"\n",
    "        return a * b\n",
    "\n",
    "    def gradient(self, out_grad: Tensor, node: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Computes the gradient of the element-wise multiplication operation.\n",
    "\n",
    "        Args:\n",
    "        - out_grad: The gradient of the output of the operation.\n",
    "        - node: The node in the computational graph where the operation was performed.\n",
    "\n",
    "        Returns:\n",
    "        The gradients with respect to the inputs.\n",
    "        \"\"\"\n",
    "        a, b = node.children\n",
    "        return out_grad * b, out_grad * a\n",
    "\n",
    "def multiply(a: Tensor, b: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Multiplies two tensors element-wise.\n",
    "\n",
    "    Args:\n",
    "    - a: The first tensor.\n",
    "    - b: The second tensor.\n",
    "\n",
    "    Returns:\n",
    "    The element-wise product of a and b.\n",
    "    \"\"\"\n",
    "    return EWiseMul()(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb531c5-f22c-40c9-901e-e373b837a846",
   "metadata": {},
   "source": [
    "## Scalar Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0246b9-ce8f-4fab-991d-7ec43745c2ea",
   "metadata": {},
   "source": [
    "Let's denote the scalar as `c` and `a` as the tensor being multiplied by the scalar. The operation can be described as `f(a) = a * c`.\n",
    "\n",
    "The function for the backward pass (i.e., the gradient) is `df/da = c`, which means the derivative of `f(a)` with respect to `a` is `c`.\n",
    "\n",
    "The LaTeX document will look as follows:\n",
    "\n",
    "We are given a function $f(a) = a \\cdot c$, where $a$ is a tensor and $c$ is a scalar. Our task is to find the derivative of this function with respect to $a$.\n",
    "\n",
    "By differentiating the function $f(a)$ with respect to $a$, we find:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{da} &= \\frac{d}{da} (a \\cdot c) \\\\\n",
    "&= c\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, the gradient of $f(a)$ with respect to $a$ is $c$.\n",
    "\n",
    "We starts by defining the function `f(a) = a * c`. It then explains that when we differentiate `f(a)` with respect to `a`, we find that the derivative is `c`. This means that the gradient of `f(a)` with respect to `a` is `c`, which matches the behavior of the `MulScalar` operator as provided in the `gradient` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a28132-02d4-4b27-bc07-5897f88a9cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MulScalar(TensorOp):\n",
    "    \"\"\"\n",
    "    Performs multiplication of a tensor and a scalar.\n",
    "\n",
    "    Example:\n",
    "    >>> a = Tensor([1, 2, 3])\n",
    "    >>> op = MulScalar(5)\n",
    "    >>> result = op.compute(a)\n",
    "    >>> print(result)\n",
    "    Tensor([5, 10, 15])\n",
    "    \"\"\"\n",
    "    def __init__(self, scalar: Union[int, float]):\n",
    "        \"\"\"\n",
    "        Initializes the operation with a scalar.\n",
    "\n",
    "        Args:\n",
    "        - scalar: The scalar to multiply the tensor with.\n",
    "        \"\"\"\n",
    "        self.scalar = scalar\n",
    "\n",
    "    def compute(self, a: NDArray) -> NDArray:\n",
    "        \"\"\"\n",
    "        Computes the product of a tensor and a scalar.\n",
    "\n",
    "        Args:\n",
    "        - a: The tensor.\n",
    "\n",
    "        Returns:\n",
    "        The product of a and the scalar.\n",
    "        \"\"\"\n",
    "        return a * self.scalar\n",
    "\n",
    "    def gradient(self, out_grad: Tensor, node: Tensor) -> Tuple[Tensor]:\n",
    "        \"\"\"\n",
    "        Computes the gradient of the multiplication operation.\n",
    "\n",
    "        Args:\n",
    "        - out_grad: The gradient of the output of the operation.\n",
    "        - node: The node in the computational graph where the operation was performed.\n",
    "\n",
    "        Returns:\n",
    "        The gradient with respect to the input.\n",
    "        \"\"\"\n",
    "        return (out_grad * self.scalar, )\n",
    "    \n",
    "def mul_scalar(a: Tensor, scalar: Union[int, float]) -> Tensor:\n",
    "    \"\"\"\n",
    "    Multiplies a tensor by a scalar.\n",
    "\n",
    "    Args:\n",
    "    - a: The tensor.\n",
    "    - scalar: The scalar to multiply.\n",
    "\n",
    "    Returns:\n",
    "    The product of a and the scalar.\n",
    "    \"\"\"\n",
    "    return MulScalar(scalar)(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ce3d8e-1880-42c4-b741-b31506241caa",
   "metadata": {},
   "source": [
    "## Element Wise Divide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4126e368-f144-4d5f-9aab-4d6de564fc0f",
   "metadata": {},
   "source": [
    "The operation described here is an element-wise division of two tensors, `a` and `b`, where the operation can be described as `f(a, b) = a / b`. \n",
    "\n",
    "We'll compute the partial derivatives with respect to `a` and `b`:\n",
    "\n",
    "1. The partial derivative of `f(a, b)` with respect to `a` (`df/da`) is `1/b`.\n",
    "\n",
    "2. The partial derivative of `f(a, b)` with respect to `b` (`df/db`) is `-a / b^2`.\n",
    "\n",
    "We are given a function $f(a, b) = \\frac{a}{b}$, where $a$ and $b$ are tensors. Our task is to find the partial derivatives of this function with respect to $a$ and $b$.\n",
    "\n",
    "Let's start with $\\frac{\\partial f}{\\partial a}$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial f}{\\partial a} &= \\frac{\\partial}{\\partial a} \\left(\\frac{a}{b}\\right) \\\\\n",
    "&= \\frac{1}{b}\n",
    "\\end{align*}\n",
    "\n",
    "Now, let's compute $\\frac{\\partial f}{\\partial b}$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial f}{\\partial b} &= \\frac{\\partial}{\\partial b} \\left(\\frac{a}{b}\\right) \\\\\n",
    "&= - \\frac{a}{b^{2}}\n",
    "\\end{align*}\n",
    "\n",
    "Here is a detailed derivative:\n",
    "\n",
    "Given a function of the form $y = \\frac{u}{v}$, where both $u$ and $v$ are functions of $x$, the quotient rule of differentiation states:\n",
    "\n",
    "$$\\frac{dy}{dx} = \\frac{v \\cdot \\frac{du}{dx} - u \\cdot \\frac{dv}{dx}}{v^2}$$\n",
    "\n",
    "In our case, we're looking at the function $y = \\frac{a}{b}$, where $a$ and $b$ are tensors. We want to find the derivative with respect to $b$ (instead of $x$ in our general formula). So we have:\n",
    "\n",
    "$$\\frac{dy}{db} = \\frac{b \\cdot \\frac{da}{db} - a \\cdot \\frac{db}{db}}{b^2}$$\n",
    "\n",
    "Since $a$ does not depend on $b$, $\\frac{da}{db} = 0$, and since any variable is equal to itself, $\\frac{db}{db} = 1$. \n",
    "\n",
    "So the derivative $\\frac{dy}{db}$ simplifies to:\n",
    "\n",
    "$$\\frac{dy}{db} = \\frac{b \\cdot 0 - a \\cdot 1}{b^2}$$\n",
    "\n",
    "Therefore, the derivative of $y$ with respect to $b$ is $-\\frac{a}{b^2}$.\n",
    "\n",
    "Therefore, the gradient of $f(a, b)$ with respect to $a$ is $\\frac{1}{b}$, and the gradient of $f(a, b)$ with respect to $b$ is $- \\frac{a}{b^{2}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d841c0bd-e4ad-4dd8-add8-0a4626a774fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class EWiseDiv(TensorOp):\n",
    "    \"\"\"\n",
    "    The EWiseDiv operation divides two tensors element-wise.\n",
    "\n",
    "    Example:\n",
    "        >>> import numpy as np\n",
    "        >>> a = Tensor(np.array([1, 2, 3]))\n",
    "        >>> b = Tensor(np.array([4, 5, 6]))\n",
    "        >>> div = EWiseDiv()\n",
    "        >>> result = div.compute(a.data, b.data)\n",
    "        >>> print(result)\n",
    "        array([0.25, 0.4, 0.5])\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def compute(self, a: NDArray, b: NDArray) -> NDArray:\n",
    "        \"\"\"\n",
    "        Computes the element-wise division of two tensors.\n",
    "\n",
    "        Args:\n",
    "            a (NDArray): The dividend tensor.\n",
    "            b (NDArray): The divisor tensor.\n",
    "\n",
    "        Returns:\n",
    "            NDArray: The resulting tensor after element-wise division.\n",
    "        \"\"\"\n",
    "        return a / b\n",
    "\n",
    "    def gradient(self, out_grad: Tensor, node: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Computes the gradient of the element-wise division operation.\n",
    "\n",
    "        Args:\n",
    "            out_grad (Tensor): The gradient of the output tensor.\n",
    "            node (Tensor): The node in the computational graph where the operation was performed.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tensor, Tensor]: The gradients with respect to the dividend and divisor tensors.\n",
    "        \"\"\"\n",
    "        a, b = node.children\n",
    "        return divide(out_grad, b), out_grad * negate(divide(a, power_scalar(b, 2)))\n",
    "\n",
    "\n",
    "def divide(a: Tensor, b: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Divides two tensors element-wise.\n",
    "\n",
    "    Args:\n",
    "        a (Tensor): The dividend tensor.\n",
    "        b (Tensor): The divisor tensor.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: The resulting tensor after element-wise division.\n",
    "\n",
    "    Example:\n",
    "        >>> import numpy as np\n",
    "        >>> a = Tensor(np.array([1, 2, 3]))\n",
    "        >>> b = Tensor(np.array([4, 5, 6]))\n",
    "        >>> result = divide(a, b)\n",
    "        >>> print(result)\n",
    "        Tensor([0.25, 0.4, 0.5])\n",
    "    \"\"\"\n",
    "    return EWiseDiv()(a, b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9983c201-4fe0-4a73-8f8f-59cdb6850753",
   "metadata": {},
   "source": [
    "## Scalar Division"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4463c6e-8ae4-4a1a-bc03-6728784059bf",
   "metadata": {},
   "source": [
    "Let's denote the scalar as `c`, and `a` as the tensor being divided by the scalar. The operation can be described as `f(a) = a / c`.\n",
    "\n",
    "The function for the backward pass (i.e., the gradient) is `df/da = 1/c`.\n",
    "\n",
    "This is the derivative of `f(a)` with respect to `a`.\n",
    "\n",
    "We are given a function $f(a) = \\frac{a}{c}$, where $a$ is a tensor and $c$ is a scalar. Our task is to find the derivative of this function with respect to $a$.\n",
    "\n",
    "By using the power rule of differentiation, where the derivative of $a^n$ is $n \\cdot a^{n-1}$, we can rewrite $f(a)$ as $f(a) = c^{-1}a$. \n",
    "\n",
    "Now, we can differentiate this with respect to $a$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{da} &= \\frac{d}{da} (c^{-1}a) \\\\\n",
    "&= c^{-1} \\frac{d}{da} (a) \\\\\n",
    "&= c^{-1} \\\\\n",
    "&= \\frac{1}{c}\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, the gradient of $f(a)$ with respect to $a$ is $\\frac{1}{c}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbceaf89-d19a-43e2-9ce2-3a9c093612ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DivScalar(TensorOp):\n",
    "    \"\"\"\n",
    "    The DivScalar operation divides a tensor by a scalar.\n",
    "\n",
    "    Example:\n",
    "        >>> import numpy as np\n",
    "        >>> a = Tensor(np.array([1, 2, 3]))\n",
    "        >>> scalar = 2\n",
    "        >>> div_scalar = DivScalar(scalar)\n",
    "        >>> result = div_scalar.compute(a.data)\n",
    "        >>> print(result)\n",
    "        array([0.5, 1.0, 1.5])\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scalar: Union[int, float]):\n",
    "        \"\"\"\n",
    "        Initialize the DivScalar operation with the scalar to divide by.\n",
    "\n",
    "        Args:\n",
    "            scalar (int, float): The scalar to divide the tensor by.\n",
    "        \"\"\"\n",
    "        self.scalar = scalar\n",
    "\n",
    "    def compute(self, a: NDArray) -> NDArray:\n",
    "        \"\"\"\n",
    "        Divides the tensor by the scalar.\n",
    "\n",
    "        Args:\n",
    "            a (NDArray): The tensor to divide.\n",
    "\n",
    "        Returns:\n",
    "            NDArray: The resulting tensor after division.\n",
    "        \"\"\"\n",
    "        return a / self.scalar\n",
    "\n",
    "    def gradient(self, out_grad: Tensor, node: Tensor) -> Tuple[Tensor, ...]:\n",
    "        \"\"\"\n",
    "        Computes the gradient of the division operation.\n",
    "\n",
    "        Args:\n",
    "            out_grad (Tensor): The gradient of the output tensor.\n",
    "            node (Tensor): The node in the computational graph where the operation was performed.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tensor, ...]: The gradient with respect to the tensor.\n",
    "        \"\"\"\n",
    "        return (out_grad / self.scalar, )\n",
    "\n",
    "def divide_scalar(a: Tensor, scalar: Union[int, float]) -> Tensor:\n",
    "    \"\"\"\n",
    "    Divides a tensor by a scalar.\n",
    "\n",
    "    Args:\n",
    "        a (Tensor): The tensor to divide.\n",
    "        scalar (int, float): The scalar to divide the tensor by.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: The resulting tensor after division.\n",
    "\n",
    "    Example:\n",
    "        >>> import numpy as np\n",
    "        >>> a = Tensor(np.array([1, 2, 3]))\n",
    "        >>> scalar = 2\n",
    "        >>> result = divide_scalar(a, scalar)\n",
    "        >>> print(result)\n",
    "        Tensor([0.5, 1.0, 1.5])\n",
    "    \"\"\"\n",
    "    return DivScalar(scalar)(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6497d8ab-3003-4784-a330-ad5f862b9ca5",
   "metadata": {},
   "source": [
    "## Negation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04085053-1446-4343-b720-17e04a1c4ee1",
   "metadata": {},
   "source": [
    "Let's denote `a` as the tensor being negated. The operation can be described as `f(a) = -a`.\n",
    "\n",
    "The function for the backward pass (i.e., the gradient) is `df/da = -1`.\n",
    "\n",
    "We are given a function $f(a) = -a$, where $a$ is a tensor. Our task is to find the derivative of this function with respect to $a$.\n",
    "\n",
    "By differentiating the function $f(a)$ with respect to $a$, we find:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{da} &= \\frac{d}{da} (-a) \\\\\n",
    "&= -1\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, the gradient of $f(a)$ with respect to $a$ is $-1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defd870b-d2e6-4212-bd4b-b3333d271c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Negate(TensorOp):\n",
    "    \"\"\"\n",
    "    Negates the given tensor.\n",
    "    \n",
    "    Example:\n",
    "    >>> a = Tensor([1, -2, 3])\n",
    "    >>> op = Negate()\n",
    "    >>> result = op.compute(a)\n",
    "    >>> print(result)\n",
    "    Tensor([-1, 2, -3])\n",
    "    \"\"\"\n",
    "    \n",
    "    def compute(self, a: NDArray) -> NDArray:\n",
    "        \"\"\"\n",
    "        Computes the negation of a tensor.\n",
    "\n",
    "        Args:\n",
    "        - a: The tensor to negate.\n",
    "\n",
    "        Returns:\n",
    "        The negation of a.\n",
    "        \"\"\"\n",
    "        return -1 * a\n",
    "\n",
    "    def gradient(self, out_grad: Tensor, node: Tensor) -> Tuple[Tensor,]:\n",
    "        \"\"\"\n",
    "        Computes the gradient of the negation operation.\n",
    "\n",
    "        Args:\n",
    "        - out_grad: The gradient of the output of the operation.\n",
    "        - node: The node in the computational graph where the operation was performed.\n",
    "\n",
    "        Returns:\n",
    "        The gradients with respect to the inputs.\n",
    "        \"\"\"\n",
    "        return (negate(out_grad), )\n",
    "\n",
    "\n",
    "def negate(a: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Negates the given tensor.\n",
    "\n",
    "    Args:\n",
    "    - a: The tensor to negate.\n",
    "\n",
    "    Returns:\n",
    "    The negation of a.\n",
    "    \n",
    "    Example:\n",
    "    >>> a = Tensor([1, -2, 3])\n",
    "    >>> result = negate(a)\n",
    "    >>> print(result)\n",
    "    Tensor([-1, 2, -3])\n",
    "    \"\"\"\n",
    "    return Negate()(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e28e521-a653-45e0-a567-46c7b800d281",
   "metadata": {},
   "source": [
    "## Exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8bf43a-9a2a-4077-a54f-54df0c6c955d",
   "metadata": {},
   "source": [
    "Explanation for the derivative of the `Exp` operator:\n",
    "\n",
    "Let's denote `a` as the tensor on which the exponential function is applied. The operation can be described as `f(a) = exp(a)`, where `exp` represents the exponential function.\n",
    "\n",
    "The function for the backward pass (i.e., the gradient) is `df/da = exp(a)`.\n",
    "\n",
    "We are given a function $f(a) = \\exp(a)$, where $a$ is a tensor. Our task is to find the derivative of this function with respect to $a$.\n",
    "\n",
    "By differentiating the function $f(a)$ with respect to $a$, we find:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{da} &= \\frac{d}{da} (\\exp(a)) \\\\\n",
    "&= \\exp(a)\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, the gradient of $f(a)$ with respect to $a$ is $\\exp(a)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a810bf4e-1d48-412b-bf89-2a6fde789ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Exp(TensorOp):\n",
    "    \"\"\"\n",
    "    Calculates the exponential of the given tensor.\n",
    "    \n",
    "    Example:\n",
    "    >>> a = Tensor([1, 2, 3])\n",
    "    >>> op = Exp()\n",
    "    >>> result = op.compute(a)\n",
    "    >>> print(result)\n",
    "    Tensor([2.71828183, 7.3890561, 20.08553692])\n",
    "    \"\"\"\n",
    "    \n",
    "    def compute(self, a: NDArray) -> NDArray:\n",
    "        \"\"\"\n",
    "        Computes the exponential of a tensor.\n",
    "\n",
    "        Args:\n",
    "        - a: The tensor.\n",
    "\n",
    "        Returns:\n",
    "        The exponential of a.\n",
    "        \"\"\"\n",
    "        self.out = ARRAY_API.exp(a)\n",
    "        return self.out\n",
    "\n",
    "    def gradient(self, out_grad: Tensor, node: Tensor) -> Tuple[Tensor,]:\n",
    "        \"\"\"\n",
    "        Computes the gradient of the exponential operation.\n",
    "\n",
    "        Args:\n",
    "        - out_grad: The gradient of the output of the operation.\n",
    "        - node: The node in the computational graph where the operation was performed.\n",
    "\n",
    "        Returns:\n",
    "        The gradients with respect to the inputs.\n",
    "        \"\"\"\n",
    "        return (out_grad * self.out, )\n",
    "\n",
    "def exp(a: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Calculates the exponential of the given tensor.\n",
    "\n",
    "    Args:\n",
    "    - a: The tensor.\n",
    "\n",
    "    Returns:\n",
    "    The exponential of a.\n",
    "    \n",
    "    Example:\n",
    "    >>> a = Tensor([1, 2, 3])\n",
    "    >>> result = exp(a)\n",
    "    >>> print(result)\n",
    "    Tensor([2.71828183, 7.3890561, 20.08553692])\n",
    "    \"\"\"\n",
    "    return Exp()(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45592e37-9a6d-42ec-8458-167a94394cc1",
   "metadata": {},
   "source": [
    "## ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8aefd6-23e2-4df7-8627-f74813b8f0bc",
   "metadata": {},
   "source": [
    "The derivative of the `ReLU` (Rectified Linear Unit) operator:\n",
    "\n",
    "Let's denote `a` as the tensor on which the ReLU function is applied. The ReLU function is defined as follows: \n",
    "\n",
    "$$\n",
    "f(a) = \n",
    "\\begin{cases}\n",
    "a, & \\text{if } a \\geq 0 \\\\\n",
    "0, & \\text{if } a < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The function for the backward pass (i.e., the gradient) is `df/da = 1` if `a >= 0`, and `df/da = 0` if `a < 0`.\n",
    "\n",
    "We are given a function $f(a) = \\max(0, a)$, where $a$ is a tensor. Our task is to find the derivative of this function with respect to $a$.\n",
    "\n",
    "By considering the definition of the ReLU function, we can write $f(a)$ as:\n",
    "\n",
    "$$\n",
    "f(a) = \n",
    "\\begin{cases}\n",
    "a, & \\text{if } a \\geq 0 \\\\\n",
    "0, & \\text{if } a < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Now, let's differentiate $f(a)$ with respect to $a$:\n",
    "\n",
    "$$\n",
    "\\frac{df}{da} = \n",
    "\\begin{cases}\n",
    "1, & \\text{if } a \\geq 0 \\\\\n",
    "0, & \\text{if } a < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Therefore, the gradient of $f(a)$ with respect to $a$ is $1$ if $a \\geq 0$, and $0$ if $a < 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32b2ce4-7d9e-4290-90d9-a56bc59cfe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ReLU(TensorOp):\n",
    "    \"\"\"\n",
    "    Applies the ReLU (Rectified Linear Unit) activation function to the given tensor.\n",
    "    \n",
    "    Example:\n",
    "    >>> a = Tensor([1, -2, 3])\n",
    "    >>> op = ReLU()\n",
    "    >>> result = op.compute(a)\n",
    "    >>> print(result)\n",
    "    Tensor([1, 0, 3])\n",
    "    \"\"\"\n",
    "    \n",
    "    def compute(self, a: NDArray) -> NDArray:\n",
    "        \"\"\"\n",
    "        Computes the ReLU activation function on a tensor.\n",
    "\n",
    "        Args:\n",
    "        - a: The tensor.\n",
    "\n",
    "        Returns:\n",
    "        The result of applying ReLU to a.\n",
    "        \"\"\"\n",
    "        self.out = ARRAY_API.clip(a, a_min=0)\n",
    "        return self.out\n",
    "\n",
    "    def gradient(self, out_grad: Tensor, node: Tensor) -> Tuple[Tensor,]:\n",
    "        \"\"\"\n",
    "        Computes the gradient of the ReLU operation.\n",
    "\n",
    "        Args:\n",
    "        - out_grad: The gradient of the output of the operation.\n",
    "        - node: The node in the computational graph where the operation was performed.\n",
    "\n",
    "        Returns:\n",
    "        The gradients with respect to the inputs.\n",
    "        \"\"\"\n",
    "        return (out_grad * Tensor(node.children[0] >= 0), )\n",
    "\n",
    "def relu(a: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Applies the ReLU (Rectified Linear Unit) activation function to the given tensor.\n",
    "\n",
    "    Args:\n",
    "    - a: The tensor.\n",
    "\n",
    "    Returns:\n",
    "    The result of applying ReLU to a.\n",
    "    \n",
    "    Example:\n",
    "    >>> a = Tensor([1, -2, 3])\n",
    "    >>> result = relu(a)\n",
    "    >>> print(result)\n",
    "    Tensor([1, 0, 3])\n",
    "    \"\"\"\n",
    "    return ReLU()(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9f074a-e23c-4e8a-8ab7-2cfba344c461",
   "metadata": {},
   "source": [
    "## Power Scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d5e875-0981-4243-86b5-d6ca6b117d5e",
   "metadata": {},
   "source": [
    "The derivative of the `PowerScalar` operator:\n",
    "\n",
    "Let's denote the scalar as `n` and `a` as the tensor being raised to the power of the scalar. The operation can be described as `f(a) = a^n`.\n",
    "\n",
    "The function for the backward pass (i.e., the gradient) is `df/da = n * a^(n-1)`.\n",
    "\n",
    "We are given a function $f(a) = a^n$, where $a$ is a tensor and $n$ is a scalar. Our task is to find the derivative of this function with respect to $a$.\n",
    "\n",
    "By differentiating the function $f(a)$ with respect to $a$, we find:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{da} &= \\frac{d}{da} (a^n) \\\\\n",
    "&= n \\cdot a^{n-1}\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, the gradient of $f(a)$ with respect to $a$ is $n \\cdot a^{n-1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364103cb-615b-4359-8061-5a8bd1455367",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PowerScalar(TensorOp):\n",
    "    \"\"\"\n",
    "    The PowerScalar operation raises a tensor to an (integer) power.\n",
    "\n",
    "    Attributes:\n",
    "        scalar (int): The power to raise the tensor to.\n",
    "\n",
    "    Example:\n",
    "        >>> import numpy as np\n",
    "        >>> tensor = Tensor(np.array([1, 2, 3]))\n",
    "        >>> pow_scalar = PowerScalar(2)\n",
    "        >>> result = pow_scalar.compute(tensor.data)\n",
    "        >>> print(result)\n",
    "        array([1, 4, 9])\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scalar: int):\n",
    "        \"\"\"\n",
    "        Constructs the PowerScalar operation.\n",
    "\n",
    "        Args:\n",
    "            scalar (int): The power to raise the tensor to.\n",
    "        \"\"\"\n",
    "        self.scalar = scalar\n",
    "\n",
    "    def compute(self, a: NDArray) -> NDArray:\n",
    "        \"\"\"\n",
    "        Computes the power operation on the input tensor.\n",
    "\n",
    "        Args:\n",
    "            a (NDArray): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            NDArray: The resulting tensor after the power operation.\n",
    "        \"\"\"\n",
    "        return ARRAY_API.power(a, self.scalar)\n",
    "\n",
    "    def gradient(self, out_grad: Tensor, node: Tensor) -> Tuple[Tensor, ]:\n",
    "        \"\"\"\n",
    "        Computes the gradient of the power operation.\n",
    "\n",
    "        Args:\n",
    "            out_grad (Tensor): The gradient of the output tensor.\n",
    "            node (Tensor): The node in the computational graph where the operation was performed.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tensor, ]: The gradient with respect to the input tensor.\n",
    "        \"\"\"\n",
    "        a = node.children[0]\n",
    "        return (self.scalar * power_scalar(a, self.scalar - 1) * out_grad, )\n",
    "\n",
    "\n",
    "def power_scalar(a: Tensor, scalar: int) -> Tensor:\n",
    "    \"\"\"\n",
    "    Raises a tensor to a power.\n",
    "\n",
    "    Args:\n",
    "        a (Tensor): The input tensor.\n",
    "        scalar (int): The power to raise the tensor to.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: The resulting tensor after the power operation.\n",
    "\n",
    "    Example:\n",
    "        >>> import numpy as np\n",
    "        >>> tensor = Tensor(np.array([1, 2, 3]))\n",
    "        >>> result = power_scalar(tensor, 2)\n",
    "        >>> print(result)\n",
    "        Tensor([1, 4, 9])\n",
    "    \"\"\"\n",
    "    return PowerScalar(scalar)(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff76cab1-8050-4df9-87fa-87fc58145f68",
   "metadata": {},
   "source": [
    "## Log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c3d794-0946-4ab1-8e9c-e4483803c4c1",
   "metadata": {},
   "source": [
    "Explanation for the derivative of the `Log` operator:\n",
    "\n",
    "Let's denote `a` as the tensor on which the logarithm is applied. The operation can be described as `f(a) = log(a)`, where `log` represents the natural logarithm.\n",
    "\n",
    "The function for the backward pass (i.e., the gradient) is `df/da = 1/a`.\n",
    "\n",
    "We are given a function $f(a) = \\log(a)$, where $a$ is a tensor. Our task is to find the derivative of this function with respect to $a$.\n",
    "\n",
    "By differentiating the function $f(a)$ with respect to $a$, we find:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{da} &= \\frac{d}{da} (\\log(a)) \\\\\n",
    "&= \\frac{1}{a}\n",
    "\\end{align*}\n",
    "\n",
    "We started by defining the function `f(a) = log(a)`, where `log` represents the natural logarithm. It then explains that when we differentiate `f(a)` with respect to `a`, we find that the derivative is `1/a`. This means that the gradient of `f(a)` with respect to `a` is `1/a`, which represents the behavior of the `Log` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb52c72-9bb5-4d01-bd08-59cb37acfa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Log(TensorOp):\n",
    "    \"\"\"\n",
    "    The Log operation applies the natural logarithm element-wise on the tensor.\n",
    "\n",
    "    Example:\n",
    "        >>> import numpy as np\n",
    "        >>> a = Tensor(np.array([1.0, 2.0, 3.0]))\n",
    "        >>> log_op = Log()\n",
    "        >>> result = log_op.compute(a.data)\n",
    "        >>> print(result)\n",
    "        array([0., 0.69314718, 1.09861229])\n",
    "    \"\"\"\n",
    "\n",
    "    def compute(self, a: NDArray) -> NDArray:\n",
    "        \"\"\"\n",
    "        Applies the natural logarithm to the tensor.\n",
    "\n",
    "        Args:\n",
    "            a (NDArray): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            NDArray: The resulting tensor after applying the natural logarithm.\n",
    "        \"\"\"\n",
    "        return ARRAY_API.log(a)\n",
    "\n",
    "    def gradient(self, out_grad: Tensor, node: Tensor) -> Tuple[Tensor, ...]:\n",
    "        \"\"\"\n",
    "        Computes the gradient of the log operation.\n",
    "\n",
    "        Args:\n",
    "            out_grad (Tensor): The gradient of the output tensor.\n",
    "            node (Tensor): The node in the computational graph where the operation was performed.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tensor, ...]: The gradient with respect to the input tensor.\n",
    "        \"\"\"\n",
    "        a = node.children[0]\n",
    "        return (out_grad / a, )\n",
    "\n",
    "def log(a: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Applies the natural logarithm to the tensor.\n",
    "\n",
    "    Args:\n",
    "        a (Tensor): The input tensor.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: The resulting tensor after applying the natural logarithm.\n",
    "\n",
    "    Example:\n",
    "        >>> import numpy as np\n",
    "        >>> a = Tensor(np.array([1.0, 2.0, 3.0]))\n",
    "        >>> result = log(a)\n",
    "        >>> print(result)\n",
    "        Tensor([0., 0.69314718, 1.09861229])\n",
    "    \"\"\"\n",
    "    return Log()(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecaeed3-334f-4d00-9027-5329af5208af",
   "metadata": {},
   "source": [
    "## Transpose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21611651-aa6d-45e0-98a0-a047b73e7cb7",
   "metadata": {},
   "source": [
    "This operation described here is the derivative of a transposition operation. Let's define our transposition operation as a function `f` such that `f(a) = a^T` where `a` is a tensor, and `a^T` is the transpose of tensor `a`. \n",
    "\n",
    "The goal here is to compute the derivative of this operation with respect to `a`. It's important to note that transposition operation doesn't change the values of the tensor's elements, but it just rearranges their positions. This implies that the gradient (derivative) of a transposed tensor is simply the transposed gradient of the original tensor. \n",
    "\n",
    "Let's denote the gradient of the transposed tensor as `g`, which can be mathematically represented as `g = df/da`, where `df/da` is the derivative of `f(a)` with respect to `a`. \n",
    "\n",
    "Given this understanding, we can make an important conclusion:\n",
    "\n",
    "1. The derivative of `f(a)` with respect to `a` is `df/da = g^T`, meaning that the derivative of the transposed tensor is simply the transposed gradient of the original tensor.\n",
    "\n",
    "This concept can be written in mathematical terms using LaTeX as follows:\n",
    "\n",
    "We have a function $f(a) = a^T$, where $a$ is a tensor and $a^T$ is its transpose. We want to find the derivative of this function with respect to $a$, that is, compute $\\frac{df}{da}$.\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{da} &= \\frac{d}{da} (a^T) \\\\\n",
    "&= (g)^T\n",
    "\\end{align*}\n",
    "\n",
    "In the equation above, $g$ is the gradient of the transposed tensor. This equation indicates that the derivative of the transpose of a tensor is the transpose of the gradient of the original tensor.\n",
    "\n",
    "Now, if we consider a Python class `Transpose` that implements this transposition operation, we would have a `gradient` method in the class that computes the derivative of the transpose operation. This method would apply the transpose function to `out_grad`, which represents the gradient of the output tensor, thereby giving us the transposed gradient of the original tensor. In the code, `transpose(out_grad, axes=self.axes)` performs the transposition of `out_grad` along the same axes that were used in the forward pass. Thus, the gradient of the transposition operation with respect to the input tensor `a` is computed as the transpose of the output gradient `out_grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138262c2-202b-4e38-b3e0-c3655835aac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Transpose(TensorOp):\n",
    "    \"\"\"\n",
    "    Tensor operation class that performs transposition of a tensor along specified axes.\n",
    "    \n",
    "    If no axes are specified, it swaps the last two dimensions of the input tensor.\n",
    "\n",
    "    Example:\n",
    "        >>> a = Tensor(np.arange(1, 7).reshape(2, 3))\n",
    "        >>> op = Transpose()\n",
    "        >>> result = op.compute(a.data)\n",
    "        >>> print(result)\n",
    "        array([[1, 4],\n",
    "               [2, 5],\n",
    "               [3, 6]])\n",
    "    \"\"\"\n",
    "    def __init__(self, axes: Optional[tuple] = None):\n",
    "        \"\"\"\n",
    "        Initialize the operation with the specified axes.\n",
    "\n",
    "        Args:\n",
    "            axes (Optional[tuple]): The pair of axes that should be swapped. If not provided, the last two axes are swapped.\n",
    "        \"\"\"\n",
    "        self.axes = axes\n",
    "\n",
    "    def compute(self, a: NDArray) -> NDArray:\n",
    "        \"\"\"\n",
    "        Perform the transpose operation.\n",
    "\n",
    "        Args:\n",
    "            a (NDArray): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            NDArray: The transposed tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.axes:\n",
    "            a = a.swapaxes(self.axes[0], self.axes[1])\n",
    "        else:\n",
    "            a = a.swapaxes(-2, -1)\n",
    "        return a\n",
    "\n",
    "    def gradient(self, out_grad: Tensor, node: Tensor) -> Tuple[Tensor, ...]:\n",
    "        \"\"\"\n",
    "        Compute the gradient of the transpose operation.\n",
    "\n",
    "        Args:\n",
    "            out_grad (Tensor): The gradient of the output tensor.\n",
    "            node (Tensor): The node in the computational graph where the operation was performed.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tensor, ...]: The gradient with respect to the input tensor.\n",
    "        \"\"\"\n",
    "        return (transpose(out_grad, axes=self.axes), )\n",
    "\n",
    "def transpose(a: Tensor, axes: Optional[tuple] = None) -> Tensor:\n",
    "    \"\"\"\n",
    "    Perform the transpose operation on the input tensor along the specified axes.\n",
    "    If no axes are specified, it swaps the last two dimensions of the input tensor.\n",
    "\n",
    "    Args:\n",
    "        a (Tensor): The input tensor.\n",
    "        axes (Optional[tuple]): The pair of axes that should be swapped. If not provided, the last two axes are swapped.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: The transposed tensor.\n",
    "\n",
    "    Example:\n",
    "        >>> a = Tensor(np.arange(1, 7).reshape(2, 3))\n",
    "        >>> result = transpose(a)\n",
    "        >>> print(result)\n",
    "        Tensor([[1, 4],\n",
    "                [2, 5],\n",
    "                [3, 6]])\n",
    "    \"\"\"\n",
    "    return Transpose(axes)(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6439958d-fd59-4e61-b249-4dd00bfc2cb4",
   "metadata": {},
   "source": [
    "## Reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4da8fb7-4f87-47a0-b5b4-468b46c2b14e",
   "metadata": {},
   "source": [
    "The operation described here is a reshaping of a tensor `a`, where the operation can be described as `f(a) = reshape(a, new_shape)`.\n",
    "\n",
    "We'll compute the derivative of this operation.\n",
    "\n",
    "The reshaping operation doesn't change the values of the tensor elements but only rearranges them. This means that the gradient of a reshaped tensor is just the reshaped gradient of the original tensor.\n",
    "\n",
    "Let's denote the gradient of the reshaped tensor as `g = df/da`, where `f(a) = reshape(a, new_shape)`.\n",
    "\n",
    "Given this, we can derive the following:\n",
    "\n",
    "1. The derivative of `f(a)` with respect to `a` is `df/da = reshape(g, original_shape)`.\n",
    "\n",
    "This conclusion can be illustrated as follows in Latex:\n",
    "\n",
    "We are given a function $f(a) = reshape(a, new\\_shape)$, where $a$ is a tensor and `reshape(a, new_shape)` is the reshaped tensor. Our task is to find the derivative of this function with respect to $a$.\n",
    "\n",
    "Let's compute $\\frac{df}{da}$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{da} &= \\frac{d}{da} (reshape(a, new\\_shape)) \\\\\n",
    "&= reshape(g, original\\_shape)\n",
    "\\end{align*}\n",
    "\n",
    "Here, $g$ is the gradient of the reshaped tensor. The derivative of a reshaped tensor is the reshaped derivative of the original tensor. The reshaped derivative has the same shape as the original tensor.\n",
    "\n",
    "Now, let's apply this to the `Reshape` class.\n",
    "\n",
    "The `gradient` method in the `Reshape` class computes the gradient of the reshape operation. The gradient of the reshaped tensor is just the reshaped gradient of the original tensor. This is implemented by applying the `reshape` function to `out_grad`, which is the gradient of the output tensor, and then returning this reshaped gradient. The shape used for the reshaping is the shape of the original tensor, which is obtained from `node.children[0].shape`.\n",
    "\n",
    "Therefore, the gradient of the reshape operation with respect to the input tensor `a` is the reshaping of the output gradient `out_grad` to the shape of the original tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4463dc19-29ad-41b0-89e1-c08f66bc27b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Reshape(TensorOp):\n",
    "    \"\"\"\n",
    "    Tensor operation class that reshapes a tensor.\n",
    "\n",
    "    Example:\n",
    "        >>> a = Tensor([1, 2, 3, 4, 5, 6])\n",
    "        >>> op = Reshape((2, 3))\n",
    "        >>> result = op.compute(a)\n",
    "        >>> print(result)\n",
    "        Tensor([[1, 2, 3],\n",
    "                 [4, 5, 6]])\n",
    "    \"\"\"\n",
    "    def __init__(self, shape: Tuple[int, ...]):\n",
    "        \"\"\"\n",
    "        Initialize the operation with the target shape.\n",
    "\n",
    "        Args:\n",
    "            shape (Tuple[int, ...]): The desired shape of the output tensor.\n",
    "        \"\"\"\n",
    "        self.shape = shape\n",
    "\n",
    "    def compute(self, a: NDArray) -> NDArray:\n",
    "        \"\"\"\n",
    "        Perform the reshape operation.\n",
    "\n",
    "        Args:\n",
    "            a (NDArray): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            NDArray: The reshaped tensor.\n",
    "        \"\"\"\n",
    "        return ARRAY_API.reshape(a, newshape=self.shape)\n",
    "\n",
    "    def gradient(self, out_grad: Tensor, node: Tensor) -> Tuple[Tensor, ...]:\n",
    "        \"\"\"\n",
    "        Compute the gradient of the reshape operation.\n",
    "\n",
    "        Args:\n",
    "            out_grad (Tensor): The gradient of the output tensor.\n",
    "            node (Tensor): The node in the computational graph where the operation was performed.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tensor, ...]: The gradient with respect to the input tensor.\n",
    "        \"\"\"\n",
    "        input_shape = node.children[0].shape\n",
    "        return reshape(out_grad, input_shape), \n",
    "\n",
    "def reshape(a: Tensor, shape: Tuple[int, ...]) -> Tensor:\n",
    "    \"\"\"\n",
    "    Reshape the input tensor to the specified shape.\n",
    "\n",
    "    Args:\n",
    "        a (Tensor): The input tensor.\n",
    "        shape (Tuple[int, ...]): The desired shape of the output tensor.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: The reshaped tensor.\n",
    "\n",
    "    Example:\n",
    "        >>> a = Tensor([1, 2, 3, 4, 5, 6])\n",
    "        >>> result = reshape(a, (2, 3))\n",
    "        >>> print(result)\n",
    "        Tensor([[1, 2, 3],\n",
    "                 [4, 5, 6]])\n",
    "    \"\"\"\n",
    "    return Reshape(shape)(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b693f6-4b73-4f7e-887a-99560def4a93",
   "metadata": {},
   "source": [
    "## Matrix Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb04615-214d-43b0-86ff-f8551f6bd3b7",
   "metadata": {},
   "source": [
    "Matrix multiplication, often denoted by \"matmul\" in some programming languages, refers to the process of multiplying two matrices together. However, in the context of calculus, it's more common to talk about the derivative of a function. \n",
    "\n",
    "When dealing with matrices, instead of talking about derivatives, we often discuss the Jacobian, which is a matrix of partial derivatives. If you have a function that takes a matrix as input and produces a scalar output, you could compute a gradient, which would be a matrix of the same shape as the input matrix.\n",
    "\n",
    "However, in the context of deep learning and backpropagation, you might be asking about the derivative of a matrix multiplication operation with respect to its inputs. This is often needed when you're training a neural network, because you need to compute gradients to update the weights.\n",
    "\n",
    "Let's denote the matrices as `A` and `B`, where `A` is a matrix of dimension `m x n` and `B` is a matrix of dimension `n x p`, and the result of the multiplication `C = A * B` is a matrix of dimension `m x p`.\n",
    "\n",
    "If we are to compute the derivative of `C` with respect to `A` (i.e., C/A), each element in `A` affects all elements in its corresponding row in `C`.\n",
    "\n",
    "Similarly, if we are to compute the derivative of `C` with respect to `B` (i.e., C/B), each element in `B` affects all elements in its corresponding column in `C`. \n",
    "\n",
    "In actual computation, if we have a scalar-valued loss function `L`, we would compute the gradient of `L` with respect to `A` (denoted as L/A), which is the same shape as `A`. To compute this, we need to know the gradient of `L` with respect to `C` (denoted as L/C), then:\n",
    "\n",
    "L/A = (L/C) * B^T   (where * denotes matrix multiplication and B^T is the transpose of B)\n",
    "\n",
    "Similarly, to compute the gradient of `L` with respect to `B` (denoted as L/B):\n",
    "\n",
    "L/B = A^T * (L/C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c60dd7-18f4-4c1a-b445-50ff9df966bb",
   "metadata": {},
   "source": [
    "The line `axes_to_sum_over = tuple(range(len(out_shape) - len(lhs_shape)))` is calculating which axes (dimensions) of the output gradient tensor (`out_grad`) need to be summed over when computing the gradient with respect to the left-hand side (`a`) input tensor.\n",
    "\n",
    "This is necessary when the rank (number of dimensions) of `out_grad` is larger than the rank of `a`. This can happen, for instance, when `a` is a matrix (2D tensor) and `out_grad` is a 3D tensor (which can result from batched matrix multiplication).\n",
    "\n",
    "The `range` function generates a sequence of integers from 0 up to (but not including) `len(out_shape) - len(lhs_shape)`. The `tuple` function then takes this sequence and turns it into a tuple. The result is a tuple of integers representing the axes to sum over.\n",
    "\n",
    "Here is a concrete example:\n",
    "\n",
    "Suppose we have a batched matrix multiplication where `A` is a matrix of shape `(m, n)`, and `out_grad` is a 3D tensor of shape `(b, m, n)`, where `b` is the batch size. \n",
    "\n",
    "In this case, `len(out_shape) - len(a_shape)` equals `1`, so `range(len(out_shape) - len(lhs_shape))` generates a sequence of integers from `0` to `1` (not inclusive), which is just `[0]`.\n",
    "\n",
    "So `axes_to_sum_over` will be `(0,)`, indicating that we need to sum over the first axis (the batch axis) of `out_grad` when computing the gradient with respect to `A`.\n",
    "\n",
    "This summing operation effectively accumulates the individual gradients for each item in the batch into a single gradient for the `A` matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dd735e-ed91-4703-b860-f06684673d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Suppose we have the following shapes for `lhs` and `out_grad`\n",
    "m, n, b = 5, 7, 3\n",
    "\n",
    "# Let's create some tensors with these shapes\n",
    "A = torch.randn(m, n)          # lhs is a 2D tensor (matrix) of shape (m, n)\n",
    "out_grad = torch.randn(b, m, n)  # out_grad is a 3D tensor of shape (b, m, n)\n",
    "\n",
    "# Let's say `rhs` is another matrix that was involved in computing out_grad\n",
    "B = torch.randn(n, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83b2489-10fb-4796-82cd-15c94d1586e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 5, 7]), torch.Size([5, 7]), torch.Size([7, 5]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_shape, A_shape, B_shape = out_grad.shape, A.shape, B.shape\n",
    "out_shape, A_shape, B_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b883959-15a4-4f63-827e-6d2cea419e16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out_shape), len(A_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b267d83b-2604-444e-8bd6-01e1c7b19cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 1)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = range(len(out_shape) - len(A_shape))\n",
    "rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865fd42c-482d-479e-9d1f-9fe7183a5d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5849d174-7aab-4406-9c4f-caad9eaf95fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "axes_to_sum_over = tuple(range(len(out_shape) - len(A_shape)))\n",
    "axes_to_sum_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad258ce-40a0-4f6c-beed-69691d3c9a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0778, -4.0464,  2.0688,  2.1114,  1.0043],\n",
       "        [-2.9080, -2.8734, -1.9057, -2.2809,  0.2633],\n",
       "        [-0.1851, -1.7494,  2.2525,  5.6627,  0.6030],\n",
       "        [-3.1977, -5.8677,  3.4838,  3.2514, -4.5241],\n",
       "        [-2.5973, -9.4338, -0.1997,  2.9862, -5.9788]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(out_grad @ B, axes_to_sum_over) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d466e7b-39ec-4106-9baa-ec95f759ac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MatMul(TensorOp):\n",
    "    \"\"\"\n",
    "    Tensor operation class that performs matrix multiplication.\n",
    "\n",
    "    Example:\n",
    "        >>> a = Tensor([[1, 2], [3, 4]])\n",
    "        >>> b = Tensor([[5, 6], [7, 8]])\n",
    "        >>> op = MatMul()\n",
    "        >>> result = op.compute(a, b)\n",
    "        >>> print(result)\n",
    "        Tensor([[19, 22],\n",
    "                 [43, 50]])\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def compute(self, a: NDArray, b: NDArray) -> NDArray:\n",
    "        \"\"\"\n",
    "        Perform the matrix multiplication operation.\n",
    "\n",
    "        Args:\n",
    "            a (NDArray): The first input tensor.\n",
    "            b (NDArray): The second input tensor.\n",
    "\n",
    "        Returns:\n",
    "            NDArray: The product of a and b.\n",
    "        \"\"\"\n",
    "        return ARRAY_API.matmul(a, b)\n",
    "\n",
    "    \n",
    "    def gradient(self, out_grad: Tensor, node: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Compute the gradient of the matrix multiplication operation.\n",
    "\n",
    "        Args:\n",
    "            out_grad (Tensor): The gradient of the output tensor.\n",
    "            node (Tensor): The node in the computational graph where the operation was performed.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tensor, Tensor]: The gradients with respect to the input tensors.\n",
    "        \"\"\"\n",
    "        a, b = node.children\n",
    "        out_shape, a_shape, b_shape = out_grad.shape, a.shape, b.shape\n",
    "        \n",
    "        # Compute the gradient with respect to a\n",
    "        if len(a_shape) == len(out_shape):\n",
    "            # If a and the output have the same dimensionality, we perform a matrix multiplication\n",
    "            # between the output gradient and the transpose of b\n",
    "            grad_wrt_a = matmul(out_grad, transpose(b))\n",
    "        else:\n",
    "            # If a has fewer dimensions than the output, we sum over the extra dimensions in the output\n",
    "            axes_to_sum_over = tuple(range(len(out_shape) - len(a_shape)))\n",
    "            grad_wrt_a = summation(matmul(out_grad, transpose(b)), axes=axes_to_sum_over)\n",
    "\n",
    "        # Compute the gradient with respect to b\n",
    "        if len(b_shape) == len(out_shape):\n",
    "            # If b and the output have the same dimensionality, we perform a matrix multiplication\n",
    "            # between the transpose of a and the output gradient\n",
    "            grad_wrt_b = matmul(transpose(a), out_grad)\n",
    "        else:\n",
    "            # If b has fewer dimensions than the output, we sum over the extra dimensions in the output\n",
    "            axes_to_sum_over = tuple(range(len(out_shape) - len(b_shape)))\n",
    "            grad_wrt_b = summation(matmul(transpose(a), out_grad), axes=axes_to_sum_over)\n",
    "\n",
    "        return grad_wrt_a, grad_wrt_b\n",
    "\n",
    "\n",
    "def matmul(a: Tensor, b: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Perform matrix multiplication on two tensors.\n",
    "\n",
    "    Args:\n",
    "        a (Tensor): The first input tensor.\n",
    "        b (Tensor): The second input tensor.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: The product of a and b.\n",
    "\n",
    "    Example:\n",
    "        >>> a = Tensor([[1, 2], [3, 4]])\n",
    "        >>> b = Tensor([[5, 6], [7, 8]])\n",
    "        >>> result = matmul(a, b)\n",
    "        >>> print(result)\n",
    "        Tensor([[19, 22],\n",
    "                 [43, 50]])\n",
    "    \"\"\"\n",
    "    return MatMul()(a, b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffe0c26-5714-4c77-9b52-139bab8eed60",
   "metadata": {},
   "source": [
    "## Summation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839315bd-7667-4192-9aed-b5e4a1d34b0a",
   "metadata": {},
   "source": [
    "The `Summation` operation, when provided with the `axes` argument, sums over these axes and thereby reduces the rank of the tensor by the number of axes summed over. The backward pass needs to take this into account, as it needs to return a gradient tensor of the same shape as the input.\n",
    "\n",
    "The forward pass (`compute` method) is straightforward - it just computes the sum over the specified axes.\n",
    "\n",
    "In the backward pass (`gradient` method), the goal is to compute the gradient of the sum operation. Since every element of the input tensor contributes equally to the sum, the derivative of the sum with respect to each element is 1. However, since the sum operation may reduce the dimensionality of the tensor (when `axes` is not `None`), we need to account for this when computing the gradient.\n",
    "\n",
    "To do this, we first create a new shape, where the dimensions specified by `axes` are replaced by 1. We then reshape `out_grad` to this new shape. This essentially \"undoes\" the dimensionality reduction performed by the sum operation. Finally, we use `broadcast_to` to make the reshaped gradient tensor the same shape as the input tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191b77b6-982e-4613-8ddc-de39ee8253dc",
   "metadata": {},
   "source": [
    "Suppose you have the following tensor in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfdd64c-c8f3-4fe8-a613-cbcd179cd93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3x3 tensor\n",
    "x = torch.tensor([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]], requires_grad=True)\n",
    "\n",
    "# Sum over axis 0\n",
    "y = x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c938f74-5389-453e-9970-9459de0182a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12., 15., 18.], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71e2520-73f3-44dc-85ae-39d609c35638",
   "metadata": {},
   "source": [
    "`y` is now a 1-dimensional tensor of shape `(3,)`, because we've summed over axis 0. If we compute the gradient of `y` with respect to `x`, we'll want the resulting gradient tensor to have the same shape as `x`, which is `(3,3)`. However, the gradient tensor we receive during backpropagation (`out_grad`) will have the same shape as `y`, which is `(3,)`.\n",
    "\n",
    "So we need to \"undo\" the dimensionality reduction by reshaping and broadcasting `out_grad` to match the shape of `x`. Here's how you can do it in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17f204d-bf16-41b7-803b-a3e6a45ce692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Mock out_grad tensor\n",
    "out_grad = torch.tensor([1., 1., 1.])\n",
    "\n",
    "# Reshape out_grad to have an additional dimension\n",
    "reshaped_grad = out_grad.reshape(3, 1)\n",
    "\n",
    "# Broadcast the reshaped_grad to match the input shape\n",
    "broadcasted_grad = reshaped_grad.expand_as(x)\n",
    "\n",
    "print(broadcasted_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9682cc23-8d76-4c17-b7e8-42001ea64903",
   "metadata": {},
   "source": [
    "Now `broadcasted_grad` has the same shape as `x`, so it can be correctly used as the gradient of `x` in further computations. This manual operation simulates what the `gradient` function of the `Summation` operation is doing in your original code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5d1385-012b-4fe3-8528-df8cf5be696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Summation(TensorOp):\n",
    "    \"\"\"\n",
    "    Op to compute the sum of a tensor along specified axes.\n",
    "\n",
    "    Example:\n",
    "    >>> a = Tensor([[1, 2, 3], [4, 5, 6]])\n",
    "    >>> op = Summation(axes=(0,))\n",
    "    >>> result = op.compute(a)\n",
    "    >>> print(result)\n",
    "    Tensor([5, 7, 9])\n",
    "\n",
    "    Args:\n",
    "    - axes (tuple, optional): The dimensions to reduce. If `None` (default), reduces all dimensions.\n",
    "\n",
    "    Methods:\n",
    "    - compute(a: NDArray) -> NDArray: Computes the sum of `a` along the specified axes.\n",
    "    - gradient(out_grad: Tensor, node: Tensor) -> Tuple[Tensor]: Computes the gradient of the sum operation.\n",
    "    \"\"\"\n",
    "    def __init__(self, axes: Optional[tuple] = None):\n",
    "        self.axes = axes\n",
    "\n",
    "    def compute(self, a: NDArray) -> NDArray:\n",
    "        \"\"\"\n",
    "        Computes the sum of `a` along the specified axes.\n",
    "\n",
    "        Args:\n",
    "        - a: The input tensor.\n",
    "\n",
    "        Returns:\n",
    "        The sum of `a` along the specified axes.\n",
    "        \"\"\"\n",
    "        return ARRAY_API.sum(a, self.axes)\n",
    "\n",
    "    def gradient(self, out_grad: Tensor, node: Tensor) -> Tuple[Tensor]:\n",
    "        \"\"\"\n",
    "        Computes the gradient of the sum operation.\n",
    "\n",
    "        Args:\n",
    "        - out_grad: The gradient of the output of the operation.\n",
    "        - node: The node in the computational graph where the operation was performed.\n",
    "\n",
    "        Returns:\n",
    "        The gradient with respect to the input.\n",
    "        \"\"\"\n",
    "        # out_grad is the gradient of the output of this operation\n",
    "        # We need to \"undo\" the dimensionality reduction performed in the forward pass\n",
    "        # That's why we create a new shape, replacing the dimensions specified by self.axes with 1\n",
    "\n",
    "        # Initialize new shape to be the same as the input shape\n",
    "        new_shape = list(node.children[0].shape)\n",
    "\n",
    "        # If axes were specified, set those dimensions to 1 in the new shape\n",
    "        if self.axes:\n",
    "            for axis in self.axes: new_shape[axis] = 1\n",
    "            \n",
    "        else:\n",
    "            new_shape = [1] * len(new_shape)\n",
    "\n",
    "        # Reshape out_grad to the new shape\n",
    "        reshaped_grad = reshape(out_grad, new_shape)\n",
    "\n",
    "        # Broadcast the reshaped out_grad to match the input shape\n",
    "        broadcasted_grad = broadcast_to(reshaped_grad, node.children[0].shape)\n",
    "\n",
    "        # The gradient method needs to return a tuple, even though there's only one input\n",
    "        return (broadcasted_grad,)\n",
    "\n",
    "\n",
    "def summation(a: Tensor, axes: Optional[tuple] = None) -> Tensor:\n",
    "    \"\"\"\n",
    "    Computes the sum of `a` along the specified axes.\n",
    "\n",
    "    Args:\n",
    "    - a: The input tensor.\n",
    "    - axes (tuple, optional): The dimensions to reduce. If `None` (default), reduces all dimensions.\n",
    "\n",
    "    Returns:\n",
    "    The sum of `a` along the specified axes.\n",
    "    \"\"\"\n",
    "    return Summation(axes)(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8a0370-6991-4a02-93d4-4401fd33056c",
   "metadata": {},
   "source": [
    "## Broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daebd380-4add-4c43-be53-d89add758e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we create a tensor a, and set requires_grad = True so that we can compute gradients with respect to it\n",
    "a = torch.tensor([1., 2., 3.], requires_grad=True)\n",
    "\n",
    "# Now, let's define a function that performs the broadcasting operation\n",
    "def broadcast_to(input, shape):\n",
    "    return input.expand(shape)\n",
    "\n",
    "# We broadcast a to a larger shape\n",
    "shape = (3, 3)\n",
    "b = broadcast_to(a, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc455d1-a34b-4edc-abd5-72adc86fa13b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [1., 2., 3.],\n",
       "        [1., 2., 3.]], grad_fn=<ExpandBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ae4881-226b-42c5-bbf8-678b83f74b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b1375d-af00-4e15-a7e6-f6bcbe259bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, we define an output tensor as the sum of elements in b\n",
    "# This is a simple function that we can differentiate, and will result in a gradient for b\n",
    "out = b.sum()\n",
    "\n",
    "# Compute gradients\n",
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd44d852-e580-49ef-93a5-3f7a61be32d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 3., 3.])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f51faf-819b-4937-94c7-5817b1fa8316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the output gradient tensor\n",
    "out_grad = torch.tensor([[1., 2., 3.], [1., 2., 3.], [1., 2., 3.]])\n",
    "out_grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5f46a8-4fcf-4c53-9e8e-5484a4b4f789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_shape = a.shape\n",
    "a_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069bcef5-d269-4625-b957-dfbc1555abb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = [1] * (len((3,3)) - len((3,3))) + list(a_shape)\n",
    "shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafece31-f1e6-451a-9e22-3a4ad6685e6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd40102-9f65-446c-903e-d290ef489289",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946d4abc-1bf7-4a87-9e73-b6c2e5ac2c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 6., 9.])\n"
     ]
    }
   ],
   "source": [
    "# The gradient for the broadcast operation is the sum of out_grad over the dimension that was broadcasted\n",
    "grad_a = out_grad.sum(dim=0)\n",
    "\n",
    "print(grad_a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b171ea64-f412-4f37-8054-495aad78c775",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BroadcastTo(TensorOp):\n",
    "    \"\"\"\n",
    "    Op to broadcast a tensor to a new shape.\n",
    "\n",
    "    Example:\n",
    "    >>> a = Tensor([1, 2, 3])\n",
    "    >>> op = BroadcastTo((3, 3))\n",
    "    >>> result = op.compute(a)\n",
    "    >>> print(result)\n",
    "    Tensor([[1, 2, 3], [1, 2, 3], [1, 2, 3]])\n",
    "\n",
    "    Args:\n",
    "    - shape (tuple): The new shape to broadcast the input tensor to.\n",
    "\n",
    "    Methods:\n",
    "    - compute(a: NDArray) -> NDArray: Broadcasts `a` to the specified shape.\n",
    "    - gradient(out_grad: Tensor, node: Tensor) -> Tuple[Tensor]: Computes the gradient of the broadcast operation.\n",
    "    \"\"\"\n",
    "    def __init__(self, shape):\n",
    "        self.shape = shape\n",
    "\n",
    "    def compute(self, a: NDArray) -> NDArray:\n",
    "        \"\"\"\n",
    "        Broadcasts `a` to the specified shape.\n",
    "\n",
    "        Args:\n",
    "        - a: The input tensor.\n",
    "\n",
    "        Returns:\n",
    "        The tensor `a` broadcasted to the specified shape.\n",
    "        \"\"\"\n",
    "        return ARRAY_API.broadcast_to(a, self.shape)\n",
    "\n",
    "    def gradient(self, out_grad: Tensor, node: Tensor) -> Tuple[Tensor]:\n",
    "        \"\"\"\n",
    "        Computes the gradient of the broadcast operation.\n",
    "\n",
    "        Args:\n",
    "        - out_grad: The gradient of the output of the operation.\n",
    "        - node: The node in the computational graph where the operation was performed.\n",
    "\n",
    "        Returns:\n",
    "        The gradient with respect to the input.\n",
    "        \"\"\"\n",
    "        # First, we need to create a shape that matches the shape of `a` but with ones \n",
    "        # prepended to match the length of `self.shape`.\n",
    "        a_shape = node.children[0].shape\n",
    "        shape = [1] * (len(self.shape) - len(a_shape)) + list(a_shape)\n",
    "\n",
    "        # Then, we identify the dimensions along which to sum in the backward pass. \n",
    "        # These are the dimensions that were expanded during the broadcast.\n",
    "        sum_over = tuple([idx for idx in range(len(self.shape)) if self.shape[idx] != shape[idx]])\n",
    "\n",
    "        # Finally, we reshape the gradient after summing over the appropriate dimensions to match `a`'s shape.\n",
    "        return reshape(summation(out_grad, sum_over), a_shape)\n",
    "\n",
    "def broadcast_to(a: Tensor, shape: Tuple[int, ...]) -> Tensor:\n",
    "    \"\"\"\n",
    "    Broadcasts `a` to the specified shape.\n",
    "\n",
    "    Args:\n",
    "    - a: The input tensor.\n",
    "    - shape: The new shape to broadcast the input tensor to.\n",
    "\n",
    "    Returns:\n",
    "    The tensor `a` broadcasted to the specified shape.\n",
    "    \"\"\"\n",
    "    return BroadcastTo(shape)(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aeff78-9357-41f8-a495-9c433a8776ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "br = BroadcastTo((5,2,3))\n",
    "a = Tensor([[1., 2., 3.], [1., 2., 3.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3761138-0a81-4a85-b966-04708310479c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87adc1bb-0b23-466f-b03c-de92b35e5407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 2, 3)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_br = br.compute(a)\n",
    "a_br.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e4c955-9b3f-4bbd-a2c3-e6ee06db40dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 2, 3)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_grad = Tensor(numpy.ones_like(a_br))\n",
    "out_grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240e5580-3a55-461e-9b0e-fd5f5647b3c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "minima.Tensor([[[1 1 1]\n",
       "  [1 1 1]]\n",
       "\n",
       " [[1 1 1]\n",
       "  [1 1 1]]\n",
       "\n",
       " [[1 1 1]\n",
       "  [1 1 1]]\n",
       "\n",
       " [[1 1 1]\n",
       "  [1 1 1]]\n",
       "\n",
       " [[1 1 1]\n",
       "  [1 1 1]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c1c27b-b01d-4563-8483-f15dc370f8f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_shape = a.shape\n",
    "a_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b434972-fe09-449f-a7de-e4648323b5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = [1] * (len(br.shape) - len(a_shape)) + list(a_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0bb6a2-effd-4d41-8038-dcc46d7b3efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 2, 3), [1, 2, 3])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "br.shape, shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835d2f72-61a7-4fb9-b94b-2dcba608c028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_over = tuple([idx for idx in range(len(br.shape)) if br.shape[idx] != shape[idx]])\n",
    "sum_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf744e8-42b4-49cd-bb21-d38a4931b917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshape(summation(out_grad, sum_over), a_shape).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02702d84-12d2-4933-9663-b14228df422c",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc235f7-c1f4-42b5-b3a1-d9cb909617d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
