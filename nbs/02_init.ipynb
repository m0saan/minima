{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27fbd750-b4bb-42ae-9dd8-0003b190a125",
   "metadata": {},
   "source": [
    "# init\n",
    "\n",
    "> The `init` module in the `minima` (mi) library provides a suite of tensor initialization functions to create and initialize tensors in various ways. Each function in this module represents a different strategy for initializing the values of a tensor, such as uniform or normal random values, constant values, or specialized initializations like Xavier or Kaiming methods.\n",
    "\n",
    "> These initialization methods serve as the starting point for the optimization process in neural networks, setting the stage for gradient descent and other optimization methods to fine-tune the model's parameters during training. Carefully chosen initial values can significantly influence the training dynamics and the final performance of a model.\n",
    "The `init` module is a critical part of the deep learning pipeline, providing the essential first step in the process of training a neural network. It ensures a smooth and effective transition from model definition to the iterative process of learning from data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7357fd7b-1a97-4472-9513-b88c591e00cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f157480-9c08-42ae-a859-012d60db3f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import math\n",
    "import minima as mi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974276f8-0ba7-49fb-94e6-d2adae691c36",
   "metadata": {},
   "source": [
    "1. **`rand`**: This function generates a tensor filled with random numbers drawn from a uniform distribution between `low` and `high` (defaulting to 0 and 1). It does this by creating an array of random values on the specified device (defaulting to CPU), then scales and shifts these values to the correct range. The result is wrapped in a `mi.Tensor` object, which supports automatic differentiation if `requires_grad` is True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538189b4-cb31-4348-9af4-6186274a5036",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def rand(\n",
    "    *shape, # The shape of the output tensor. Variable length argument list. \n",
    "    low=0.0, # Lower bound of the uniform distribution. Default is 0.0.\n",
    "    high=1.0, # Upper bound of the uniform distribution. Default is 1.0.\n",
    "    device=None, # The device where the tensor will be allocated. Default is CPU.\n",
    "    dtype='float32', # The data type of the tensor. Default is 'float32'.\n",
    "    requires_grad=False # If True, the tensor is created with gradient tracking. Default is False.\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates a tensor with random numbers uniformly distributed between `low` and `high`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    *shape : int\n",
    "    low : float, optional\n",
    "    high : float, optional\n",
    "    device : Device, optional\n",
    "    dtype : str, optional\n",
    "    requires_grad : bool, optional\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mi.Tensor\n",
    "        A tensor of shape `shape`, filled with random numbers from the uniform distribution between `low` and `high`.\n",
    "\n",
    "    \"\"\"\n",
    "    device = mi.cpu() if device is None else device\n",
    "    array = device.rand(*shape) * (high - low) + low\n",
    "    return mi.Tensor(array, device=device, dtype=dtype, requires_grad=requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b68976-7696-43bf-b0ca-f7935eda9331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "minima.Tensor(\n",
       "[[0.724743 0.236047 0.065364 0.855749 0.91864 ]\n",
       " [0.027286 0.800982 0.64705  0.968984 0.489134]\n",
       " [0.931952 0.758258 0.887548 0.867378 0.108525]\n",
       " [0.03819  0.143269 0.210976 0.892318 0.069396]\n",
       " [0.257424 0.554249 0.235325 0.064803 0.843057]\n",
       " [0.696038 0.812699 0.54037  0.754445 0.385663]\n",
       " [0.461943 0.538387 0.582451 0.802216 0.6077  ]\n",
       " [0.045212 0.726626 0.886866 0.190699 0.00549 ]\n",
       " [0.685753 0.342417 0.554111 0.813416 0.375196]\n",
       " [0.170601 0.631679 0.474656 0.363225 0.162466]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand(10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b739cc-e625-4160-ae41-022effe3c565",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = rand(10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc7790c-bd97-454b-8bb0-a1fdc94f7144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('float32'), minima.cpu(), False)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.dtype, t.device, t.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fec7bc6-86c7-436a-808f-35de5f94509d",
   "metadata": {},
   "source": [
    "2. **`randn`**: Similar to `rand`, but generates numbers from a normal distribution with the specified mean and standard deviation (defaulting to 0 and 1). This is done by creating an array of normally-distributed random values, then scaling and shifting them to match the requested parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9bee34-4205-44f8-86d9-fd7d7b33c39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def randn(\n",
    "    *shape, # The shape of the output tensor. Variable length argument list.\n",
    "    mean=0.0,# Mean of the normal distribution. Default is 0.0.\n",
    "    std=1.0, # Standard deviation of the normal distribution. Default is 1.0.\n",
    "    device=None,# The device where the tensor will be allocated. Default is CPU.\n",
    "    dtype=\"float32\",# The data type of the tensor. Default is 'float32'.\n",
    "    requires_grad=False # If True, the tensor is created with gradient tracking. Default is False.\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates a tensor with random numbers normally distributed with specified mean and standard deviation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    *shape : int\n",
    "    mean : float, optional\n",
    "    std : float, optional\n",
    "    device : Device, optional\n",
    "    dtype : str, optional\n",
    "    requires_grad : bool, optional\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mi.Tensor\n",
    "        A tensor of shape `shape`, filled with random numbers from the normal distribution with the specified mean and standard deviation.\n",
    "    \"\"\"\n",
    "    device = mi.cpu() if device is None else device\n",
    "    array = device.randn(*shape) * std + mean\n",
    "    return mi.Tensor(array, device=device, dtype=dtype, requires_grad=requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718fc7c1-1628-4368-8e9b-d0cf22e9c00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = randn(5,5, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69baef10-0dfe-43d8-b999-9e58ac9d619f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "minima.Tensor(\n",
       "[[ 0.439627 -0.765065 -0.323372 -1.518769 -0.563107]\n",
       " [-0.255756  0.96025   1.512503 -0.662302 -1.201184]\n",
       " [ 0.650412  0.263193  1.310423  1.383127  1.237785]\n",
       " [-0.008076  0.028429  1.874965  0.977454 -0.068408]\n",
       " [-1.75604   0.546302  0.359429  0.864159  1.347796]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010d6791-37d8-4b0f-addc-8947697af949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 5), dtype('float32'), minima.cpu(), True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape, t.dtype, t.device, t.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d1a3e9-0427-49bc-96cc-bea58ea00f05",
   "metadata": {},
   "source": [
    "3. **`constant`**: This function creates a tensor filled with a constant value `c` (defaulting to 1). It does this by creating an array of ones on the specified device and then scaling these ones by the constant value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda1969b-50f2-41cf-996d-119a4fc10ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def constant(\n",
    "    *shape, # The shape of the output tensor. Variable length argument list.\n",
    "    c=1.0, # The constant value to fill the tensor with. Default is 1.0.\n",
    "    device=None, # The device where the tensor will be allocated. Default is CPU.\n",
    "    dtype=\"float32\", # The data type of the tensor. Default is 'float32'.\n",
    "    requires_grad=False # If True, the tensor is created with gradient tracking. Default is False.\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates a tensor filled with a constant value.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    *shape : int\n",
    "    c : float, optional\n",
    "    device : Device, optional\n",
    "    dtype : str, optional\n",
    "    requires_grad : bool, optional\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mi.Tensor\n",
    "        A tensor of shape `shape`, filled with the constant value `c`.\n",
    "    \"\"\"\n",
    "    device = mi.cpu() if device is None else device\n",
    "    array = device.ones(*shape, dtype=dtype) * c # note: can change dtype\n",
    "    return mi.Tensor(array, device=device, dtype=dtype, requires_grad=requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2066880d-8dd1-44fc-a734-23384d48f511",
   "metadata": {},
   "source": [
    "4. **`ones` and `zeros`**: These functions are simply shortcuts for creating tensors filled with ones or zeros, respectively. They're implemented by calling the `constant` function with `c` set to 1 or 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce69cd16-b102-4719-adc2-2e3384e465e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def ones(\n",
    "    *shape, # The shape of the output tensor. Variable length argument list.\n",
    "    device=None, # The device where the tensor will be allocated. Default is CPU.\n",
    "    dtype=\"float32\", # The data type of the tensor. Default is 'float32'.\n",
    "    requires_grad=False # If True, the tensor is created with gradient tracking. Default is False.\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates a tensor filled with ones.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    *shape : int\n",
    "    device : Device, optional\n",
    "    dtype : str, optional\n",
    "    requires_grad : bool, optional\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mi.Tensor\n",
    "        A tensor of shape `shape`, filled with ones.\n",
    "    \"\"\"\n",
    "    return constant(*shape, c=1.0, device=device, dtype=dtype, requires_grad=requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1dbe54-c223-4a98-b5ba-5047643303f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def zeros(\n",
    "    *shape, # The shape of the output tensor. Variable length argument list.\n",
    "    device=None, # The device where the tensor will be allocated. Default is CPU.\n",
    "    dtype=\"float32\", # The data type of the tensor. Default is 'float32'.\n",
    "    requires_grad=False # If True, the tensor is created with gradient tracking. Default is False.\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates a tensor filled with zeros.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    *shape : int\n",
    "    device : Device, optional\n",
    "    dtype : str, optional\n",
    "    requires_grad : bool, optional\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mi.Tensor\n",
    "        A tensor of shape `shape`, filled with zeros.\n",
    "    \"\"\"\n",
    "    return constant(*shape, c=0.0, device=device, dtype=dtype, requires_grad=requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cedbc9-465b-4cf0-9808-73d7a85b939f",
   "metadata": {},
   "source": [
    "5. **`randb`**: This function creates a binary tensor, with each element independently being True with probability `p` (defaulting to 0.5). This is done by generating uniformly-distributed random numbers and checking whether they're less than or equal to `p`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0530b009-bf8c-4041-8996-bae788916f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def randb(\n",
    "    *shape, # The shape of the output tensor. Variable length argument list.\n",
    "    p=0.5, # The probability of generating a `True` (1) in the binary tensor. Default is 0.5.\n",
    "    device=None, # The device where the tensor will be allocated. Default is CPU.\n",
    "    dtype=\"bool\", # The data type of the tensor. Default is 'bool'.\n",
    "    requires_grad=False # If True, the tensor is created with gradient tracking. Default is False.\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates a binary tensor with random values of `True` or `False`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    *shape : int\n",
    "    p : float, optional\n",
    "    device : Device, optional\n",
    "    dtype : str, optional\n",
    "    requires_grad : bool, optional\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mi.Tensor\n",
    "        A binary tensor of shape `shape`, filled with random boolean values, where the probability of `True` is `p`.\n",
    "    \"\"\"\n",
    "    device = mi.cpu() if device is None else device\n",
    "    array = device.rand(*shape) <= p\n",
    "    return mi.Tensor(array, device=device, dtype=dtype, requires_grad=requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65449aee-3c32-4e7e-b05f-e168788aee75",
   "metadata": {},
   "source": [
    "6. **`one_hot`**: This function creates a one-hot encoding tensor. Given a size `n` and an index `i`, it creates a tensor of size `n` with a 1 at the `i`-th position and 0s elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d368ba-89f5-4628-bf6b-30cff26d2a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def one_hot(\n",
    "    n, # The size of the one-hot vector.\n",
    "    i, # The index to be set to `1` in the one-hot vector.\n",
    "    device=None, # The device where the tensor will be allocated. Default is CPU.\n",
    "    dtype=\"float32\", # The data type of the tensor. Default is 'float32'.\n",
    "    requires_grad=False # If True, the tensor is created with gradient tracking. Default is False.\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates a one-hot encoding tensor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int\n",
    "    i : int\n",
    "    device : Device, optional\n",
    "    dtype : str, optional\n",
    "    requires_grad : bool, optional\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mi.Tensor\n",
    "        A one-hot tensor of size `n`, with the `i`th element set to `1` and all others set to `0`.\n",
    "    \"\"\"\n",
    "    device = mi.cpu() if device is None else device\n",
    "    return mi.Tensor(device.one_hot(n,i.numpy(), dtype=dtype), device=device, requires_grad=requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8068d9d1-0d91-4988-a473-2bb2280c73c6",
   "metadata": {},
   "source": [
    "### Glorot/Xavier Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ae412a-f9ba-4759-b218-68b10474de14",
   "metadata": {},
   "source": [
    "Xavier initialization, also known as Glorot initialization, is a technique for initializing the weights in artificial neural networks to improve the stability and speed of neural network training. In the paper Understanding the difficulty of training deep feedforward neural networks, researchers identified a value for the variance of the weights that works well to mitigate the problems we've discussed.\n",
    "\n",
    "Here's a high-level idea of how it works:\n",
    "\n",
    "Neural networks are trained using a method called backpropagation, which involves iteratively adjusting the weights of the network based on the difference between the network's current output and its desired output.\n",
    "\n",
    "One challenge with this process is that the scale of the initial weights can have a large impact on the network's learning dynamics. If the weights are too large or too small, the network might learn very slowly, or not at all. This is particularly an issue in deep networks where there are many layers of weights to learn.\n",
    "\n",
    "Xavier initialization seeks to address this issue by scaling the initial weights in proportion to the number of inputs and outputs of the neuron. Specifically, in Xavier initialization, the weights are drawn from a distribution with zero a mean of 0 and a variance defined as: \n",
    "\n",
    "$$\n",
    "\\text{var}(w)=\\frac{2}{n_{in}+n_{out}}\n",
    "$$\n",
    "\n",
    "where $n_{in}$ is the number of inputs to the neuron and $n_{out}$ is the number of outputs. In order to induce the weights to acquire a standard deviation of $\\sqrt{\\frac{2}{n_{in}+n_{out}}}$, consequently causing a variance of $\\frac{2}{n_{in}+n_{out}}$, the weights are initially produced randomly from a normal distribution with a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "Subsequently, every weight is multiplied by $\\sqrt{\\frac{2}{n_{in}+n_{out}}}$, effectively shifting the standard deviation of the distribution to $\\sqrt{\\frac{2}{n_{in}+n_{out}}}$.\n",
    "\n",
    "![Xavier initialization from a normal distribution](../assets/10.xav-init-normal.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0860fb-8b2b-46d6-9ab7-b41c242e5985",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def xavier_normal(\n",
    "    fan_in, # The number of input units in the weight tensor.\n",
    "    fan_out, # The number of output units in the weight tensor.\n",
    "    gain=1.0, # Scaling factor for the standard deviation of the normal distribution. Default is 1.0.\n",
    "    **kwargs # Additional arguments.\n",
    "):\n",
    "    \"\"\"\n",
    "    Initializes a tensor using Xavier (Glorot) Normal initialization.\n",
    "\n",
    "    This initializer is designed to keep the scale of the gradients roughly the same\n",
    "    in all layers. It samples weights from a normal distribution centered around 0 with \n",
    "    standard deviation `gain * sqrt(2 / (fan_in + fan_out))`\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fan_in : int\n",
    "        The number of input units in the weight tensor.\n",
    "    fan_out : int\n",
    "        The number of output units in the weight tensor.\n",
    "    gain : float, optional\n",
    "        Scaling factor for the standard deviation of the normal distribution. Default is 1.0.\n",
    "    **kwargs\n",
    "        Additional arguments.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mi.Tensor\n",
    "        A tensor initialized using Xavier Normal initialization.\n",
    "    \"\"\"\n",
    "    std = gain * math.sqrt(2 / (fan_in + fan_out))\n",
    "    return randn(fan_in, fan_out) * std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dbc42b-b13f-4168-9f84-6204076ab210",
   "metadata": {},
   "source": [
    "It's worth noting that there is also a Xavier initialization variant suitable for uniform distributions as opposed to normal distributions. The resultant weight matrix will comprise values sampled from a uniform distribution within the scope of $(-a, a)$, with $a$ equalling $\\sqrt{\\frac{6}{n_{in}+n_{out}}}$.\n",
    "\n",
    "![Xavier initialization from a uniform distribution](../assets/11.xav-uniform.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d8f7a5-1ec1-424e-801b-9542e2a5261a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def xavier_uniform(\n",
    "    fan_in, # The number of input units in the weight tensor.\n",
    "    fan_out, # The number of output units in the weight tensor.\n",
    "    gain=1.0, # Scaling factor for the range of the uniform distribution. Default is 1.0.\n",
    "    **kwargs # Additional arguments.\n",
    "):\n",
    "    \"\"\"\n",
    "    Initializes a tensor using Xavier (Glorot) Uniform initialization.\n",
    "\n",
    "    This initializer is designed to keep the scale of the gradients roughly the same\n",
    "    in all layers. It samples weights from a uniform distribution within the range \n",
    "    `[-gain * sqrt(6 / (fan_in + fan_out)), gain * sqrt(6 / (fan_in + fan_out))]`\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fan_in : int\n",
    "        The number of input units in the weight tensor.\n",
    "    fan_out : int\n",
    "        The number of output units in the weight tensor.\n",
    "    gain : float, optional\n",
    "        Scaling factor for the range of the uniform distribution. Default is 1.0.\n",
    "    **kwargs\n",
    "        Additional arguments.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mi.Tensor\n",
    "        A tensor initialized using Xavier Uniform initialization.\n",
    "    \"\"\"\n",
    "    a = gain * math.sqrt(6 / (fan_in + fan_out))\n",
    "    return rand(fan_in, fan_out, low=-a, high=a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c2b0a4-ae23-422f-a69d-1b2e61b3577f",
   "metadata": {},
   "source": [
    "Both normal and uniform distributions have demonstrated effectiveness in practical applications, and it is up to the network designer to select the preferred method. Xavier initialization is frequently utilized in practical scenarios to promote more stable training and circumvent issues that stem from unstable gradients, such as the vanishing and exploding gradient predicaments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb04de89-8310-4c26-83d3-dbc3a3869e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights with Xavier/Glorot initialization\n",
    "W = xavier_uniform(fan_in=10, fan_out=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43d2073-07e0-4e88-8f9c-2ff7563c1f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "minima.Tensor(\n",
       "[[-0.362476  0.425931 -0.604767 -0.585322 -0.469232]\n",
       " [-0.207212 -0.106902 -0.349397 -0.082876  0.145293]\n",
       " [ 0.274891 -0.136795 -0.210589  0.275138  0.481523]\n",
       " [ 0.258619  0.307068  0.458597  0.236569  0.462849]\n",
       " [ 0.029899 -0.559897  0.514509  0.32062   0.208706]\n",
       " [-0.488839 -0.433476  0.089545  0.466521 -0.407354]\n",
       " [-0.243207  0.266691  0.27616   0.263078 -0.267017]\n",
       " [-0.61643  -0.143201  0.083898  0.366265  0.022065]\n",
       " [ 0.149755 -0.155406 -0.494278  0.481983 -0.509169]\n",
       " [ 0.159343  0.597055 -0.36376   0.376093 -0.399417]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22961e7d-414e-4287-b5be-b6c62c870ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = xavier_normal(fan_in=10, fan_out=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5572ebce-3093-4421-ad12-e37c8260dcc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "minima.Tensor(\n",
       "[[ 0.176985  0.475322 -0.472049 -0.197204 -0.124735]\n",
       " [-0.021712  0.164783 -0.76619  -0.109438  0.25406 ]\n",
       " [ 0.747072  0.355753 -0.609391  0.107684 -0.27346 ]\n",
       " [ 0.418525 -0.260188 -0.422199 -0.179245 -0.042915]\n",
       " [ 0.167997 -0.004962  0.17853  -0.520827  0.349572]\n",
       " [-0.019444 -0.406027  0.332068 -0.591041 -0.408733]\n",
       " [-0.535821 -0.790127 -0.098206  0.25483   0.509668]\n",
       " [-0.401008 -0.051625  0.376056  0.1105   -1.083598]\n",
       " [ 0.06975   0.191631  0.233941  0.101632  0.235047]\n",
       " [-0.034112 -0.465086  0.053479  0.444051 -0.654049]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30772e1d-bd7b-4200-bf2b-30bc439fa653",
   "metadata": {},
   "source": [
    "The original Xavier initialization was designed for use with the sigmoid activation function, which is symmetric around zero. If you're using a different activation function, like ReLU, you might need a different initialization scheme, like He initialization, which is a modification of Xavier initialization designed for ReLU and other non-symmetric activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def6e431-fdc2-4fa8-b4c1-5a3ef5fb7115",
   "metadata": {},
   "source": [
    "### He Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b27217d-311f-47f4-afa3-ca6acc160b38",
   "metadata": {},
   "source": [
    "Kaiming Initialization, also known as He Initialization, is a method used in initializing the weights of Neural Networks. This initialization method is designed specifically for neural networks with Rectified Linear Unit (ReLU) activation functions. It was proposed by Kaiming He et al. in their 2015 paper \"Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\".\n",
    "\n",
    "**Principles of Kaiming Initialization:**\n",
    "\n",
    "The basic idea of Kaiming Initialization is to keep the variance of the input and output of each layer of the neural network as consistent as possible during the forward and backward propagation. This is to solve the problem of gradient dispersion or explosion caused by the deepening of the neural network layer, which can help the model learn effectively.\n",
    "\n",
    "Kaiming initialization initializes a weight matrix $w$ with random values sampled from a normal distribution with mean of $0$ and variance\n",
    "\n",
    "$$\\text{var}(w)=\\frac{2}{n_{i}}$$\n",
    "\n",
    "Here, `n_i` is the number of inputs to the neuron, `w` is the weight vector.\n",
    "\n",
    "Just as with Xavier initialization, to force the weights distribution to take on this variance, the weights ar first randomly generated from a normal distribution with centered around 0 with a standard deviation of 1. Then, each weight is multiplied by \n",
    "\n",
    "$$\\sqrt{\\frac{2}{n_{i}}}$$\n",
    "\n",
    "![Kaiming initialization from a normal distributiont](../assets/12.kaiming-normal.svg)\n",
    "\n",
    "where `n` is the number of inputs coming into a neuron (also known as the \"fan-in\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c580a1eb-8321-4dac-8fa1-f09cf7012fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def kaiming_normal(\n",
    "    fan_in,  # Number of input units in the weight tensor.\n",
    "    fan_out, # Number of output units in the weight tensor.\n",
    "    nonlinearity=\"relu\", # The non-linear function (`nn.functional` name), recommended to use only with 'relu' or 'leaky_relu'. Default is 'relu'.\n",
    "    **kwargs # Additional keyword arguments\n",
    "):\n",
    "    \"\"\"\n",
    "    Fills the input Tensor with values according to the method described in\n",
    "    \"Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification\" - He, K. et al. (2015), using a normal distribution.\n",
    "    The resulting tensor will have values sampled from normal distribution with mean=0 and std=sqrt(2 / fan_in).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fan_in : int\n",
    "        Number of input units in the weight tensor.\n",
    "    fan_out : int\n",
    "        Number of output units in the weight tensor.\n",
    "    nonlinearity : str, optional\n",
    "        The non-linear function (`nn.functional` name), recommended to use only with 'relu' or 'leaky_relu'. Default is 'relu'.\n",
    "    **kwargs : optional\n",
    "        Additional keyword arguments.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mi.Tensor\n",
    "        A tensor of shape (fan_in, fan_out), filled with random numbers from the normal distribution according to the Kaiming initialization.\n",
    "    \"\"\"\n",
    "    assert nonlinearity == \"relu\", \"Only relu supported currently\"\n",
    "    std = np.sqrt(2) / np.sqrt(fan_in)\n",
    "    return randn(fan_in, fan_out) * std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b561b40-a3a0-4419-8af8-3c9ce4c446ed",
   "metadata": {},
   "source": [
    "\n",
    "There is also a version of Kaiming initialization to use for uniform distributions rather than normal distributions. The resulting weight matrix will have values sampled from a uniform distribution within the range $(-a, a)$, where \n",
    "\n",
    "$$a = \\sqrt{\\frac{6}{n_{i}}}$$\n",
    "\n",
    "![Kaiming initialization from a uniform distributiont](../assets/13.kaiming-uniform.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62698296-6b61-4aa8-9238-46cd3a5bc465",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def kaiming_uniform(\n",
    "    fan_in,  # Number of input units in the weight tensor.\n",
    "    fan_out, # Number of output units in the weight tensor.\n",
    "    nonlinearity=\"relu\", # The non-linear function (`nn.functional` name), recommended to use only with 'relu' or 'leaky_relu'. Default is 'relu'.\n",
    "    **kwargs # Additional keyword arguments\n",
    "):\n",
    "    \"\"\"\n",
    "    Fills the input Tensor with values according to the method described in\n",
    "    \"Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification\" - He, K. et al. (2015), using a uniform distribution.\n",
    "    The resulting tensor will have values sampled from uniform distribution in the range [-std, std] where std = sqrt(2 / fan_in).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fan_in : int\n",
    "        Number of input units in the weight tensor.\n",
    "    fan_out : int\n",
    "        Number of output units in the weight tensor.\n",
    "    nonlinearity : str, optional\n",
    "        The non-linear function (`nn.functional` name), recommended to use only with 'relu' or 'leaky_relu'. Default is 'relu'.\n",
    "    **kwargs : optional\n",
    "        Additional keyword arguments.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mi.Tensor\n",
    "        A tensor of shape (fan_in, fan_out), filled with random numbers from the uniform distribution according to the Kaiming initialization.\n",
    "    \"\"\"\n",
    "    assert nonlinearity == \"relu\", \"Only relu supported currently\"\n",
    "    gain = math.sqrt(2)\n",
    "    std = gain * math.sqrt(3/fan_in)\n",
    "    return rand(fan_in, fan_out, low=-std, high=std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f74655c-a66d-471f-8178-4b04b0ee69ad",
   "metadata": {},
   "source": [
    "**Advantages of Kaiming Initialization:**\n",
    "\n",
    "1. It helps to keep the variance of the gradients roughly the same across all layers. This ensures that all layers in the network learn at about the same speed, avoiding the saturation of activation functions, and it can also help speed up the convergence of the network.\n",
    "2. It performs better with ReLU and its variants because it accounts for the fact that the variance of the output of a neuron with a ReLU activation function is half the variance of its input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db18cd9-190f-4103-9866-6e7fe7f4863a",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a1dfee-dfdd-402c-ba30-997ddeab9776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
