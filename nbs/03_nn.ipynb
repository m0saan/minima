{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dda21224-965c-4e74-b9b5-e58d8dd9e0a5",
   "metadata": {},
   "source": [
    "# nn\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263e352d-55d5-464a-80fd-f98c03b56d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02044ec-9480-46c2-9441-bf7ca89bdcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import List, Callable, Any\n",
    "from minima.autograd import Tensor\n",
    "from minima import operators\n",
    "import minima.init as init\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a16df1f-71e2-4b36-80b7-f59581c7dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Parameter(Tensor):\n",
    "    \"\"\"\n",
    "    A kind of Tensor that is to be considered a module parameter.\n",
    "\n",
    "    Parameters are `Tensor` subclasses, that have a very special property when used with\n",
    "    `Module` s - when they're assigned as Module attributes they are automatically added\n",
    "    to the list of its parameters, and will appear in `Module.parameters()` iterator.\n",
    "    Another difference is that parameters can't be volatile and that they require gradient by default.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aebab21-ed3b-4b2b-aeb3-01c8c7b1c6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _unpack_params(value: object) -> List[Tensor]:\n",
    "    \"\"\"\n",
    "    Unpack parameters from different Python objects.\n",
    "\n",
    "    This function takes an object of type `Parameter`, `Module`, `dict`, `list`, or `tuple` and \n",
    "    recursively extracts any contained `Parameter` instances, returning them as a list. For other \n",
    "    object types, it returns an empty list.\n",
    "\n",
    "    Args:\n",
    "        value (object): The input object which could be of type `Parameter`, `Module`, `dict`, \n",
    "                        `list`, `tuple`, or any other type.\n",
    "\n",
    "    Returns:\n",
    "        List[Tensor]: A list containing all the `Parameter` instances found within the input object.\n",
    "                      If no `Parameter` instances are found, an empty list is returned.\n",
    "                      \n",
    "    Example:\n",
    "        module = nn.Module(...)\n",
    "        params = _unpack_params(module)\n",
    "        print(params)  # Prints list of `Parameter` instances contained in `module`.\n",
    "    \"\"\"\n",
    "    if isinstance(value, Parameter):\n",
    "        return [value]\n",
    "    elif isinstance(value, Module):\n",
    "        return list(value.parameters())\n",
    "    elif isinstance(value, dict):\n",
    "        return [item for v in value.values() for item in _unpack_params(v)]\n",
    "    elif isinstance(value, (list, tuple)):\n",
    "        return [item for v in value for item in _unpack_params(v)]\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0bfa6d-8425-4b16-ac25-838a76f020ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _child_modules(value: object) -> List[\"Module\"]:\n",
    "    \"\"\"\n",
    "    Recursively unpack child modules from different Python objects.\n",
    "\n",
    "    This function takes an object of type `Module`, `dict`, `list`, or `tuple` and \n",
    "    recursively extracts any contained `Module` instances, returning them as a list. \n",
    "    For other object types, it returns an empty list.\n",
    "\n",
    "    Args:\n",
    "        value (object): The input object which could be of type `Module`, `dict`, \n",
    "                        `list`, `tuple`, or any other type.\n",
    "\n",
    "    Returns:\n",
    "        List[Module]: A list containing all the `Module` instances found within \n",
    "                      the input object. If no `Module` instances are found, \n",
    "                      an empty list is returned.\n",
    "\n",
    "    Example:\n",
    "        class MyModule(Module):\n",
    "            def __init__(self):\n",
    "                super().__init__()\n",
    "                self.layer1 = nn.Linear(20, 20)\n",
    "                self.layer2 = nn.Linear(20, 20)\n",
    "        \n",
    "        my_module = MyModule()\n",
    "        children = _child_modules(my_module)\n",
    "        print(children)  # Prints list of `Module` instances contained in `my_module`.\n",
    "    \"\"\"\n",
    "    if isinstance(value, Module):\n",
    "        return [value] + _child_modules(value.__dict__)\n",
    "    elif isinstance(value, dict):\n",
    "        return [item for v in value.values() for item in _child_modules(v)]\n",
    "    elif isinstance(value, (list, tuple)):\n",
    "        return [item for v in value for item in _child_modules(v)]\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159a2cae-078a-4bd3-8a09-cee5af7ba728",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Module:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.training = True\n",
    "        \n",
    "    def parameters(self) -> List[Parameter]:\n",
    "        return _unpack_params(self.__dict__)\n",
    "    \n",
    "    def _children(self) -> List[Parameter]:\n",
    "        return _child_modules(self.__dict__)\n",
    "    \n",
    "    def eval(self):\n",
    "        self.training = False\n",
    "        for m in self._children():\n",
    "            m.training = False\n",
    "            \n",
    "    def train(self):\n",
    "        self.training = True\n",
    "        for m in self._children():\n",
    "            m.training = True\n",
    "            \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6891e812-ee6c-407c-b03b-e91aa2633050",
   "metadata": {},
   "source": [
    "---\n",
    "# minima.nn.Linear\n",
    "\n",
    "The `minima.nn.Linear` class applies a linear transformation to the incoming data.\n",
    "\n",
    "### Method\n",
    "\n",
    "`minima.nn.Linear(in_features, out_features, bias=True, device=None, dtype=\"float32\")`\n",
    "\n",
    "#### Parameters\n",
    "\n",
    "- **in_features** (int): The number of features (size of each input sample) from the input data.\n",
    "\n",
    "- **out_features** (int): The number of features (size of each output sample) that the layer should produce.\n",
    "\n",
    "- **bias** (bool, optional): If set to `False`, the layer will not learn an additive bias. Defaults to `True`.\n",
    "\n",
    "- **device** (Device, optional): The device to run the computations on. If not specified, it will default to the device specified in the global context.\n",
    "\n",
    "- **dtype** (str, optional): The data type of the output. Defaults to `\"float32\"`.\n",
    "\n",
    "#### Returns\n",
    "\n",
    "An instance of the `minima.nn.Linear` layer.\n",
    "\n",
    "### Variables\n",
    "\n",
    "- **weight** (Tensor): The learnable weights of the model, of shape (`in_features`, `out_features`). The values should be initialized using the Kaiming Uniform initialization method with `fan_in = in_features`.\n",
    "\n",
    "- **bias** (Tensor): The learnable bias of the model, of shape (`out_features`). If the `bias` parameter is set to `True`, the values should be initialized using the Kaiming Uniform initialization method with `fan_in = out_features`.\n",
    "\n",
    "### Details\n",
    "\n",
    "The `minima.nn.Linear` class applies a linear transformation to the incoming data using the equation $y = xA^T + b$, where:\n",
    "- $x$ is the input,\n",
    "- $A^T$ is the transpose of the weight matrix,\n",
    "- $b$ is the bias.\n",
    "\n",
    "The input shape is expected to be $(N, H_{in})$ where $H_{in}$ is `in_features` and the output shape will be $(N, H_{out})$ where $H_{out}$ is `out_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4041d0e-b95e-46c6-b0f6-0987cbd9763a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Linear(Module):\n",
    "    \"\"\"\n",
    "    A class representing a fully connected (linear) layer in a neural network.\n",
    "    This class inherits from the `Module` class.\n",
    "\n",
    "    Attributes:\n",
    "        in_features (int): The number of input features.\n",
    "        out_features (int): The number of output features.\n",
    "        device (str): The device to store the Parameters on (defaults to None, which means CPU).\n",
    "        dtype (str): The data type of the Parameters (defaults to 'float32').\n",
    "        weight (Parameter): The weight parameters of the layer.\n",
    "        bias (Parameter): The bias parameters of the layer, or None if bias=False.\n",
    "\n",
    "    Methods:\n",
    "        forward(X: Tensor) -> Tensor: Compute the forward pass of the layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features, bias=True, device=None, dtype=\"float32\"):\n",
    "        \"\"\"\n",
    "        Initialize the layer with given input/output feature sizes and, optionally, bias, device, and dtype.\n",
    "\n",
    "        Args:\n",
    "            in_features (int): The number of input features.\n",
    "            out_features (int): The number of output features.\n",
    "            bias (bool, optional): Whether or not to include a bias term. Default is True.\n",
    "            device (str, optional): The device to store the Parameters on. Default is None, which means CPU.\n",
    "            dtype (str, optional): The data type of the Parameters. Default is 'float32'.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "\n",
    "        self.weight = Parameter(init.kaiming_uniform(fan_in=in_features, fan_out=out_features, device=device, dtype=dtype))\n",
    "        self.bias = (Parameter(init.kaiming_uniform(fan_in=out_features, fan_out=1, device=device, dtype=dtype)).reshape((1, out_features))\n",
    "                     if bias else None)\n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        return f'Linear(in_features={self.in_features}, out_features={self.out_features}, bias={self.bias is not None})'\n",
    "            \n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Compute the forward pass of the layer.\n",
    "\n",
    "        This function applies the linear transformation to the input tensor X, \n",
    "        i.e., performs the matrix multiplication of X and the weight tensor, \n",
    "        and then adds the bias tensor (if bias is not None).\n",
    "\n",
    "        Args:\n",
    "            X (Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The output tensor.\n",
    "        \"\"\"\n",
    "        \n",
    "        out = X @ self.weight\n",
    "        out = out + self.bias.broadcast_to(out.shape) if self.bias else out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2219fc4b-6db4-45c7-953e-ced54ad91044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<__main__.MyModule object>, Linear(in_features=10, out_features=20, bias=True), Linear(in_features=20, out_features=10, bias=True)]\n"
     ]
    }
   ],
   "source": [
    "class MyModule(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = Linear(10, 20)\n",
    "        self.layer2 = Linear(20, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "my_module = MyModule()\n",
    "\n",
    "children = _child_modules(my_module)\n",
    "print(children)  # Prints list of `Module` instances contained in `my_module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd658723-8093-4bea-b70f-1d88e6507c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=10, out_features=5, bias=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Linear(in_features=10, out_features=5, bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51bcf1a-a277-49ab-83e7-531e090df371",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644f7f70-b8aa-44a9-93e8-f319f68123c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
