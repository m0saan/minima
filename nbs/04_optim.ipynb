{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33695328-8cc1-4882-b927-716255e39a27",
   "metadata": {},
   "source": [
    "# optim\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9455d3-bb53-4b0e-8f83-b3c74aea27b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1851e7-c0db-432f-8b7e-07c23612d944",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Optimization module\"\"\"\n",
    "import minima as mi\n",
    "from minima.nn import Parameter\n",
    "from minima.autograd import Tensor\n",
    "from minima import init\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d117a048-d97f-45cb-857e-239ee00bc438",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "\n",
    "    def step(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            p.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39b16d1-e0db-4747-92d5-aa4fd533a212",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    def __init__(self, params, lr=0.01, momentum=0.0, wd=0.0):\n",
    "        super().__init__(params)\n",
    "\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.u = {}\n",
    "        self.wd = wd\n",
    "\n",
    "    def step(self):\n",
    "        for self.idx, p in enumerate(self.params):\n",
    "            self._reg_step(p)\n",
    "            self._opt_step(p)\n",
    "            \n",
    "                \n",
    "    def _opt_step(self, p):\n",
    "        if self.idx not in self.u:\n",
    "            self.u[self.idx] = init.zeros(*p.shape)\n",
    "        self.u[self.idx] = self.momentum * self.u[self.idx] + (1 - self.momentum) * p.grad.data\n",
    "        p.data = p.data - self.lr * self.u[self.idx]\n",
    "\n",
    "    def _reg_step(self, p):\n",
    "        if self.wd != 0:\n",
    "            p.data *= (1 - self.lr * self.wd)\n",
    "        # all same :3\n",
    "        # p.data *= (1 - self.lr * self.weight_decay)\n",
    "        # p.data = p.data - self.lr * self.weight_decay * p.data\n",
    "        # p.data -= self.lr * self.weight_decay * p.data\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            p.grad = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7aa00356-7cb1-4aa3-9bf6-70e5967cd2a3",
   "metadata": {},
   "source": [
    "## Adam Optimizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54a810ab-455b-41a8-bba0-edb974d39f4d",
   "metadata": {},
   "source": [
    "This is a [PyTorch](https://pytorch.org) implementation of popular optimizer *Adam* from paper\n",
    " [Adam: A Method for Stochastic Optimization](https://papers.labml.ai/paper/1412.6980).\n",
    "\n",
    "*Adam* update is,\n",
    "$$\n",
    "\\begin{align}\n",
    "m_t &\\leftarrow \\beta_1 m_{t-1} + (1 - \\beta_1) \\cdot g_t \\\\\n",
    "v_t &\\leftarrow \\beta_2 v_{t-1} + (1 - \\beta_2) \\cdot g_t^2 \\\\\n",
    "\\hat{m}_t &\\leftarrow \\frac{m_t}{1-\\beta_1^t} \\\\\n",
    "\\hat{v}_t &\\leftarrow \\frac{v_t}{1-\\beta_2^t} \\\\\n",
    "\\theta_t &\\leftarrow \\theta_{t-1} - \\alpha \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\alpha$, $\\beta_1$, $\\beta_2$ and $\\epsilon$ are scalar hyper parameters.\n",
    "$m_t$ and $v_t$ are first and second order moments.\n",
    "$\\hat{m}_t$  and $\\hat{v}_t$ are biased corrected moments.\n",
    "$\\epsilon$ is used as a fix for division by zero error, but also acts as a form of a hyper-parameter\n",
    "that acts against variance in gradients.\n",
    "\n",
    "Effective step taken assuming $\\epsilon = 0$ is,\n",
    "$$\\Delta t = \\alpha \\cdot \\frac{\\hat{m}_t}{\\hat{v}_t}$$\n",
    "This is bounded by,\n",
    "$$\\vert \\Delta t \\vert \\le \\alpha \\cdot \\frac{1 - \\beta_1}{\\sqrt{1-\\beta_2}}$$\n",
    "when $1-\\beta_1 \\gt \\sqrt{1-\\beta_2}$\n",
    "and\n",
    "$$\\vert \\Delta t\\vert  \\le \\alpha$$\n",
    "otherwise.\n",
    "And in most common scenarios,\n",
    "$$\\vert \\Delta t \\vert \\approx \\alpha$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034f1bd2-baaa-40ae-b334-47c508f84e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam(Optimizer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        params, # `params` is the list of parameters\n",
    "        lr=0.01, # `lr` is the learning rate $\\alpha$\n",
    "        beta1=0.9, #\n",
    "        beta2=0.999, #\n",
    "        eps=1e-8, # `eps` is $\\hat{\\epsilon}$ or $\\epsilon$ based on `optimized_update`\n",
    "        weight_decay=0.0, # is an instance of class `WeightDecay` defined in [`__init__.py`](index.html)\n",
    "    ):\n",
    "        super().__init__(params)\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        self.wd = weight_decay\n",
    "        self.t = 0\n",
    "\n",
    "        self.exp_avg = {}\n",
    "        self.exp_avg_sq = {}\n",
    "\n",
    "    def step(self):\n",
    "        for self.idx, p in enumerate(self.params):\n",
    "            self._reg_step(p)\n",
    "            self._opt_step(p)\n",
    "            \n",
    "                \n",
    "    def _opt_step(self, p):\n",
    "        if self.idx not in self.exp_avg:\n",
    "            self.exp_avg[self.idx] = init.zeros(*p.shape)\n",
    "            self.exp_avg_sq[self.idx] = init.zeros(*p.shape)\n",
    "        \n",
    "        # Update biased first and second moment estimates\n",
    "        self.exp_avg[self.idx] = self.beta1 * self.exp_avg[self.idx] + (1 - self.beta1) * p.grad.data\n",
    "        self.exp_avg_sq[self.idx] = self.beta2 * self.exp_avg_sq[self.idx] + (1 - self.beta2) * p.grad.data**2\n",
    "        \n",
    "        # Compute bias-corrected first and second moment estimates\n",
    "        exp_avg_hat = self.exp_avg[self.idx] / (1 - self.beta1 ** (self.idx + 1))\n",
    "        exp_avg_sq_hat = self.exp_avg_sq[self.idx] / (1 - self.beta2 ** (self.idx + 1))\n",
    "        p.data = p.data - self.lr * exp_avg_hat / (exp_avg_sq_hat ** 0.5 + self.eps)\n",
    "\n",
    "    def _reg_step(self, p):\n",
    "        if self.wd != 0:\n",
    "            p.data *= (1 - self.lr * self.wd)\n",
    "        # all same :3\n",
    "        # p.data *= (1 - self.lr * self.weight_decay)\n",
    "        # p.data = p.data - self.lr * self.weight_decay * p.data\n",
    "        # p.data -= self.lr * self.weight_decay * p.data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9ea53c25-e2b7-47a0-ac9d-161213193c18",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c837fe1b-86db-439a-ba29-1567cc5699e1",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2943be0-3cdf-4622-9421-9b1405b18bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae20858-2009-470d-a863-ac0bd45ed6ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
