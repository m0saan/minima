{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33695328-8cc1-4882-b927-716255e39a27",
   "metadata": {},
   "source": [
    "# optim\n",
    "\n",
    "> The `Optim` module in minima is a flexible and powerful toolbox for optimizing the parameters of your deep learning models. Built on a high-level,  \n",
    "> intuitive, and pythonic API, it provides several out-of-the-box optimization strategies, such as Stochastic Gradient Descent (SGD), Adam, and more.  \n",
    "> In the heart of this module lies the abstract `Optimizer` class that defines a standard interface for all the optimization strategies.  \n",
    "> Each specific optimizer class implements this interface, which ensures a consistent usage and allows for easy swapping of different strategies in your training loop.  \n",
    "\n",
    "> Among the features of this module are:  \n",
    "> - Efficient gradient computations and updates.  \n",
    "> - Advanced optimization strategies with adaptive learning rates.  \n",
    "> - Easy to extend to custom optimization strategies.  \n",
    "> - Supports weight decay regularization for avoiding overfitting.  \n",
    "\n",
    "> Whether you're training a simple linear regression or a complex deep neural network, `Optim` has got you covered. With its simple and consistent interface, the module makes the task of optimizing your models a breeze.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9455d3-bb53-4b0e-8f83-b3c74aea27b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1851e7-c0db-432f-8b7e-07c23612d944",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import minima as mi\n",
    "from minima.nn import Parameter\n",
    "from minima.autograd import Tensor\n",
    "from minima import init\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d117a048-d97f-45cb-857e-239ee00bc438",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Optimizer:\n",
    "    \"\"\"\n",
    "    Base class for all optimizers. Not meant to be instantiated directly.\n",
    "\n",
    "    This class represents the abstract concept of an optimizer, and contains methods that \n",
    "    all concrete optimizer classes must implement. It is designed to handle the parameters \n",
    "    of a machine learning model, providing functionality to perform a step of optimization \n",
    "    and to zero out gradients.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    params : Iterable\n",
    "        The parameters of the model to be optimized.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    NotImplementedError\n",
    "        If the `step` method is not implemented in a subclass.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        params # The parameters of the model to be optimized.\n",
    "    ):\n",
    "        self.params = params\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step.\n",
    "\n",
    "        This method must be overridden by any subclass to provide the specific optimization logic.\n",
    "        \n",
    "        Raises\n",
    "        ------\n",
    "        NotImplementedError\n",
    "            If the method is not implemented in a subclass.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\"\n",
    "        Zeros out all gradients in `params`.\n",
    "\n",
    "        This method is typically used before backpropagation to ensure that gradients \n",
    "        are not being accumulated from multiple passes.\n",
    "        \"\"\"\n",
    "        for p in self.params:\n",
    "            p.grad = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a408375e-08e1-4eba-ac22-20d04edefc8f",
   "metadata": {},
   "source": [
    "## SGD Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685e3697-2668-4806-992f-8fc1fbb6dd5a",
   "metadata": {},
   "source": [
    "This is a PyTorch-style implementation of the classic optimizer Stochastic Gradient Descent (SGD).\n",
    "\n",
    "SGD update is,\n",
    "\n",
    "$$\n",
    "\\theta_{t} = \\theta_{t-1} - \\alpha \\cdot g_{t}\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is the learning rate, and $g_{t}$ is the gradient at time step $t$. $Î¸_{t}$ represents the model parameters at time step $t$.\n",
    "\n",
    "The learning rate $\\alpha$ is a scalar hyperparameter that controls the size of the update at each iteration.\n",
    "\n",
    "An optional momentum term can be added to the update rule:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "v_{t} & \\leftarrow \\mu v_{t-1} + (1-\\mu) \\cdot g_t \\\\\n",
    "\\theta_{t} & \\leftarrow \\theta_{t-1} - \\alpha \\cdot v_t \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $v_{t}$ is the momentum term at time step $t$, and $\\mu$ is the momentum factor. The momentum term increases for dimensions whose gradients point in the same   \n",
    "direction and reduces updates for dimensions whose gradients change direction, thereby adding a form of preconditioning.  \n",
    "\n",
    "A weight decay term can also be included, which adds a regularization effect:\n",
    "\n",
    "$$\n",
    "\\theta_{t} = (1 - \\alpha \\cdot \\lambda) \\cdot \\theta_{t-1} - \\alpha \\cdot g_t\n",
    "$$\n",
    "\n",
    "where $\\lambda$ is the weight decay factor. This results in the model weights shrinking at each time step, which can prevent overfitting by keeping the model complexity in check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39b16d1-e0db-4747-92d5-aa4fd533a212",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SGD(Optimizer):\n",
    "    \"\"\"\n",
    "    Implements stochastic gradient descent (optionally with momentum).\n",
    "\n",
    "    This is a basic optimizer that's suitable for many machine learning models, and is often\n",
    "    used as a baseline for comparing other optimizers' performance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params : Iterable\n",
    "        The parameters of the model to be optimized.\n",
    "    lr : float, optional\n",
    "        The learning rate.\n",
    "    momentum : float, optional\n",
    "        The momentum factor.\n",
    "    wd : float, optional\n",
    "        The weight decay (L2 regularization).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        params, # The parameters of the model to be optimized.\n",
    "        lr=0.01, # The learning rate.\n",
    "        momentum=0.0, # The momentum factor.\n",
    "        wd=0.0 # The weight decay (L2 regularization).\n",
    "    ):\n",
    "        super().__init__(params)\n",
    "\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.u = {}\n",
    "        self.wd = wd\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step.\n",
    "\n",
    "        This method uses the current gradients to adjust the parameters using stochastic gradient descent.\n",
    "        \"\"\"\n",
    "        for self.idx, p in enumerate(self.params):\n",
    "            self._reg_step(p)\n",
    "            self._opt_step(p)\n",
    "\n",
    "    def _opt_step(self, p):\n",
    "        \"\"\"\n",
    "        Performs the optimization step for a single parameter tensor.\n",
    "\n",
    "        If momentum is set, it applies momentum by using a running average of the previous gradients.\n",
    "        \"\"\"\n",
    "        if self.idx not in self.u:\n",
    "            self.u[self.idx] = init.zeros(*p.shape)\n",
    "        self.u[self.idx] = self.momentum * self.u[self.idx] + (1 - self.momentum) * p.grad.data\n",
    "        p.data = p.data - self.lr * self.u[self.idx]\n",
    "\n",
    "    def _reg_step(self, p):\n",
    "        \"\"\"\n",
    "        Applies weight decay for a single parameter tensor.\n",
    "\n",
    "        This form of L2 regularization can help prevent overfitting.\n",
    "        \"\"\"\n",
    "        if self.wd != 0:\n",
    "            p.data *= (1 - self.lr * self.wd)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7aa00356-7cb1-4aa3-9bf6-70e5967cd2a3",
   "metadata": {},
   "source": [
    "## Adam Optimizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54a810ab-455b-41a8-bba0-edb974d39f4d",
   "metadata": {},
   "source": [
    "This is a PyTorch-like implementation of popular optimizer *Adam* from paper\n",
    " [Adam: A Method for Stochastic Optimization](https://papers.labml.ai/paper/1412.6980).\n",
    "\n",
    "*Adam* update is,\n",
    "$$\n",
    "\\begin{align}\n",
    "m_t &\\leftarrow \\beta_1 m_{t-1} + (1 - \\beta_1) \\cdot g_t \\\\\n",
    "v_t &\\leftarrow \\beta_2 v_{t-1} + (1 - \\beta_2) \\cdot g_t^2 \\\\\n",
    "\\hat{m}_t &\\leftarrow \\frac{m_t}{1-\\beta_1^t} \\\\\n",
    "\\hat{v}_t &\\leftarrow \\frac{v_t}{1-\\beta_2^t} \\\\\n",
    "\\theta_t &\\leftarrow \\theta_{t-1} - \\alpha \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\alpha$, $\\beta_1$, $\\beta_2$ and $\\epsilon$ are scalar hyper parameters.\n",
    "$m_t$ and $v_t$ are first and second order moments.\n",
    "$\\hat{m}_t$  and $\\hat{v}_t$ are biased corrected moments.\n",
    "$\\epsilon$ is used as a fix for division by zero error, but also acts as a form of a hyper-parameter\n",
    "that acts against variance in gradients.\n",
    "\n",
    "Effective step taken assuming $\\epsilon = 0$ is,\n",
    "$$\\Delta t = \\alpha \\cdot \\frac{\\hat{m}_t}{\\hat{v}_t}$$\n",
    "This is bounded by,\n",
    "$$\\vert \\Delta t \\vert \\le \\alpha \\cdot \\frac{1 - \\beta_1}{\\sqrt{1-\\beta_2}}$$\n",
    "when $1-\\beta_1 \\gt \\sqrt{1-\\beta_2}$\n",
    "and\n",
    "$$\\vert \\Delta t\\vert  \\le \\alpha$$\n",
    "otherwise.\n",
    "And in most common scenarios,\n",
    "$$\\vert \\Delta t \\vert \\approx \\alpha$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c7b291-fea2-4051-921f-224d99ff8e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Adam(Optimizer):\n",
    "    \"\"\"\n",
    "    Implements the Adam optimization algorithm.\n",
    "\n",
    "    Adam is an adaptive learning rate optimization algorithm that has been designed specifically for training \n",
    "    deep neural networks. It leverages the power of adaptive learning rates methods to find individual learning \n",
    "    rates for each parameter.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params : Iterable\n",
    "        The parameters of the model to be optimized.\n",
    "    lr : float, optional\n",
    "        The learning rate. Default is 0.01.\n",
    "    beta1 : float, optional\n",
    "        The exponential decay rate for the first moment estimates. Default is 0.9.\n",
    "    beta2 : float, optional\n",
    "        The exponential decay rate for the second moment estimates. Default is 0.999.\n",
    "    eps : float, optional\n",
    "        A small constant for numerical stability. Default is 1e-8.\n",
    "    weight_decay : float, optional\n",
    "        Weight decay (L2 penalty). Default is 0.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    t : int\n",
    "        The time step for the Adam optimizer.\n",
    "    exp_avg : dict\n",
    "        The dictionary to store the exponential moving average of gradient values.\n",
    "    exp_avg_sq : dict\n",
    "        The dictionary to store the exponential moving average of squared gradient values.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        params, # `params` is the list of parameters\n",
    "        lr=0.01, # `lr` is the learning rate $\\alpha$\n",
    "        beta1=0.9, # The exponential decay rate for the first moment estimates. Default is 0.9.\n",
    "        beta2=0.999, # The exponential decay rate for the second moment estimates. Default is 0.999.\n",
    "        eps=1e-8, # `eps` is $\\hat{\\epsilon}$ or $\\epsilon$ based on `optimized_update`\n",
    "        weight_decay=0.0, # is an instance of class `WeightDecay` defined in [`__init__.py`](index.html)\n",
    "    ):\n",
    "        super().__init__(params)\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        self.wd = weight_decay\n",
    "        self.t = 0\n",
    "\n",
    "        self.exp_avg = {}\n",
    "        self.exp_avg_sq = {}\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step.\n",
    "\n",
    "        This method updates the parameters based on the current gradient.\n",
    "        \"\"\"\n",
    "        for self.idx, p in enumerate(self.params):\n",
    "            self._reg_step(p)\n",
    "            self._opt_step(p)\n",
    "\n",
    "    def _opt_step(self, p):\n",
    "        \"\"\"\n",
    "        Performs the optimization step for a single parameter tensor.\n",
    "\n",
    "        The method updates the moving averages of the gradient (m) and the squared gradient (v), and then \n",
    "        computes the bias-corrected estimates of these two variables. These bias-corrected estimates are \n",
    "        then used to update the parameter.\n",
    "        \"\"\"\n",
    "        if self.idx not in self.exp_avg:\n",
    "            self.exp_avg[self.idx] = init.zeros(*p.shape)\n",
    "            self.exp_avg_sq[self.idx] = init.zeros(*p.shape)\n",
    "        \n",
    "        # Update biased first and second moment estimates\n",
    "        self.exp_avg[self.idx] = self.beta1 * self.exp_avg[self.idx] + (1 - self.beta1) * p.grad.data\n",
    "        self.exp_avg_sq[self.idx] = self.beta2 * self.exp_avg_sq[self.idx] + (1 - self.beta2) * p.grad.data**2\n",
    "        \n",
    "        # Compute bias-corrected first and second moment estimates\n",
    "        exp_avg_hat = self.exp_avg[self.idx] / (1 - self.beta1 ** (self.idx + 1))\n",
    "        exp_avg_sq_hat = self.exp_avg_sq[self.idx] / (1 - self.beta2 ** (self.idx + 1))\n",
    "        p.data = p.data - self.lr * exp_avg_hat / (exp_avg_sq_hat ** 0.5 + self.eps)\n",
    "\n",
    "    def _reg_step(self, p):\n",
    "        \"\"\"\n",
    "        Applies weight decay for a single parameter tensor.\n",
    "\n",
    "        This form of L2 regularization can help prevent overfitting. It adjusts the parameter by \n",
    "        a small factor of its current value.\n",
    "        \"\"\"\n",
    "        if self.wd != 0:\n",
    "            p.data *= (1 - self.lr * self.wd)\n",
    "        # all same :3\n",
    "        # p.data *= (1 - self.lr * self.weight_decay)\n",
    "        # p.data = p.data - self.lr * self.weight_decay * p.data\n",
    "        # p.data -= self.lr * self.weight_decay * p.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c837fe1b-86db-439a-ba29-1567cc5699e1",
   "metadata": {},
   "source": [
    "#| hide\n",
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2943be0-3cdf-4622-9421-9b1405b18bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae20858-2009-470d-a863-ac0bd45ed6ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
