{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ae06a69-4476-4501-9f1b-5bfec8abdaae",
   "metadata": {},
   "source": [
    "# ndarray\n",
    "\n",
    "> The NDArray class in minima is a kind of container that can hold all sorts of elements, especially numbers (but for now just f32 numbers).  \n",
    "> It can work with data that has any number of dimensions. For example, it can handle simple 1-dimensional rows or columns, more complex 2-dimensional  \n",
    ">  matrices, or even arrays with more dimensions. The cool thing about this NDArray class is that it can work with different backends.  \n",
    "> So, you could use it with Numpy, or a CPU backend, or even a GPU backend. It's a versatile tool for handling multi-dimensional data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4752b256-9fde-45a7-b10d-782c201e91d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3567a0c-c363-4727-8732-29be65312358",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import math\n",
    "import numpy as np\n",
    "from minima import ndarray_backend_numpy\n",
    "from typing import Optional, Sequence, Tuple, Union, Callable, Any\n",
    "from minima.utility import *\n",
    "# from . import ndarray_backend_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ffc60d-199a-495c-9b8f-16bfbf8397ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BackendDevice:\n",
    "    \"\"\"A backend device, wraps the implementation module.\"\"\"\n",
    "\n",
    "    def __init__(self, name, mod):\n",
    "        self.name = name\n",
    "        self.mod = mod\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.name == other.name\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"device(type='{self.name}')\"\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self.mod, name)\n",
    "\n",
    "    def enabled(self):\n",
    "        return self.mod is not None\n",
    "\n",
    "    def randn(self, *shape, dtype=\"float32\"):\n",
    "        return NDArray(numpy.random.randn(*shape).astype(dtype), device=self)\n",
    "\n",
    "    def rand(self, *shape, dtype=\"float32\"):\n",
    "        return NDArray(numpy.random.rand(*shape).astype(dtype), device=self)\n",
    "\n",
    "    def one_hot(self, n, i, dtype=\"float32\"):\n",
    "        return NDArray(numpy.eye(n, dtype=dtype)[i], device=self)\n",
    "\n",
    "    def empty(self, shape, dtype=\"float32\"):\n",
    "        dtype = \"float32\" if dtype is None else dtype\n",
    "        assert dtype == \"float32\"\n",
    "        return NDArray.make(shape, device=self)\n",
    "\n",
    "    def full(self, shape, fill_value, dtype=\"float32\"):\n",
    "        dtype = \"float32\" if dtype is None else dtype\n",
    "        assert dtype == \"float32\"\n",
    "        arr = self.empty(shape, dtype)\n",
    "        arr.fill(fill_value)\n",
    "        return arr\n",
    "\n",
    "def cpu_numpy():\n",
    "    \"\"\"Return numpy device\"\"\"\n",
    "    return BackendDevice('cpu_numpy', ndarray_backend_numpy)\n",
    "\n",
    "def default_device():\n",
    "    return cpu_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8430e6c-01e1-4ef2-bac3-144c7b2bcb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class NDArray:\n",
    "\n",
    "    \"\"\"\n",
    "    NDArray represents a n-dimensional array with operations that can be performed on multiple devices. \n",
    "    This class is an abstraction over numpy and other backend devices, providing a unified interface to interact with arrays.\n",
    "\n",
    "    Use cases of this class include numerical operations, scientific computing, and machine learning.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    value : NDArray or np.ndarray or other array-like structures\n",
    "        The array-like structure to be transformed into NDArray.\n",
    "    device : Optional[BackendDevice]\n",
    "        The device on which the array computations should be performed. \n",
    "        If None, the default device is used.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    _shape : tuple\n",
    "        The shape of the array.\n",
    "    _strides : tuple\n",
    "        The strides of the array.\n",
    "    _offset : int\n",
    "        The offset in the underlying buffer.\n",
    "    _device : BackendDevice\n",
    "        The device on which the array computations are performed.\n",
    "    _handle : Buffer\n",
    "        The underlying buffer that holds the data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        value: Union['NDArray', np.ndarray, Sequence], # The value on which to create the NDArray from\n",
    "        device: Optional[BackendDevice] = None # The device on which the array computations are performed.\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Constructs a new NDArray instance from an existing `NDArray`, numpy array, or a Python sequence. \n",
    "        This array can be used to perform high-performance computations on the specified device.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        value : Union[NDArray, np.ndarray, Sequence]\n",
    "            The value to create the NDArray from. If it's an NDArray, it is deep-copied to the new NDArray. \n",
    "            If it's a numpy array, it's copied to a new NDArray. If it's a Python sequence, it's converted to \n",
    "            a numpy array and then copied to a new NDArray.\n",
    "\n",
    "        device : Optional[BackendDevice]\n",
    "            The device on which the array computations are performed. Defaults to the device of the input value \n",
    "            if it's an NDArray, or to the default device otherwise.\n",
    "        \"\"\"\n",
    "        \n",
    "        if isinstance(value, NDArray): # copy of existing NDArray\n",
    "            if device is None: device = value.device\n",
    "            self._init(value.to(device) + 0.0)\n",
    "        elif isinstance(value, np.ndarray): # copy of existing np array\n",
    "            device = device if device is not None else default_device()\n",
    "            array = self.make(value.shape, device=device)\n",
    "            array.device.from_numpy(np.ascontiguousarray(value), array._handle)\n",
    "            self._init(array)\n",
    "        else:\n",
    "            array = NDArray(np.array(value), device=device)\n",
    "            self._init(array)\n",
    "\n",
    "    def _init(self, other) -> None:\n",
    "        \"\"\"\n",
    "        A private method that initializes the new NDArray with the values, shape, strides, offset, device, \n",
    "        and handle of another NDArray.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        other : NDArray\n",
    "            The NDArray to initialize from.\n",
    "        \"\"\"\n",
    "        self._shape = other._shape\n",
    "        self._strides = other._strides\n",
    "        self._offset = other._offset\n",
    "        self._device = other._device\n",
    "        self._handle = other._handle\n",
    "\n",
    "    @staticmethod\n",
    "    def make(\n",
    "        shape: Sequence[int], # The shape of the new array.\n",
    "        strides: Optional[Sequence[int]] = None, # The strides of the new array. If None, compact strides are computed.\n",
    "        device: Optional[BackendDevice] = None, # The device on which the new array computations should be performed. If None, the default device is used.\n",
    "        offset: Optional[int] = None, # The offset in the underlying buffer of the new array. If None, it defaults to 0.\n",
    "        handle: Optional[Any] = None # The underlying buffer that should hold the data. If None, a new buffer is allocated.\n",
    "    ) -> 'NDArray':\n",
    "        \"\"\"\n",
    "        Constructs a new NDArray with the specified shape, strides, device, offset, and handle.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        shape : Sequence[int]\n",
    "            The shape of the new array.\n",
    "        strides : Optional[Sequence[int]]\n",
    "            The strides of the new array. If None, compact strides are computed.\n",
    "        device : Optional[BackendDevice]\n",
    "            The device on which the new array computations should be performed. If None, the default device is used.\n",
    "        offset : Optional[int]\n",
    "            The offset in the underlying buffer of the new array. If None, it defaults to 0.\n",
    "        handle : Optional[Buffer]\n",
    "            The underlying buffer that should hold the data. If None, a new buffer is allocated.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        NDArray\n",
    "            A new NDArray instance.\n",
    "        \"\"\"\n",
    "        array = NDArray.__new__(NDArray)\n",
    "        array._shape = tuple(shape)\n",
    "        array._strides = NDArray.compact_strides(shape) if strides is None else strides\n",
    "        array._device = default_device() if device is None else device\n",
    "        array._offset = offset\n",
    "        array._handle = array.device.Array(prod(shape)) if handle is None else handle\n",
    "        return array\n",
    "\n",
    "    @staticmethod\n",
    "    def compact_strides(shape) -> Tuple:\n",
    "        res = [1] + [prod(shape[-i:]) for i in range(1, len(shape))]\n",
    "        return tuple(res[::-1])\n",
    "\n",
    "    def _is_compact(self) -> bool:\n",
    "        return self.strides == self.compact_strides(self.shape) and prod(self.shape) == self._handle.size\n",
    "\n",
    "    def compact(self) -> 'NDArray':\n",
    "        \"\"\"\n",
    "        Returns a compact version of this array. If the array is already compact, it returns itself.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        NDArray\n",
    "            The compact version of this array.\n",
    "        \"\"\"\n",
    "        if self._is_compact():\n",
    "            return self\n",
    "        out = NDArray.make(shape=self.shape, device=self.device)\n",
    "        self.device.compact(self._handle, out._handle, self.shape, self.strides, self.offset)\n",
    "        return out\n",
    "        \n",
    "    def as_strided(self, shape, strides) -> 'NDArray':\n",
    "        assert len(shape) == len(strides)\n",
    "        return NDArray.make(shape=shape, strides=strides, handle=self._handle)\n",
    "\n",
    "    def flat(self) -> 'NDArray':\n",
    "        return self.reshape((self.size, ))\n",
    "\n",
    "    def to(self, device: BackendDevice) -> 'NDArray':\n",
    "        \"\"\"\n",
    "        Transfers this array to the specified device.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        device : BackendDevice\n",
    "            The device to which this array should be transferred.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        NDArray\n",
    "            This array after it has been transferred to `device`.\n",
    "        \"\"\"\n",
    "        return self if device == self.device else NDArray(self.numpy(), device=device)\n",
    "        \n",
    "    def numpy(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Returns a numpy representation of this array.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            A numpy array that has the same data as this array.\n",
    "        \"\"\"\n",
    "        return self.device.to_numpy(self._handle, self._shape, self._strides, self._offset)\n",
    "\n",
    "    @property\n",
    "    def shape(self) -> Tuple[int, ...]:\n",
    "        return self._shape\n",
    "\n",
    "    @property\n",
    "    def strides(self) -> Tuple[int, ...]:\n",
    "        return self._strides\n",
    "\n",
    "    @property\n",
    "    def device(self) -> BackendDevice:\n",
    "        return self._device\n",
    "\n",
    "    @property\n",
    "    def dtype(self) -> str:\n",
    "        # only support float32 for now\n",
    "        return \"float32\"\n",
    "\n",
    "    @property\n",
    "    def ndim(self) -> int:\n",
    "        \"\"\" Return number of dimensions. \"\"\"\n",
    "        return len(self._shape)\n",
    "\n",
    "    @property\n",
    "    def size(self) -> int:\n",
    "        return prod(self._shape)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return \"NDArray(\" + self.numpy().__str__() + f\", device={self.device})\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.numpy().__str__()\n",
    "\n",
    "    def fill(self, val) -> 'NDArray':\n",
    "        return self.device.fill(self._handle, val)\n",
    "\n",
    "    ### Elementwise functions\n",
    "\n",
    "    def log(self):\n",
    "        \"\"\"\n",
    "        Computes the natural logarithm element-wise for the NDArray. \n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        NDArray\n",
    "            A new NDArray with the natural logarithm applied element-wise. The shape of the returned array matches\n",
    "            the original NDArray.\n",
    "        \"\"\"\n",
    "        out = NDArray.make(self.shape, device=self.device)\n",
    "        self.device.ewise_log(self.compact()._handle, out._handle)\n",
    "        return out\n",
    "\n",
    "    def exp(self):\n",
    "        \"\"\"\n",
    "        Computes the exponential function element-wise for the NDArray.\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        NDArray\n",
    "            A new NDArray with the exponential function applied element-wise. The shape of the returned array matches\n",
    "            the original NDArray.\n",
    "        \"\"\"\n",
    "        \n",
    "        out = NDArray.make(self.shape, device=self.device)\n",
    "        self.device.ewise_exp(self.compact()._handle, out._handle)\n",
    "        return out\n",
    "\n",
    "    def tanh(self):\n",
    "        \"\"\"\n",
    "        Computes the hyperbolic tangent element-wise for the NDArray.\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        NDArray\n",
    "            A new NDArray with the hyperbolic tangent applied element-wise. The shape of the returned array matches\n",
    "            the original NDArray.\n",
    "        \"\"\"\n",
    "        \n",
    "        out = NDArray.make(self.shape, device=self.device)\n",
    "        self.device.ewise_tanh(self.compact()._handle, out._handle)\n",
    "        return out\n",
    "\n",
    "    def reshape(self, new_shape):\n",
    "        \"\"\"\n",
    "        Reshape the matrix without copying memory.  This will return a matrix\n",
    "        that corresponds to a reshaped array but points to the same memory as\n",
    "        the original array.\n",
    "        Raises:\n",
    "            ValueError if product of current shape is not equal to the product\n",
    "            of the new shape, or if the matrix is not compact.\n",
    "        Args:\n",
    "            new_shape (tuple): new shape of the array\n",
    "        Returns:\n",
    "            NDArray : reshaped array; this will point to the same memory as the original NDArray.\n",
    "        \"\"\"\n",
    "        \n",
    "        if prod(new_shape) != prod(self.shape) or not self.is_compact():\n",
    "            raise ValueError(\"Invalid reshape\")\n",
    "        return self.as_strided(shape=new_shape, strides=NDArray.compact_strides(new_shape))\n",
    "\n",
    "    def permute(self, new_axes):\n",
    "        \"\"\"\n",
    "        Permute order of the dimensions.  new_axes describes a permutation of the\n",
    "        existing axes, so e.g.:\n",
    "          - If we have an array with dimension \"BHWC\" then .permute((0,3,1,2))\n",
    "            would convert this to \"BCHW\" order.\n",
    "          - For a 2D array, .permute((1,0)) would transpose the array.\n",
    "        Like reshape, this operation should not copy memory, but achieves the\n",
    "        permuting by just adjusting the shape/strides of the array.  That is,\n",
    "        it returns a new array that has the dimensions permuted as desired, but\n",
    "        which points to the same memory as the original array.\n",
    "        Args:\n",
    "            new_axes (tuple): permutation order of the dimensions\n",
    "        Returns:\n",
    "            NDarray : new NDArray object with permuted dimensions, pointing\n",
    "            to the same memory as the original NDArray (i.e., just shape and\n",
    "            strides changed).\n",
    "        \"\"\"\n",
    "        \n",
    "        new_shape = tuple(self.shape[i] for i in new_axes)\n",
    "        new_strides = tuple(self.strides[i] for i in new_axes)\n",
    "        return self.as_strided(shape=new_shape, strides=new_strides)\n",
    "\n",
    "    def broadcast_to(self, new_shape):\n",
    "        \"\"\"\n",
    "        Broadcast an array to a new shape.  new_shape's elements must be the\n",
    "        same as the original shape, except for dimensions in the self where\n",
    "        the size = 1 (which can then be broadcast to any size).  As with the\n",
    "        previous calls, this will not copy memory, and just achieves\n",
    "        broadcasting by manipulating the strides.\n",
    "        Raises:\n",
    "            assertion error if new_shape[i] != shape[i] for all i where\n",
    "            shape[i] != 1\n",
    "        Args:\n",
    "            new_shape (tuple): shape to broadcast to\n",
    "        Returns:\n",
    "            NDArray: the new NDArray object with the new broadcast shape; should\n",
    "            point to the same memory as the original array.\n",
    "        \"\"\"\n",
    "        \n",
    "        for old_shape_i, new_shape_i in zip(self.shape, new_shape):\n",
    "            if old_shape_i != 1:\n",
    "                assert new_shape_i == old_shape_i\n",
    "        new_strides = tuple(0 if old_shape_i == 1 else stride_i for old_shape_i, stride_i in zip(self.shape, self.strides))\n",
    "        return self.as_strided(shape=new_shape, strides=new_strides)\n",
    "\n",
    "    def _ewise_or_scalar(self, other: Union['NDArray', float], ewise_fn: Callable, scalr_fn: Callable) -> 'NDArray':\n",
    "        \"\"\"\n",
    "        This private method applies an element-wise function (`ewise_fn`) to two `NDArray` instances, or a scalar function (`scalr_fn`) \n",
    "        to this `NDArray` and a scalar value. It returns a new `NDArray` instance with the results.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        other : Union[NDArray, float]\n",
    "            The second operand for the operation. It can be either another `NDArray` (for element-wise operations) or a scalar \n",
    "            (for scalar operations).\n",
    "    \n",
    "        ewise_fn : Callable\n",
    "            A function to apply element-wise if `other` is an `NDArray`. This function should take two `NDArray` handles and \n",
    "            output a handle.\n",
    "    \n",
    "        scalr_fn : Callable\n",
    "            A function to apply if `other` is a scalar. This function should take an `NDArray` handle and a scalar, and \n",
    "            output a handle.\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        NDArray\n",
    "            A new `NDArray` instance with the results of the operation.\n",
    "    \n",
    "        Raises\n",
    "        ------\n",
    "        AssertionError\n",
    "            If `other` is an `NDArray` but does not have the same shape as `self`.\n",
    "        \"\"\"\n",
    "        out = NDArray.make(shape=self.shape, device=self.device)\n",
    "        if isinstance(other, NDArray):\n",
    "            assert self.shape == other.shape, f'operands could not be added together with shapes {self.shape} {other.shape}'\n",
    "            ewise_fn(self.compact()._handle, other.compact()._handle, out._handle)\n",
    "        else:\n",
    "            scalr_fn(self.compact()._handle, other, out._handle)\n",
    "        return out\n",
    "\n",
    "    def __add__(self, other: Union['NDArray', float]) -> 'NDArray':\n",
    "        \"\"\"\n",
    "        Performs element-wise addition between this array and `other`. If `other` is not an NDArray, it is treated as a scalar.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        other : NDArray or scalar\n",
    "            The other operand in the addition.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        NDArray\n",
    "            The result of the addition.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        AssertionError\n",
    "            If `other` is an NDArray and does not have the same shape as this array.\n",
    "        \"\"\"\n",
    "        return self._ewise_or_scalar(other, ewise_fn=self.device.ewise_add, scalr_fn=self.device.scalar_add)\n",
    "\n",
    "    def __sub__(self, other) -> 'NDArray':\n",
    "        \"\"\"\n",
    "        Implements the subtract operation. This method performs element-wise subtraction between two NDArrays\n",
    "        or an NDArray and a scalar.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        other : NDArray or scalar\n",
    "            The array or scalar to subtract from the current NDArray.\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        NDArray\n",
    "            The resultant NDArray after performing subtraction.\n",
    "        \"\"\"\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other) -> 'NDArray':\n",
    "        \"\"\"\n",
    "        Implements the reverse subtract operation. This is used when the NDArray is on the right side of a subtraction.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        other : scalar\n",
    "            The scalar to subtract the NDArray from.\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        NDArray\n",
    "            The resultant NDArray after performing subtraction.\n",
    "        \"\"\"\n",
    "        \n",
    "        return (-self) + other\n",
    "\n",
    "    def __mul__(self, other) -> 'NDArray':\n",
    "        \"\"\"\n",
    "        Implements the multiply operation. This method performs element-wise multiplication between two NDArrays\n",
    "        or an NDArray and a scalar.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        other : NDArray or scalar\n",
    "            The array or scalar to multiply with the current NDArray.\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        NDArray\n",
    "            The resultant NDArray after performing multiplication.\n",
    "        \"\"\"\n",
    "        return self._ewise_or_scalar(other, ewise_fn=self.device.ewise_mul, scalr_fn=self.device.scalar_mul)\n",
    "\n",
    "    def __truediv__(self,  other) -> 'NDArray':\n",
    "        \"\"\"\n",
    "        Implements the true divide operation. This method performs element-wise division between two NDArrays\n",
    "        or an NDArray and a scalar.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        other : NDArray or scalar\n",
    "            The array or scalar to divide the current NDArray by.\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        NDArray\n",
    "            The resultant NDArray after performing division.\n",
    "        \"\"\"\n",
    "        return self._ewise_or_scalar(other, ewise_fn=self.device.ewise_div, scalr_fn=self.device.scalar_div)\n",
    "\n",
    "    def __neg__(self):\n",
    "        \"\"\"\n",
    "        Implements the negation operation. This method performs element-wise negation for self(NDArray).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        self : NDArray\n",
    "            The array to negate.\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        NDArray\n",
    "            The resultant NDArray after performing negation.\n",
    "        \"\"\"\n",
    "        \n",
    "        return self * (-1)\n",
    "\n",
    "    def __pow__(self, scalar) -> 'NDArray':\n",
    "        out = NDArray.make(self.shape, self.device)\n",
    "        self.device.scalr_power(self.compact()._handle, scalar, out._handle)\n",
    "        return out\n",
    "\n",
    "    __radd__ = __add__\n",
    "    __rmul__ = __mul__\n",
    "\n",
    "    def maximum(self, other):\n",
    "        return self.ewise_or_scalar(other, self.device.ewise_maximum, self.device.scalar_maximum)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.ewise_or_scalar(other, self.device.ewise_eq, self.device.scalar_eq)\n",
    "\n",
    "    def __ge__(self, other):\n",
    "        return self.ewise_or_scalar(other, self.device.ewise_ge, self.device.scalar_ge)\n",
    "\n",
    "    def __ne__(self, other):\n",
    "        return 1 - (self == other)\n",
    "\n",
    "    def __gt__(self, other):\n",
    "        return (self >= other) * (self != other)\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return 1 - (self >= other)\n",
    "\n",
    "    def __le__(self, other):\n",
    "        return 1 - (self > other)\n",
    "        \n",
    "\n",
    "    def process_slice(self, sl, dim):\n",
    "        \"\"\" Convert a slice to an explicit start/stop/step \"\"\"\n",
    "        start, stop, step = sl.start, sl.stop, sl.step\n",
    "        if start == None:\n",
    "            start = 0\n",
    "        if start < 0:\n",
    "            start = self.shape[dim]\n",
    "        if stop == None:\n",
    "            stop = self.shape[dim]\n",
    "        if stop < 0:\n",
    "            stop = self.shape[dim] + stop\n",
    "        if step == None:\n",
    "            step = 1\n",
    "\n",
    "        # we're not gonna handle negative strides and that kind of thing\n",
    "        assert stop > start, \"Start must be less than stop\"\n",
    "        assert step > 0, \"No support for  negative increments\"\n",
    "        return slice(start, stop, step)\n",
    "\n",
    "    def __getitem__(self, idxs):\n",
    "        \"\"\"\n",
    "        Implements the get item operation to access elements or sub-arrays of our NDArray instance. \n",
    "        This method supports slicing and integer-based access similar to NumPy. It returns a new NDArray\n",
    "        object that represents a view into the original array without copying memory.\n",
    "    \n",
    "        Raises\n",
    "        ------\n",
    "        AssertionError\n",
    "            If a slice has negative size or step, or if the number of slices is not equal to the number of dimensions.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        idxs : tuple\n",
    "            A tuple of slice or integer elements corresponding to the subset of the matrix to get.\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        NDArray\n",
    "            A new NDArray object corresponding to the selected subset of elements. This should not copy memory but \n",
    "            just manipulate the shape/strides/offset of the new array, referencing the same array as the original one.\n",
    "        \"\"\"\n",
    "\n",
    "        # handle singleton as tuple, everything as slices\n",
    "        if not isinstance(idxs, tuple):\n",
    "            idxs = (idxs,)\n",
    "        idxs = tuple(\n",
    "            [\n",
    "                self.process_slice(s, i) if isinstance(s, slice) else slice(s, s + 1, 1)\n",
    "                for i, s in enumerate(idxs)\n",
    "            ]\n",
    "        )\n",
    "        assert len(idxs) == self.ndim, \"Need indexes equal to number of dimensions\"\n",
    "        shape = tuple((idx.stop - idx.start) // idx.step for idx in idxs)\n",
    "        offset = sum(idx.start * stride for idx, stride in zip(idxs, self.strides))\n",
    "        strides = tuple(idx.step * stride for idx, stride in zip(idxs, self.strides)) # Corrected line -> haha was FUN!!\n",
    "        return NDArray.make(shape, strides=strides, device=self.device, handle=self._handle, offset=offset)\n",
    "\n",
    "    def __setitem__(self, idxs, other):\n",
    "        \"\"\"\n",
    "        Implements the set item operation to modify elements or sub-arrays of our NDArray instance. \n",
    "        This method supports slicing and integer-based access similar to NumPy. It modifies the original NDArray\n",
    "        in place.\n",
    "        -> uses same semantics as __getitem__().\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        idxs : tuple\n",
    "            A tuple of slice or integer elements corresponding to the subset of the matrix to set.\n",
    "    \n",
    "        other : NDArray or scalar\n",
    "            The array or scalar value to set into the specified subset of the matrix. If `other` is an NDArray,\n",
    "            its shape should match the shape of the subset defined by `idxs`.\n",
    "    \n",
    "        Raises\n",
    "        ------\n",
    "        AssertionError\n",
    "            If `other` is an NDArray and its shape does not match the shape of the subset defined by `idxs`.\n",
    "        \"\"\"\n",
    "        view = self.__getitem__(idxs)\n",
    "        if isinstance(other, NDArray):\n",
    "            assert prod(view.shape) == prod(other.shape)\n",
    "            self.device.ewise_setitem(\n",
    "                other.compact()._handle,\n",
    "                view._handle,\n",
    "                view.shape,\n",
    "                view.strides,\n",
    "                view._offset,\n",
    "            )\n",
    "        else:\n",
    "            self.device.scalar_setitem(\n",
    "                other,\n",
    "                view._handle,\n",
    "                view.shape,\n",
    "                view.strides,\n",
    "                view._offset,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aab255-a5a2-4367-ad05-0544d341e435",
   "metadata": {},
   "source": [
    "The concept of \"strides\" is crucial to understanding how multi-dimensional arrays are stored and accessed in memory. The term \"stride\" refers to the number of elements (or steps) you need to move in memory to go from one element to the next along a particular axis of an array.\n",
    "\n",
    "In the context of PyTorch and many other libraries that work with multi-dimensional arrays, the strides attribute of a tensor gives you the number of elements you need to skip in memory to move one step along each dimension of the tensor.\n",
    "\n",
    "The strides of a tensor are defined as a tuple of integers, where each integer represents the step size for the corresponding dimension. \n",
    "\n",
    "Let's look at your example:\n",
    "\n",
    "```python\n",
    "x = torch.arange(24).reshape(2,3,4)\n",
    "print(x.stride()) # outputs: (12, 4, 1)\n",
    "```\n",
    "\n",
    "Here, the tensor `x` has a shape of (2,3,4), and the stride is (12,4,1).\n",
    "\n",
    "- The first element of the stride tuple, 12, tells you that you need to step over 12 elements in memory to get from one element to the next along the first axis (axis=0, the one that has size 2). This makes sense, because there are 12 elements in each \"block\" of this dimension (3*4 = 12).\n",
    "\n",
    "- The second element, 4, says that you need to step over 4 elements in memory to move from one element to the next along the second axis (axis=1, the one that has size 3). This is because there are 4 elements in each \"row\" of this dimension.\n",
    "\n",
    "- The last element, 1, shows that you only need to step over 1 element in memory to move from one element to the next along the last axis (axis=2, the one that has size 4). This is because elements along this axis are contiguous in memory.\n",
    "\n",
    "In conclusion, the concept of strides is critical for efficient storage and computations on multi-dimensional arrays, as it allows libraries like PyTorch to perform complex operations without needing to actually rearrange or copy any data in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bce74f-9df4-48b6-b765-8f8fc689cf04",
   "metadata": {},
   "source": [
    "Let's start with a simple 1D case. Imagine we have an array of size 10 and we want to select every second element. Instead of physically copying every second element to a new array, we could just create a new \"view\" of the array with a stride of 2. This means that to move to the next element in our sliced array, we jump over 2 elements in the original data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3680da94-4c50-4bae-816f-21ddc090e2c4",
   "metadata": {},
   "source": [
    "For multi-dimensional arrays, the principle is the same but each dimension has its own stride. When you slice a tensor, you're essentially creating a new tensor (a view) that starts from a different offset and possibly uses different strides. \n",
    "\n",
    "Consider a 2D case: if you slice the first dimension (e.g., `array[1:, :]`), you're changing the offset to start from the second element along that dimension. Essentially, you're jumping over a number of elements equal to the stride of that dimension. \n",
    "\n",
    "If you slice the second dimension (e.g., `array[:, ::2]` to select every second column), you're not changing the offset, but you're doubling the stride for the second dimension. This tells the tensor to skip one element in memory for every step in that dimension, giving you every second column.\n",
    "\n",
    "In summary, slicing doesn't involve any data copying. Instead, it changes the starting point (offset) and how you move along each dimension (stride) of the tensor. This makes slicing operations very efficient, even on large tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5a2384-b1d9-4271-a0a1-624b133dd0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37aea5d-b779-4555-8069-de2444305242",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
