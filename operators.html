<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="The operators module in this framework provides a collection of tensor operations for building computational graphs in deep learning. Each class in this module represents a different type of operation that can be performed on tensors, such as element-wise addition, scalar multiplication, division, exponentiation, etc.">

<title>minima - operators</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="styles.css">
<meta property="og:title" content="minima - operators">
<meta property="og:description" content="The operators module in this framework provides a collection of tensor operations for building computational graphs in deep learning. Each class in this module represents a different type of operation that can be performed on tensors, such as element-wise addition, scalar multiplication, division, exponentiation, etc.">
<meta property="og:site-name" content="minima">
<meta name="twitter:title" content="minima - operators">
<meta name="twitter:description" content="The operators module in this framework provides a collection of tensor operations for building computational graphs in deep learning. Each class in this module represents a different type of operation that can be performed on tensors, such as element-wise addition, scalar multiplication, division, exponentiation, etc.">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">minima</span>
    </a>
  </div>
        <div class="quarto-navbar-tools ms-auto">
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./operators.html">operators</a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome to minima</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./autograd.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">autograd</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./operators.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">operators</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./init.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">init</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./nn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">nn</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./optim.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">optim</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">data</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ndarray.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ndarray</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ndarray_backend_numpy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ndarray_backend_numpy</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./utility.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">utility</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#note-about-the-out_grad-parameter" id="toc-note-about-the-out_grad-parameter" class="nav-link active" data-scroll-target="#note-about-the-out_grad-parameter">Note about the <code>out_grad</code> parameter</a></li>
  <li><a href="#element-wise-addition" id="toc-element-wise-addition" class="nav-link" data-scroll-target="#element-wise-addition">Element Wise Addition</a>
  <ul class="collapse">
  <li><a href="#add" id="toc-add" class="nav-link" data-scroll-target="#add">add</a></li>
  <li><a href="#ewiseadd" id="toc-ewiseadd" class="nav-link" data-scroll-target="#ewiseadd">EWiseAdd</a></li>
  </ul></li>
  <li><a href="#scalar-addition" id="toc-scalar-addition" class="nav-link" data-scroll-target="#scalar-addition">Scalar Addition</a>
  <ul class="collapse">
  <li><a href="#add_scalar" id="toc-add_scalar" class="nav-link" data-scroll-target="#add_scalar">add_scalar</a></li>
  <li><a href="#addscalar" id="toc-addscalar" class="nav-link" data-scroll-target="#addscalar">AddScalar</a></li>
  </ul></li>
  <li><a href="#element-wise-multiplication" id="toc-element-wise-multiplication" class="nav-link" data-scroll-target="#element-wise-multiplication">Element Wise Multiplication</a>
  <ul class="collapse">
  <li><a href="#multiply" id="toc-multiply" class="nav-link" data-scroll-target="#multiply">multiply</a></li>
  <li><a href="#ewisemul" id="toc-ewisemul" class="nav-link" data-scroll-target="#ewisemul">EWiseMul</a></li>
  </ul></li>
  <li><a href="#scalar-multiplication" id="toc-scalar-multiplication" class="nav-link" data-scroll-target="#scalar-multiplication">Scalar Multiplication</a>
  <ul class="collapse">
  <li><a href="#mul_scalar" id="toc-mul_scalar" class="nav-link" data-scroll-target="#mul_scalar">mul_scalar</a></li>
  <li><a href="#mulscalar" id="toc-mulscalar" class="nav-link" data-scroll-target="#mulscalar">MulScalar</a></li>
  </ul></li>
  <li><a href="#element-wise-divide" id="toc-element-wise-divide" class="nav-link" data-scroll-target="#element-wise-divide">Element Wise Divide</a>
  <ul class="collapse">
  <li><a href="#divide" id="toc-divide" class="nav-link" data-scroll-target="#divide">divide</a></li>
  <li><a href="#ewisediv" id="toc-ewisediv" class="nav-link" data-scroll-target="#ewisediv">EWiseDiv</a></li>
  </ul></li>
  <li><a href="#scalar-division" id="toc-scalar-division" class="nav-link" data-scroll-target="#scalar-division">Scalar Division</a>
  <ul class="collapse">
  <li><a href="#divide_scalar" id="toc-divide_scalar" class="nav-link" data-scroll-target="#divide_scalar">divide_scalar</a></li>
  <li><a href="#divscalar" id="toc-divscalar" class="nav-link" data-scroll-target="#divscalar">DivScalar</a></li>
  </ul></li>
  <li><a href="#negation" id="toc-negation" class="nav-link" data-scroll-target="#negation">Negation</a>
  <ul class="collapse">
  <li><a href="#negate" id="toc-negate" class="nav-link" data-scroll-target="#negate">negate</a></li>
  <li><a href="#negate-1" id="toc-negate-1" class="nav-link" data-scroll-target="#negate-1">Negate</a></li>
  </ul></li>
  <li><a href="#exp" id="toc-exp" class="nav-link" data-scroll-target="#exp">Exp</a>
  <ul class="collapse">
  <li><a href="#exp-1" id="toc-exp-1" class="nav-link" data-scroll-target="#exp-1">exp</a></li>
  <li><a href="#exp-2" id="toc-exp-2" class="nav-link" data-scroll-target="#exp-2">Exp</a></li>
  </ul></li>
  <li><a href="#relu" id="toc-relu" class="nav-link" data-scroll-target="#relu">ReLU</a>
  <ul class="collapse">
  <li><a href="#relu-1" id="toc-relu-1" class="nav-link" data-scroll-target="#relu-1">relu</a></li>
  <li><a href="#relu-2" id="toc-relu-2" class="nav-link" data-scroll-target="#relu-2">ReLU</a></li>
  </ul></li>
  <li><a href="#power-scalar" id="toc-power-scalar" class="nav-link" data-scroll-target="#power-scalar">Power Scalar</a>
  <ul class="collapse">
  <li><a href="#power_scalar" id="toc-power_scalar" class="nav-link" data-scroll-target="#power_scalar">power_scalar</a></li>
  <li><a href="#powerscalar" id="toc-powerscalar" class="nav-link" data-scroll-target="#powerscalar">PowerScalar</a></li>
  </ul></li>
  <li><a href="#log" id="toc-log" class="nav-link" data-scroll-target="#log">Log</a></li>
  <li><a href="#transpose" id="toc-transpose" class="nav-link" data-scroll-target="#transpose">Transpose</a>
  <ul class="collapse">
  <li><a href="#transpose-1" id="toc-transpose-1" class="nav-link" data-scroll-target="#transpose-1">transpose</a></li>
  <li><a href="#transpose-2" id="toc-transpose-2" class="nav-link" data-scroll-target="#transpose-2">Transpose</a></li>
  </ul></li>
  <li><a href="#reshape" id="toc-reshape" class="nav-link" data-scroll-target="#reshape">Reshape</a>
  <ul class="collapse">
  <li><a href="#reshape-1" id="toc-reshape-1" class="nav-link" data-scroll-target="#reshape-1">reshape</a></li>
  <li><a href="#reshape-2" id="toc-reshape-2" class="nav-link" data-scroll-target="#reshape-2">Reshape</a></li>
  </ul></li>
  <li><a href="#matrix-multiplication" id="toc-matrix-multiplication" class="nav-link" data-scroll-target="#matrix-multiplication">Matrix Multiplication</a>
  <ul class="collapse">
  <li><a href="#matmul" id="toc-matmul" class="nav-link" data-scroll-target="#matmul">matmul</a></li>
  <li><a href="#matmul-1" id="toc-matmul-1" class="nav-link" data-scroll-target="#matmul-1">MatMul</a></li>
  </ul></li>
  <li><a href="#summation" id="toc-summation" class="nav-link" data-scroll-target="#summation">Summation</a>
  <ul class="collapse">
  <li><a href="#summation-1" id="toc-summation-1" class="nav-link" data-scroll-target="#summation-1">summation</a></li>
  <li><a href="#summation-2" id="toc-summation-2" class="nav-link" data-scroll-target="#summation-2">Summation</a></li>
  </ul></li>
  <li><a href="#broadcast" id="toc-broadcast" class="nav-link" data-scroll-target="#broadcast">Broadcast</a>
  <ul class="collapse">
  <li><a href="#broadcast_to" id="toc-broadcast_to" class="nav-link" data-scroll-target="#broadcast_to">broadcast_to</a></li>
  <li><a href="#broadcastto" id="toc-broadcastto" class="nav-link" data-scroll-target="#broadcastto">BroadcastTo</a></li>
  </ul></li>
  <li><a href="#logsumexp" id="toc-logsumexp" class="nav-link" data-scroll-target="#logsumexp">LogSumExp</a>
  <ul class="collapse">
  <li><a href="#logsumexp-1" id="toc-logsumexp-1" class="nav-link" data-scroll-target="#logsumexp-1">logsumexp</a></li>
  <li><a href="#logsumexp-2" id="toc-logsumexp-2" class="nav-link" data-scroll-target="#logsumexp-2">LogSumExp</a></li>
  </ul></li>
  <li><a href="#export" id="toc-export" class="nav-link" data-scroll-target="#export">Export</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/m0saan/minima/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">operators</h1>
</div>

<div>
  <div class="description">
    The <code>operators</code> module in this framework provides a collection of tensor operations for building computational graphs in deep learning. Each class in this module represents a different type of operation that can be performed on tensors, such as element-wise addition, scalar multiplication, division, exponentiation, etc.
  </div>
</div>


<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->
<section id="note-about-the-out_grad-parameter" class="level2">
<h2 class="anchored" data-anchor-id="note-about-the-out_grad-parameter">Note about the <code>out_grad</code> parameter</h2>
<p>During backpropagation in a neural network, we compute gradients starting from the output layer and propagate them back towards the input layer. The key idea here is that each layer receives the gradient of the loss with respect to its output (let’s call this <code>out_grad</code>), and it needs to compute and pass back the gradient of the loss with respect to its input (let’s call this <code>in_grad</code>). This is needed so that the parameters of each layer can be updated correctly during gradient descent.</p>
<p>The <code>out_grad</code> parameter refers to the gradient of the loss function with respect to the output of the node. Multiplying this with the local gradient gives the gradient of the loss with respect to the input to the node, according to the chain rule of calculus, which is the basis for backpropagation in neural networks.</p>
<p>The chain rule is a fundamental concept in calculus that provides a method to compute the derivative of composite functions. In simple terms, the chain rule states that the derivative of a composite function is the derivative of the outer function multiplied by the derivative of the inner function.</p>
<p>Given a composite function that is the composition of two functions, say, <span class="math inline">\(f(g(x))\)</span>, the chain rule can be stated as follows:</p>
<p><span class="math display">\[\frac{df}{dx} = \frac{df}{dg} \cdot \frac{dg}{dx}\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(\frac{df}{dx}\)</span> is the derivative of the composite function <span class="math inline">\(f(g(x))\)</span> with respect to <span class="math inline">\(x\)</span>,</li>
<li><span class="math inline">\(\frac{df}{dg}\)</span> is the derivative of the outer function <span class="math inline">\(f\)</span> with respect to its argument <span class="math inline">\(g(x)\)</span>, and</li>
<li><span class="math inline">\(\frac{dg}{dx}\)</span> is the derivative of the inner function <span class="math inline">\(g(x)\)</span> with respect to <span class="math inline">\(x\)</span>.</li>
</ul>
<p>The chain rule can be extended to the case where we have more than two composite functions.</p>
</section>
<section id="element-wise-addition" class="level2">
<h2 class="anchored" data-anchor-id="element-wise-addition">Element Wise Addition</h2>
<p>Let’s walk through the step-by-step derivative calculation for the <a href="https://m0saan.github.io/minima/operators.html#ewiseadd"><code>EWiseAdd</code></a> operation:</p>
<p>We have the function <code>f(a, b) = a + b</code>, where <code>a</code> and <code>b</code> are tensors. Our goal is to compute the partial derivatives with respect to <code>a</code> and <code>b</code>.</p>
<p>Let’s start by calculating the derivative of <code>f</code> with respect to <code>a</code>, denoted as <code>df/da</code>:</p>
<p>Step 1: Compute the derivative of <code>f</code> with respect to <code>a</code>.</p>
<p><span class="math inline">\(\frac{{\partial f}}{{\partial a}} = \frac{{\partial}}{{\partial a}} (a + b)\)</span></p>
<p>Since <code>a</code> is the variable we are differentiating with respect to, the derivative of <code>a</code> with respect to itself is 1:</p>
<p><span class="math display">\[\frac{{\partial f}}{{\partial a}} = 1\]</span></p>
<p>Therefore, <span class="math display">\[\frac{{\partial f}}{{\partial a}} = 1.\]</span></p>
<p>Step 2: Compute the derivative of <code>f</code> with respect to <code>b</code>.</p>
<p><span class="math display">\[\frac{{\partial f}}{{\partial b}} = \frac{{\partial}}{{\partial b}} (a + b)\]</span></p>
<p>Again, since <code>b</code> is the variable we are differentiating with respect to, the derivative of <code>b</code> with respect to itself is 1:</p>
<p><span class="math display">\[\frac{{\partial f}}{{\partial b}} = 1\]</span></p>
<p>Therefore, <span class="math display">\[\frac{{\partial f}}{{\partial b}} = 1\]</span></p>
<p>Hence, the partial derivatives of <code>f(a, b) = a + b</code> with respect to <code>a</code> and <code>b</code> are both equal to 1.</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/operators.py#L65" target="_blank" style="float:right; font-size:smaller">source</a></p>
<section id="add" class="level3">
<h3 class="anchored" data-anchor-id="add">add</h3>
<blockquote class="blockquote">
<pre><code> add (a:minima.autograd.Tensor, b:minima.autograd.Tensor)</code></pre>
</blockquote>
<p>Adds two tensors element-wise.</p>
<p>Args: - a: The first tensor. - b: The second tensor.</p>
<p>Returns: The element-wise sum of a and b.</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/operators.py#L26" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="ewiseadd" class="level3">
<h3 class="anchored" data-anchor-id="ewiseadd">EWiseAdd</h3>
<blockquote class="blockquote">
<pre><code> EWiseAdd ()</code></pre>
</blockquote>
<p>Performs element-wise addition of two tensors.</p>
<p>Example: &gt;&gt;&gt; a = Tensor([1, 2, 3]) &gt;&gt;&gt; b = Tensor([4, 5, 6]) &gt;&gt;&gt; op = EWiseAdd() &gt;&gt;&gt; result = op.compute(a, b) &gt;&gt;&gt; print(result) Tensor([5, 7, 9])</p>
<p>Create two 1-D tensors</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> Tensor([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>])</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> Tensor([<span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Create an EWiseAdd operation instance</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>op <span class="op">=</span> EWiseAdd()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Compute the element-wise sum of a and b</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> op.compute(a, b)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>result</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>minima.Tensor(
[5 7 9])</code></pre>
</div>
</div>
<p>Alternatively, you can use the add function directly</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> add(a, b)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>result</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>minima.Tensor(
[5 7 9])</code></pre>
</div>
</div>
<p>or</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>op(a,b)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>minima.Tensor(
[5 7 9])</code></pre>
</div>
</div>
<p>For 2-D tensors, we can compute the element-wise sum of a and b in the same way</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> Tensor([[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], [<span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>]])</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> Tensor([[<span class="dv">7</span>, <span class="dv">8</span>, <span class="dv">9</span>], [<span class="dv">10</span>, <span class="dv">11</span>, <span class="dv">12</span>]])</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> op.compute(a, b)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>result</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>minima.Tensor(
[[ 8 10 12]
 [14 16 18]])</code></pre>
</div>
</div>
</section>
</section>
<section id="scalar-addition" class="level2">
<h2 class="anchored" data-anchor-id="scalar-addition">Scalar Addition</h2>
<p>Explanation for the derivative of the <a href="https://m0saan.github.io/minima/operators.html#addscalar"><code>AddScalar</code></a> operator:</p>
<p>Let’s denote the scalar as <code>c</code> and <code>a</code> as the tensor being added by the scalar. The operation can be described as <code>f(a) = a + c</code>.</p>
<p>The function for the backward pass (i.e., the gradient) is <code>df/da = 1</code>, which means the derivative of <code>f(a)</code> with respect to <code>a</code> is simply <code>1</code>.</p>
<p>We are given a function <span class="math inline">\(f(a) = a + c\)</span>, where <span class="math inline">\(a\)</span> is a tensor and <span class="math inline">\(c\)</span> is a scalar. Our task is to find the derivative of this function with respect to <span class="math inline">\(a\)</span>.</p>
<p>By differentiating the function <span class="math inline">\(f(a)\)</span> with respect to <span class="math inline">\(a\)</span>, we find:</p>
<p><span class="math display">\[\begin{align*}
\frac{df}{da} &amp;= \frac{d}{da} (a + c) \\
&amp;= 1
\end{align*}\]</span></p>
<p>Therefore, the gradient of <span class="math inline">\(f(a)\)</span> with respect to <span class="math inline">\(a\)</span> is <span class="math inline">\(1\)</span>.</p>
<p>We starts by defining the function <code>f(a) = a + c</code>. It then explains that when we differentiate <code>f(a)</code> with respect to <code>a</code>, we find that the derivative is <code>1</code>. This means that the gradient of <code>f(a)</code> with respect to <code>a</code> is <code>1</code>, which matches the behavior of the <a href="https://m0saan.github.io/minima/operators.html#addscalar"><code>AddScalar</code></a> operator as provided in the <code>gradient</code> method.</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/operators.py#L124" target="_blank" style="float:right; font-size:smaller">source</a></p>
<section id="add_scalar" class="level3">
<h3 class="anchored" data-anchor-id="add_scalar">add_scalar</h3>
<blockquote class="blockquote">
<pre><code> add_scalar (a:minima.autograd.Tensor, scalar:Union[int,float])</code></pre>
</blockquote>
<p>Adds a scalar to a tensor.</p>
<p>Args: - a: The tensor. - scalar: The scalar to add.</p>
<p>Returns: The sum of a and the scalar.</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/operators.py#L79" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="addscalar" class="level3">
<h3 class="anchored" data-anchor-id="addscalar">AddScalar</h3>
<blockquote class="blockquote">
<pre><code> AddScalar (scalar:Union[int,float])</code></pre>
</blockquote>
<p>Performs addition of a tensor and a scalar.</p>
<p>Example: &gt;&gt;&gt; a = Tensor([1, 2, 3]) &gt;&gt;&gt; op = AddScalar(5) &gt;&gt;&gt; result = op.compute(a) &gt;&gt;&gt; print(result) Tensor([6, 7, 8])</p>
</section>
</section>
<section id="element-wise-multiplication" class="level2">
<h2 class="anchored" data-anchor-id="element-wise-multiplication">Element Wise Multiplication</h2>
<p>Explanation for the derivative of the <a href="https://m0saan.github.io/minima/operators.html#ewisemul"><code>EWiseMul</code></a> (element-wise multiplication) operator:</p>
<p>Let’s denote the two input tensors as <code>a</code> and <code>b</code>. The operation can be described as <code>f(a, b) = a * b</code>, where <code>*</code> represents element-wise multiplication.</p>
<p>The function for the backward pass (i.e., the gradient) is <code>df/da = b</code> and <code>df/db = a</code>. This means that the derivative of <code>f(a, b)</code> with respect to <code>a</code> is <code>b</code>, and the derivative with respect to <code>b</code> is <code>a</code>.</p>
<p>We are given a function <span class="math inline">\(f(a, b) = a \odot b\)</span>, where <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are tensors, and <span class="math inline">\(\odot\)</span> represents element-wise multiplication. Our task is to find the derivatives of this function with respect to <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>.</p>
<p>By differentiating the function <span class="math inline">\(f(a, b)\)</span> with respect to <span class="math inline">\(a\)</span>, we find:</p>
<p><span class="math display">\[\begin{align*}
\frac{df}{da} &amp;= \frac{d}{da} (a \odot b) \\
&amp;= b
\end{align*}\]</span></p>
<p>Therefore, the gradient of <span class="math inline">\(f(a, b)\)</span> with respect to <span class="math inline">\(a\)</span> is <span class="math inline">\(b\)</span>.</p>
<p>Similarly, by differentiating the function <span class="math inline">\(f(a, b)\)</span> with respect to <span class="math inline">\(b\)</span>, we find:</p>
<p><span class="math display">\[\begin{align*}
\frac{df}{db} &amp;= \frac{d}{db} (a \odot b) \\
&amp;= a
\end{align*}\]</span></p>
<p>Therefore, the gradient of <span class="math inline">\(f(a, b)\)</span> with respect to <span class="math inline">\(b\)</span> is <span class="math inline">\(a\)</span>.</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/operators.py#L177" target="_blank" style="float:right; font-size:smaller">source</a></p>
<section id="multiply" class="level3">
<h3 class="anchored" data-anchor-id="multiply">multiply</h3>
<blockquote class="blockquote">
<pre><code> multiply (a:minima.autograd.Tensor, b:minima.autograd.Tensor)</code></pre>
</blockquote>
<p>Multiplies two tensors element-wise.</p>
<p>Args: - a: The first tensor. - b: The second tensor.</p>
<p>Returns: The element-wise product of a and b.</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/operators.py#L138" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="ewisemul" class="level3">
<h3 class="anchored" data-anchor-id="ewisemul">EWiseMul</h3>
<blockquote class="blockquote">
<pre><code> EWiseMul ()</code></pre>
</blockquote>
<p>Performs element-wise multiplication of two tensors.</p>
<p>Example: &gt;&gt;&gt; a = Tensor([1, 2, 3]) &gt;&gt;&gt; b = Tensor([4, 5, 6]) &gt;&gt;&gt; op = EWiseMul() &gt;&gt;&gt; result = op.compute(a, b) &gt;&gt;&gt; print(result) Tensor([4, 10, 18])</p>
</section>
</section>
<section id="scalar-multiplication" class="level2">
<h2 class="anchored" data-anchor-id="scalar-multiplication">Scalar Multiplication</h2>
<p>Let’s denote the scalar as <code>c</code> and <code>a</code> as the tensor being multiplied by the scalar. The operation can be described as <code>f(a) = a * c</code>.</p>
<p>The function for the backward pass (i.e., the gradient) is <code>df/da = c</code>, which means the derivative of <code>f(a)</code> with respect to <code>a</code> is <code>c</code>.</p>
<p>The LaTeX document will look as follows:</p>
<p>We are given a function <span class="math inline">\(f(a) = a \cdot c\)</span>, where <span class="math inline">\(a\)</span> is a tensor and <span class="math inline">\(c\)</span> is a scalar. Our task is to find the derivative of this function with respect to <span class="math inline">\(a\)</span>.</p>
<p>By differentiating the function <span class="math inline">\(f(a)\)</span> with respect to <span class="math inline">\(a\)</span>, we find:</p>
<p><span class="math display">\[\begin{align*}
\frac{df}{da} &amp;= \frac{d}{da} (a \cdot c) \\
&amp;= c
\end{align*}\]</span></p>
<p>Therefore, the gradient of <span class="math inline">\(f(a)\)</span> with respect to <span class="math inline">\(a\)</span> is <span class="math inline">\(c\)</span>.</p>
<p>We starts by defining the function <code>f(a) = a * c</code>. It then explains that when we differentiate <code>f(a)</code> with respect to <code>a</code>, we find that the derivative is <code>c</code>. This means that the gradient of <code>f(a)</code> with respect to <code>a</code> is <code>c</code>, which matches the behavior of the <a href="https://m0saan.github.io/minima/operators.html#mulscalar"><code>MulScalar</code></a> operator as provided in the <code>gradient</code> method.</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/operators.py#L236" target="_blank" style="float:right; font-size:smaller">source</a></p>
<section id="mul_scalar" class="level3">
<h3 class="anchored" data-anchor-id="mul_scalar">mul_scalar</h3>
<blockquote class="blockquote">
<pre><code> mul_scalar (a:minima.autograd.Tensor, scalar:Union[int,float])</code></pre>
</blockquote>
<p>Multiplies a tensor by a scalar.</p>
<p>Args: - a: The tensor. - scalar: The scalar to multiply.</p>
<p>Returns: The product of a and the scalar.</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/operators.py#L191" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="mulscalar" class="level3">
<h3 class="anchored" data-anchor-id="mulscalar">MulScalar</h3>
<blockquote class="blockquote">
<pre><code> MulScalar (scalar:Union[int,float])</code></pre>
</blockquote>
<p>Performs multiplication of a tensor and a scalar.</p>
<p>Example: &gt;&gt;&gt; a = Tensor([1, 2, 3]) &gt;&gt;&gt; op = MulScalar(5) &gt;&gt;&gt; result = op.compute(a) &gt;&gt;&gt; print(result) Tensor([5, 10, 15])</p>
</section>
</section>
<section id="element-wise-divide" class="level2">
<h2 class="anchored" data-anchor-id="element-wise-divide">Element Wise Divide</h2>
<p>The operation described here is an element-wise division of two tensors, <code>a</code> and <code>b</code>, where the operation can be described as <code>f(a, b) = a / b</code>.</p>
<p>We’ll compute the partial derivatives with respect to <code>a</code> and <code>b</code>:</p>
<ol type="1">
<li><p>The partial derivative of <code>f(a, b)</code> with respect to <code>a</code> (<code>df/da</code>) is <code>1/b</code>.</p></li>
<li><p>The partial derivative of <code>f(a, b)</code> with respect to <code>b</code> (<code>df/db</code>) is <code>-a / b^2</code>.</p></li>
</ol>
<p>We are given a function <span class="math inline">\(f(a, b) = \frac{a}{b}\)</span>, where <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are tensors. Our task is to find the partial derivatives of this function with respect to <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>.</p>
<p>Let’s start with <span class="math inline">\(\frac{\partial f}{\partial a}\)</span>:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial f}{\partial a} &amp;= \frac{\partial}{\partial a} \left(\frac{a}{b}\right) \\
&amp;= \frac{1}{b}
\end{align*}\]</span></p>
<p>Now, let’s compute <span class="math inline">\(\frac{\partial f}{\partial b}\)</span>:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial f}{\partial b} &amp;= \frac{\partial}{\partial b} \left(\frac{a}{b}\right) \\
&amp;= - \frac{a}{b^{2}}
\end{align*}\]</span></p>
<p>Here is a detailed derivative:</p>
<p>Given a function of the form <span class="math inline">\(y = \frac{u}{v}\)</span>, where both <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span> are functions of <span class="math inline">\(x\)</span>, the quotient rule of differentiation states:</p>
<p><span class="math display">\[\frac{dy}{dx} = \frac{v \cdot \frac{du}{dx} - u \cdot \frac{dv}{dx}}{v^2}\]</span></p>
<p>In our case, we’re looking at the function <span class="math inline">\(y = \frac{a}{b}\)</span>, where <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are tensors. We want to find the derivative with respect to <span class="math inline">\(b\)</span> (instead of <span class="math inline">\(x\)</span> in our general formula). So we have:</p>
<p><span class="math display">\[\frac{dy}{db} = \frac{b \cdot \frac{da}{db} - a \cdot \frac{db}{db}}{b^2}\]</span></p>
<p>Since <span class="math inline">\(a\)</span> does not depend on <span class="math inline">\(b\)</span>, <span class="math inline">\(\frac{da}{db} = 0\)</span>, and since any variable is equal to itself, <span class="math inline">\(\frac{db}{db} = 1\)</span>.</p>
<p>So the derivative <span class="math inline">\(\frac{dy}{db}\)</span> simplifies to:</p>
<p><span class="math display">\[\frac{dy}{db} = \frac{b \cdot 0 - a \cdot 1}{b^2}\]</span></p>
<p>Therefore, the derivative of <span class="math inline">\(y\)</span> with respect to <span class="math inline">\(b\)</span> is <span class="math inline">\(-\frac{a}{b^2}\)</span>.</p>
<p>Therefore, the gradient of <span class="math inline">\(f(a, b)\)</span> with respect to <span class="math inline">\(a\)</span> is <span class="math inline">\(\frac{1}{b}\)</span>, and the gradient of <span class="math inline">\(f(a, b)\)</span> with respect to <span class="math inline">\(b\)</span> is <span class="math inline">\(- \frac{a}{b^{2}}\)</span>.</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/operators.py#L293" target="_blank" style="float:right; font-size:smaller">source</a></p>
<section id="divide" class="level3">
<h3 class="anchored" data-anchor-id="divide">divide</h3>
<blockquote class="blockquote">
<pre><code> divide (a:minima.autograd.Tensor, b:minima.autograd.Tensor)</code></pre>
</blockquote>
<p>Divides two tensors element-wise.</p>
<p>Args: a (Tensor): The dividend tensor. b (Tensor): The divisor tensor.</p>
<p>Returns: Tensor: The resulting tensor after element-wise division.</p>
<p>Example: &gt;&gt;&gt; import numpy as np &gt;&gt;&gt; a = Tensor(np.array([1, 2, 3])) &gt;&gt;&gt; b = Tensor(np.array([4, 5, 6])) &gt;&gt;&gt; result = divide(a, b) &gt;&gt;&gt; print(result) Tensor([0.25, 0.4, 0.5])</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/operators.py#L250" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="ewisediv" class="level3">
<h3 class="anchored" data-anchor-id="ewisediv">EWiseDiv</h3>
<blockquote class="blockquote">
<pre><code> EWiseDiv ()</code></pre>
</blockquote>
<p>The EWiseDiv operation divides two tensors element-wise.</p>
<p>Example: &gt;&gt;&gt; import numpy as np &gt;&gt;&gt; a = Tensor(np.array([1, 2, 3])) &gt;&gt;&gt; b = Tensor(np.array([4, 5, 6])) &gt;&gt;&gt; div = EWiseDiv() &gt;&gt;&gt; result = div.compute(a.data, b.data) &gt;&gt;&gt; print(result) array([0.25, 0.4, 0.5])</p>
</section>
</section>
<section id="scalar-division" class="level2">
<h2 class="anchored" data-anchor-id="scalar-division">Scalar Division</h2>
<p>Let’s denote the scalar as <code>c</code>, and <code>a</code> as the tensor being divided by the scalar. The operation can be described as <code>f(a) = a / c</code>.</p>
<p>The function for the backward pass (i.e., the gradient) is <code>df/da = 1/c</code>.</p>
<p>This is the derivative of <code>f(a)</code> with respect to <code>a</code>.</p>
<p>We are given a function <span class="math inline">\(f(a) = \frac{a}{c}\)</span>, where <span class="math inline">\(a\)</span> is a tensor and <span class="math inline">\(c\)</span> is a scalar. Our task is to find the derivative of this function with respect to <span class="math inline">\(a\)</span>.</p>
<p>By using the power rule of differentiation, where the derivative of <span class="math inline">\(a^n\)</span> is <span class="math inline">\(n \cdot a^{n-1}\)</span>, we can rewrite <span class="math inline">\(f(a)\)</span> as <span class="math inline">\(f(a) = c^{-1}a\)</span>.</p>
<p>Now, we can differentiate this with respect to <span class="math inline">\(a\)</span>:</p>
<p><span class="math display">\[\begin{align*}
\frac{df}{da} &amp;= \frac{d}{da} (c^{-1}a) \\
&amp;= c^{-1} \frac{d}{da} (a) \\
&amp;= c^{-1} \\
&amp;= \frac{1}{c}
\end{align*}\]</span></p>
<p>Therefore, the gradient of <span class="math inline">\(f(a)\)</span> with respect to <span class="math inline">\(a\)</span> is <span class="math inline">\(\frac{1}{c}\)</span>.</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/operators.py#L365" target="_blank" style="float:right; font-size:smaller">source</a></p>
<section id="divide_scalar" class="level3">
<h3 class="anchored" data-anchor-id="divide_scalar">divide_scalar</h3>
<blockquote class="blockquote">
<pre><code> divide_scalar (a:minima.autograd.Tensor, scalar:Union[int,float])</code></pre>
</blockquote>
<p>Divides a tensor by a scalar.</p>
<p>Args: a (Tensor): The tensor to divide. scalar (int, float): The scalar to divide the tensor by.</p>
<p>Returns: Tensor: The resulting tensor after division.</p>
<p>Example: &gt;&gt;&gt; import numpy as np &gt;&gt;&gt; a = Tensor(np.array([1, 2, 3])) &gt;&gt;&gt; scalar = 2 &gt;&gt;&gt; result = divide_scalar(a, scalar) &gt;&gt;&gt; print(result) Tensor([0.5, 1.0, 1.5])</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/operators.py#L316" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="divscalar" class="level3">
<h3 class="anchored" data-anchor-id="divscalar">DivScalar</h3>
<blockquote class="blockquote">
<pre><code> DivScalar (scalar:Union[int,float])</code></pre>
</blockquote>
<p>The DivScalar operation divides a tensor by a scalar.</p>
<p>Example: &gt;&gt;&gt; import numpy as np &gt;&gt;&gt; a = Tensor(np.array([1, 2, 3])) &gt;&gt;&gt; scalar = 2 &gt;&gt;&gt; div_scalar = DivScalar(scalar) &gt;&gt;&gt; result = div_scalar.compute(a.data) &gt;&gt;&gt; print(result) array([0.5, 1.0, 1.5])</p>
</section>
</section>
<section id="negation" class="level2">
<h2 class="anchored" data-anchor-id="negation">Negation</h2>
<p>Let’s denote <code>a</code> as the tensor being negated. The operation can be described as <code>f(a) = -a</code>.</p>
<p>The function for the backward pass (i.e., the gradient) is <code>df/da = -1</code>.</p>
<p>We are given a function <span class="math inline">\(f(a) = -a\)</span>, where <span class="math inline">\(a\)</span> is a tensor. Our task is to find the derivative of this function with respect to <span class="math inline">\(a\)</span>.</p>
<p>By differentiating the function <span class="math inline">\(f(a)\)</span> with respect to <span class="math inline">\(a\)</span>, we find:</p>
<p><span class="math display">\[\begin{align*}
\frac{df}{da} &amp;= \frac{d}{da} (-a) \\
&amp;= -1
\end{align*}\]</span></p>
<p>Therefore, the gradient of <span class="math inline">\(f(a)\)</span> with respect to <span class="math inline">\(a\)</span> is <span class="math inline">\(-1\)</span>.</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/operators.py#L425" target="_blank" style="float:right; font-size:smaller">source</a></p>
<section id="negate" class="level3">
<h3 class="anchored" data-anchor-id="negate">negate</h3>
<blockquote class="blockquote">
<pre><code> negate (a:minima.autograd.Tensor)</code></pre>
</blockquote>
<p>Negates the given tensor.</p>
<p>Args: - a: The tensor to negate.</p>
<p>Returns: The negation of a.</p>
<p>Example: &gt;&gt;&gt; a = Tensor([1, -2, 3]) &gt;&gt;&gt; result = negate(a) &gt;&gt;&gt; print(result) Tensor([-1, 2, -3])</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/operators.py#L387" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="negate-1" class="level3">
<h3 class="anchored" data-anchor-id="negate-1">Negate</h3>
<blockquote class="blockquote">
<pre><code> Negate ()</code></pre>
</blockquote>
<p>Negates the given tensor.</p>
<p>Example: &gt;&gt;&gt; a = Tensor([1, -2, 3]) &gt;&gt;&gt; op = Negate() &gt;&gt;&gt; result = op.compute(a) &gt;&gt;&gt; print(result) Tensor([-1, 2, -3])</p>
</section>
</section>
<section id="exp" class="level2">
<h2 class="anchored" data-anchor-id="exp">Exp</h2>
<p>Explanation for the derivative of the <a href="https://m0saan.github.io/minima/operators.html#exp"><code>Exp</code></a> operator:</p>
<p>Let’s denote <code>a</code> as the tensor on which the exponential function is applied. The operation can be described as <code>f(a) = exp(a)</code>, where <a href="https://m0saan.github.io/minima/operators.html#exp"><code>exp</code></a> represents the exponential function.</p>
<p>The function for the backward pass (i.e., the gradient) is <code>df/da = exp(a)</code>.</p>
<p>We are given a function <span class="math inline">\(f(a) = \exp(a)\)</span>, where <span class="math inline">\(a\)</span> is a tensor. Our task is to find the derivative of this function with respect to <span class="math inline">\(a\)</span>.</p>
<p>By differentiating the function <span class="math inline">\(f(a)\)</span> with respect to <span class="math inline">\(a\)</span>, we find:</p>
<p><span class="math display">\[\begin{align*}
\frac{df}{da} &amp;= \frac{d}{da} (\exp(a)) \\
&amp;= \exp(a)
\end{align*}\]</span></p>
<p>Therefore, the gradient of <span class="math inline">\(f(a)\)</span> with respect to <span class="math inline">\(a\)</span> is <span class="math inline">\(\exp(a)\)</span>.</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/operators.py#L482" target="_blank" style="float:right; font-size:smaller">source</a></p>
<section id="exp-1" class="level3">
<h3 class="anchored" data-anchor-id="exp-1">exp</h3>
<blockquote class="blockquote">
<pre><code> exp (a:minima.autograd.Tensor)</code></pre>
</blockquote>
<p>Calculates the exponential of the given tensor.</p>
<p>Args: - a: The tensor.</p>
<p>Returns: The exponential of a.</p>
<p>Example: &gt;&gt;&gt; a = Tensor([1, 2, 3]) &gt;&gt;&gt; result = exp(a) &gt;&gt;&gt; print(result) Tensor([2.71828183, 7.3890561, 20.08553692])</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/operators.py#L444" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="exp-2" class="level3">
<h3 class="anchored" data-anchor-id="exp-2">Exp</h3>
<blockquote class="blockquote">
<pre><code> Exp ()</code></pre>
</blockquote>
<p>Calculates the exponential of the given tensor.</p>
<p>Example: &gt;&gt;&gt; a = Tensor([1, 2, 3]) &gt;&gt;&gt; op = Exp() &gt;&gt;&gt; result = op.compute(a) &gt;&gt;&gt; print(result) Tensor([2.71828183, 7.3890561, 20.08553692])</p>
</section>
</section>
<section id="relu" class="level2">
<h2 class="anchored" data-anchor-id="relu">ReLU</h2>
<p>The derivative of the <a href="https://m0saan.github.io/minima/operators.html#relu"><code>ReLU</code></a> (Rectified Linear Unit) operator:</p>
<p>Let’s denote <code>a</code> as the tensor on which the ReLU function is applied. The ReLU function is defined as follows:</p>
<p><span class="math display">\[
f(a) =
\begin{cases}
a, &amp; \text{if } a \geq 0 \\
0, &amp; \text{if } a &lt; 0
\end{cases}
\]</span></p>
<p>The function for the backward pass (i.e., the gradient) is <code>df/da = 1</code> if <code>a &gt;= 0</code>, and <code>df/da = 0</code> if <code>a &lt; 0</code>.</p>
<p>We are given a function <span class="math inline">\(f(a) = \max(0, a)\)</span>, where <span class="math inline">\(a\)</span> is a tensor. Our task is to find the derivative of this function with respect to <span class="math inline">\(a\)</span>.</p>
<p>By considering the definition of the ReLU function, we can write <span class="math inline">\(f(a)\)</span> as:</p>
<p><span class="math display">\[
f(a) =
\begin{cases}
a, &amp; \text{if } a \geq 0 \\
0, &amp; \text{if } a &lt; 0
\end{cases}
\]</span></p>
<p>Now, let’s differentiate <span class="math inline">\(f(a)\)</span> with respect to <span class="math inline">\(a\)</span>:</p>
<p><span class="math display">\[
\frac{df}{da} =
\begin{cases}
1, &amp; \text{if } a \geq 0 \\
0, &amp; \text{if } a &lt; 0
\end{cases}
\]</span></p>
<p>Therefore, the gradient of <span class="math inline">\(f(a)\)</span> with respect to <span class="math inline">\(a\)</span> is <span class="math inline">\(1\)</span> if <span class="math inline">\(a \geq 0\)</span>, and <span class="math inline">\(0\)</span> if <span class="math inline">\(a &lt; 0\)</span>.</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/operators.py#L540" target="_blank" style="float:right; font-size:smaller">source</a></p>
<section id="relu-1" class="level3">
<h3 class="anchored" data-anchor-id="relu-1">relu</h3>
<blockquote class="blockquote">
<pre><code> relu (a:minima.autograd.Tensor)</code></pre>
</blockquote>
<p>Applies the ReLU (Rectified Linear Unit) activation function to the given tensor.</p>
<p>Args: - a: The tensor.</p>
<p>Returns: The result of applying ReLU to a.</p>
<p>Example: &gt;&gt;&gt; a = Tensor([1, -2, 3]) &gt;&gt;&gt; result = relu(a) &gt;&gt;&gt; print(result) Tensor([1, 0, 3])</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/operators.py#L501" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="relu-2" class="level3">
<h3 class="anchored" data-anchor-id="relu-2">ReLU</h3>
<blockquote class="blockquote">
<pre><code> ReLU ()</code></pre>
</blockquote>
<p>Applies the ReLU (Rectified Linear Unit) activation function to the given tensor.</p>
<p>Example: &gt;&gt;&gt; a = Tensor([1, -2, 3]) &gt;&gt;&gt; op = ReLU() &gt;&gt;&gt; result = op.compute(a) &gt;&gt;&gt; print(result) Tensor([1, 0, 3])</p>
</section>
</section>
<section id="power-scalar" class="level2">
<h2 class="anchored" data-anchor-id="power-scalar">Power Scalar</h2>
<p>The derivative of the <a href="https://m0saan.github.io/minima/operators.html#powerscalar"><code>PowerScalar</code></a> operator:</p>
<p>Let’s denote the scalar as <code>n</code> and <code>a</code> as the tensor being raised to the power of the scalar. The operation can be described as <code>f(a) = a^n</code>.</p>
<p>The function for the backward pass (i.e., the gradient) is <code>df/da = n * a^(n-1)</code>.</p>
<p>We are given a function <span class="math inline">\(f(a) = a^n\)</span>, where <span class="math inline">\(a\)</span> is a tensor and <span class="math inline">\(n\)</span> is a scalar. Our task is to find the derivative of this function with respect to <span class="math inline">\(a\)</span>.</p>
<p>By differentiating the function <span class="math inline">\(f(a)\)</span> with respect to <span class="math inline">\(a\)</span>, we find:</p>
<p><span class="math display">\[\begin{align*}
\frac{df}{da} &amp;= \frac{d}{da} (a^n) \\
&amp;= n \cdot a^{n-1}
\end{align*}\]</span></p>
<p>Therefore, the gradient of <span class="math inline">\(f(a)\)</span> with respect to <span class="math inline">\(a\)</span> is <span class="math inline">\(n \cdot a^{n-1}\)</span>.</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/operators.py#L613" target="_blank" style="float:right; font-size:smaller">source</a></p>
<section id="power_scalar" class="level3">
<h3 class="anchored" data-anchor-id="power_scalar">power_scalar</h3>
<blockquote class="blockquote">
<pre><code> power_scalar (a:minima.autograd.Tensor, scalar:int)</code></pre>
</blockquote>
<p>Raises a tensor to a power.</p>
<p>Args: a (Tensor): The input tensor. scalar (int): The power to raise the tensor to.</p>
<p>Returns: Tensor: The resulting tensor after the power operation.</p>
<p>Example: &gt;&gt;&gt; import numpy as np &gt;&gt;&gt; tensor = Tensor(np.array([1, 2, 3])) &gt;&gt;&gt; result = power_scalar(tensor, 2) &gt;&gt;&gt; print(result) Tensor([1, 4, 9])</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/operators.py#L560" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="powerscalar" class="level3">
<h3 class="anchored" data-anchor-id="powerscalar">PowerScalar</h3>
<blockquote class="blockquote">
<pre><code> PowerScalar (scalar:int)</code></pre>
</blockquote>
<p>The PowerScalar operation raises a tensor to an (integer) power.</p>
<p>Attributes: scalar (int): The power to raise the tensor to.</p>
<p>Example: &gt;&gt;&gt; import numpy as np &gt;&gt;&gt; tensor = Tensor(np.array([1, 2, 3])) &gt;&gt;&gt; pow_scalar = PowerScalar(2) &gt;&gt;&gt; result = pow_scalar.compute(tensor.data) &gt;&gt;&gt; print(result) array([1, 4, 9])</p>
</section>
</section>
<section id="log" class="level2">
<h2 class="anchored" data-anchor-id="log">Log</h2>
<p>Explanation for the derivative of the <code>Log</code> operator:</p>
<p>Let’s denote <code>a</code> as the tensor on which the logarithm is applied. The operation can be described as <code>f(a) = log(a)</code>, where <code>log</code> represents the natural logarithm.</p>
<p>The function for the backward pass (i.e., the gradient) is <code>df/da = 1/a</code>.</p>
<p>We are given a function <span class="math inline">\(f(a) = \log(a)\)</span>, where <span class="math inline">\(a\)</span> is a tensor. Our task is to find the derivative of this function with respect to <span class="math inline">\(a\)</span>.</p>
<p>By differentiating the function <span class="math inline">\(f(a)\)</span> with respect to <span class="math inline">\(a\)</span>, we find:</p>
<p><span class="math display">\[\begin{align*}
\frac{df}{da} &amp;= \frac{d}{da} (\log(a)) \\
&amp;= \frac{1}{a}
\end{align*}\]</span></p>
<p>We started by defining the function <code>f(a) = log(a)</code>, where <code>log</code> represents the natural logarithm. It then explains that when we differentiate <code>f(a)</code> with respect to <code>a</code>, we find that the derivative is <code>1/a</code>. This means that the gradient of <code>f(a)</code> with respect to <code>a</code> is <code>1/a</code>, which represents the behavior of the <code>Log</code> operator.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Log(TensorOp):</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="co">    The Log operation applies the natural logarithm element-wise on the tensor.</span></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Example:</span></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; import numpy as np</span></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; a = Tensor(np.array([1.0, 2.0, 3.0]))</span></span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; log_op = Log()</span></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; result = log_op.compute(a.data)</span></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; print(result)</span></span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a><span class="co">        array([0., 0.69314718, 1.09861229])</span></span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> compute(<span class="va">self</span>, a: NDArray) <span class="op">-&gt;</span> NDArray:</span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a><span class="co">        Applies the natural logarithm to the tensor.</span></span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a><span class="co">            a (NDArray): The input tensor.</span></span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a><span class="co">            NDArray: The resulting tensor after applying the natural logarithm.</span></span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb31-24"><a href="#cb31-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> ARRAY_API.log(a)</span>
<span id="cb31-25"><a href="#cb31-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-26"><a href="#cb31-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> gradient(<span class="va">self</span>, out_grad: Tensor, node: Tensor) <span class="op">-&gt;</span> Tuple[Tensor, ...]:</span>
<span id="cb31-27"><a href="#cb31-27" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb31-28"><a href="#cb31-28" aria-hidden="true" tabindex="-1"></a><span class="co">        Computes the gradient of the log operation.</span></span>
<span id="cb31-29"><a href="#cb31-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-30"><a href="#cb31-30" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb31-31"><a href="#cb31-31" aria-hidden="true" tabindex="-1"></a><span class="co">            out_grad (Tensor): The gradient of the output tensor.</span></span>
<span id="cb31-32"><a href="#cb31-32" aria-hidden="true" tabindex="-1"></a><span class="co">            node (Tensor): The node in the computational graph where the operation was performed.</span></span>
<span id="cb31-33"><a href="#cb31-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-34"><a href="#cb31-34" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb31-35"><a href="#cb31-35" aria-hidden="true" tabindex="-1"></a><span class="co">            Tuple[Tensor, ...]: The gradient with respect to the input tensor.</span></span>
<span id="cb31-36"><a href="#cb31-36" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb31-37"><a href="#cb31-37" aria-hidden="true" tabindex="-1"></a>        a <span class="op">=</span> node.children[<span class="dv">0</span>]</span>
<span id="cb31-38"><a href="#cb31-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (out_grad <span class="op">/</span> a, )</span>
<span id="cb31-39"><a href="#cb31-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-40"><a href="#cb31-40" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> log(a: Tensor) <span class="op">-&gt;</span> Tensor:</span>
<span id="cb31-41"><a href="#cb31-41" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb31-42"><a href="#cb31-42" aria-hidden="true" tabindex="-1"></a><span class="co">    Applies the natural logarithm to the tensor.</span></span>
<span id="cb31-43"><a href="#cb31-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-44"><a href="#cb31-44" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb31-45"><a href="#cb31-45" aria-hidden="true" tabindex="-1"></a><span class="co">        a (Tensor): The input tensor.</span></span>
<span id="cb31-46"><a href="#cb31-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-47"><a href="#cb31-47" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb31-48"><a href="#cb31-48" aria-hidden="true" tabindex="-1"></a><span class="co">        Tensor: The resulting tensor after applying the natural logarithm.</span></span>
<span id="cb31-49"><a href="#cb31-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-50"><a href="#cb31-50" aria-hidden="true" tabindex="-1"></a><span class="co">    Example:</span></span>
<span id="cb31-51"><a href="#cb31-51" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; import numpy as np</span></span>
<span id="cb31-52"><a href="#cb31-52" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; a = Tensor(np.array([1.0, 2.0, 3.0]))</span></span>
<span id="cb31-53"><a href="#cb31-53" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; result = log(a)</span></span>
<span id="cb31-54"><a href="#cb31-54" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; print(result)</span></span>
<span id="cb31-55"><a href="#cb31-55" aria-hidden="true" tabindex="-1"></a><span class="co">        Tensor([0., 0.69314718, 1.09861229])</span></span>
<span id="cb31-56"><a href="#cb31-56" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb31-57"><a href="#cb31-57" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Log()(a)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="transpose" class="level2">
<h2 class="anchored" data-anchor-id="transpose">Transpose</h2>
<p>This operation described here is the derivative of a transposition operation. Let’s define our transposition operation as a function <code>f</code> such that <code>f(a) = a^T</code> where <code>a</code> is a tensor, and <code>a^T</code> is the transpose of tensor <code>a</code>.</p>
<p>The goal here is to compute the derivative of this operation with respect to <code>a</code>. It’s important to note that transposition operation doesn’t change the values of the tensor’s elements, but it just rearranges their positions. This implies that the gradient (derivative) of a transposed tensor is simply the transposed gradient of the original tensor.</p>
<p>Let’s denote the gradient of the transposed tensor as <code>g</code>, which can be mathematically represented as <code>g = df/da</code>, where <code>df/da</code> is the derivative of <code>f(a)</code> with respect to <code>a</code>.</p>
<p>Given this understanding, we can make an important conclusion:</p>
<ol type="1">
<li>The derivative of <code>f(a)</code> with respect to <code>a</code> is <code>df/da = g^T</code>, meaning that the derivative of the transposed tensor is simply the transposed gradient of the original tensor.</li>
</ol>
<p>This concept can be written in mathematical terms using LaTeX as follows:</p>
<p>We have a function <span class="math inline">\(f(a) = a^T\)</span>, where <span class="math inline">\(a\)</span> is a tensor and <span class="math inline">\(a^T\)</span> is its transpose. We want to find the derivative of this function with respect to <span class="math inline">\(a\)</span>, that is, compute <span class="math inline">\(\frac{df}{da}\)</span>.</p>
<p><span class="math display">\[\begin{align*}
\frac{df}{da} &amp;= \frac{d}{da} (a^T) \\
&amp;= (g)^T
\end{align*}\]</span></p>
<p>In the equation above, <span class="math inline">\(g\)</span> is the gradient of the transposed tensor. This equation indicates that the derivative of the transpose of a tensor is the transpose of the gradient of the original tensor.</p>
<p>Now, if we consider a Python class <a href="https://m0saan.github.io/minima/operators.html#transpose"><code>Transpose</code></a> that implements this transposition operation, we would have a <code>gradient</code> method in the class that computes the derivative of the transpose operation. This method would apply the transpose function to <code>out_grad</code>, which represents the gradient of the output tensor, thereby giving us the transposed gradient of the original tensor. In the code, <code>transpose(out_grad, axes=self.axes)</code> performs the transposition of <code>out_grad</code> along the same axes that were used in the forward pass. Thus, the gradient of the transposition operation with respect to the input tensor <code>a</code> is computed as the transpose of the output gradient <code>out_grad</code>.</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/operators.py#L688" target="_blank" style="float:right; font-size:smaller">source</a></p>
<section id="transpose-1" class="level3">
<h3 class="anchored" data-anchor-id="transpose-1">transpose</h3>
<blockquote class="blockquote">
<pre><code> transpose (a:minima.autograd.Tensor, axes:Optional[tuple]=None)</code></pre>
</blockquote>
<p>Perform the transpose operation on the input tensor along the specified axes. If no axes are specified, it swaps the last two dimensions of the input tensor.</p>
<p>Args: a (Tensor): The input tensor. axes (Optional[tuple]): The pair of axes that should be swapped. If not provided, the last two axes are swapped.</p>
<p>Returns: Tensor: The transposed tensor.</p>
<p>Example: &gt;&gt;&gt; a = Tensor(np.arange(1, 7).reshape(2, 3)) &gt;&gt;&gt; result = transpose(a) &gt;&gt;&gt; print(result) Tensor([[1, 4], [2, 5], [3, 6]])</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/operators.py#L634" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="transpose-2" class="level3">
<h3 class="anchored" data-anchor-id="transpose-2">Transpose</h3>
<blockquote class="blockquote">
<pre><code> Transpose (axes:Optional[tuple]=None)</code></pre>
</blockquote>
<p>Tensor operation class that performs transposition of a tensor along specified axes.</p>
<p>If no axes are specified, it swaps the last two dimensions of the input tensor.</p>
<p>Example: &gt;&gt;&gt; a = Tensor(np.arange(1, 7).reshape(2, 3)) &gt;&gt;&gt; op = Transpose() &gt;&gt;&gt; result = op.compute(a.data) &gt;&gt;&gt; print(result) array([[1, 4], [2, 5], [3, 6]])</p>
</section>
</section>
<section id="reshape" class="level2">
<h2 class="anchored" data-anchor-id="reshape">Reshape</h2>
<p>The operation described here is a reshaping of a tensor <code>a</code>, where the operation can be described as <code>f(a) = reshape(a, new_shape)</code>.</p>
<p>We’ll compute the derivative of this operation.</p>
<p>The reshaping operation doesn’t change the values of the tensor elements but only rearranges them. This means that the gradient of a reshaped tensor is just the reshaped gradient of the original tensor.</p>
<p>Let’s denote the gradient of the reshaped tensor as <code>g = df/da</code>, where <code>f(a) = reshape(a, new_shape)</code>.</p>
<p>Given this, we can derive the following:</p>
<ol type="1">
<li>The derivative of <code>f(a)</code> with respect to <code>a</code> is <code>df/da = reshape(g, original_shape)</code>.</li>
</ol>
<p>This conclusion can be illustrated as follows in Latex:</p>
<p>We are given a function <span class="math inline">\(f(a) = reshape(a, new\_shape)\)</span>, where <span class="math inline">\(a\)</span> is a tensor and <code>reshape(a, new_shape)</code> is the reshaped tensor. Our task is to find the derivative of this function with respect to <span class="math inline">\(a\)</span>.</p>
<p>Let’s compute <span class="math inline">\(\frac{df}{da}\)</span>:</p>
<p><span class="math display">\[\begin{align*}
\frac{df}{da} &amp;= \frac{d}{da} (reshape(a, new\_shape)) \\
&amp;= reshape(g, original\_shape)
\end{align*}\]</span></p>
<p>Here, <span class="math inline">\(g\)</span> is the gradient of the reshaped tensor. The derivative of a reshaped tensor is the reshaped derivative of the original tensor. The reshaped derivative has the same shape as the original tensor.</p>
<p>Now, let’s apply this to the <a href="https://m0saan.github.io/minima/operators.html#reshape"><code>Reshape</code></a> class.</p>
<p>The <code>gradient</code> method in the <a href="https://m0saan.github.io/minima/operators.html#reshape"><code>Reshape</code></a> class computes the gradient of the reshape operation. The gradient of the reshaped tensor is just the reshaped gradient of the original tensor. This is implemented by applying the <a href="https://m0saan.github.io/minima/operators.html#reshape"><code>reshape</code></a> function to <code>out_grad</code>, which is the gradient of the output tensor, and then returning this reshaped gradient. The shape used for the reshaping is the shape of the original tensor, which is obtained from <code>node.children[0].shape</code>.</p>
<p>Therefore, the gradient of the reshape operation with respect to the input tensor <code>a</code> is the reshaping of the output gradient <code>out_grad</code> to the shape of the original tensor.</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/operators.py#L759" target="_blank" style="float:right; font-size:smaller">source</a></p>
<section id="reshape-1" class="level3">
<h3 class="anchored" data-anchor-id="reshape-1">reshape</h3>
<blockquote class="blockquote">
<pre><code> reshape (a:minima.autograd.Tensor, shape:Tuple[int,...])</code></pre>
</blockquote>
<p>Reshape the input tensor to the specified shape.</p>
<p>Args: a (Tensor): The input tensor. shape (Tuple[int, …]): The desired shape of the output tensor.</p>
<p>Returns: Tensor: The reshaped tensor.</p>
<p>Example: &gt;&gt;&gt; a = Tensor([1, 2, 3, 4, 5, 6]) &gt;&gt;&gt; result = reshape(a, (2, 3)) &gt;&gt;&gt; print(result) Tensor([[1, 2, 3], [4, 5, 6]])</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/operators.py#L712" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="reshape-2" class="level3">
<h3 class="anchored" data-anchor-id="reshape-2">Reshape</h3>
<blockquote class="blockquote">
<pre><code> Reshape (shape:Tuple[int,...])</code></pre>
</blockquote>
<p>Tensor operation class that reshapes a tensor.</p>
<p>Example: &gt;&gt;&gt; a = Tensor([1, 2, 3, 4, 5, 6]) &gt;&gt;&gt; op = Reshape((2, 3)) &gt;&gt;&gt; result = op.compute(a) &gt;&gt;&gt; print(result) Tensor([[1, 2, 3], [4, 5, 6]])</p>
</section>
</section>
<section id="matrix-multiplication" class="level2">
<h2 class="anchored" data-anchor-id="matrix-multiplication">Matrix Multiplication</h2>
<p>Matrix multiplication, often denoted by “matmul” in some programming languages, refers to the process of multiplying two matrices together. However, in the context of calculus, it’s more common to talk about the derivative of a function.</p>
<p>When dealing with matrices, instead of talking about derivatives, we often discuss the Jacobian, which is a matrix of partial derivatives. If you have a function that takes a matrix as input and produces a scalar output, you could compute a gradient, which would be a matrix of the same shape as the input matrix.</p>
<p>However, in the context of deep learning and backpropagation, you might be asking about the derivative of a matrix multiplication operation with respect to its inputs. This is often needed when you’re training a neural network, because you need to compute gradients to update the weights.</p>
<p>Let’s denote the matrices as <code>A</code> and <code>B</code>, where <code>A</code> is a matrix of dimension <code>m x n</code> and <code>B</code> is a matrix of dimension <code>n x p</code>, and the result of the multiplication <code>C = A * B</code> is a matrix of dimension <code>m x p</code>.</p>
<p>If we are to compute the derivative of <code>C</code> with respect to <code>A</code> (i.e., ∂C/∂A), each element in <code>A</code> affects all elements in its corresponding row in <code>C</code>.</p>
<p>Similarly, if we are to compute the derivative of <code>C</code> with respect to <code>B</code> (i.e., ∂C/∂B), each element in <code>B</code> affects all elements in its corresponding column in <code>C</code>.</p>
<p>In actual computation, if we have a scalar-valued loss function <code>L</code>, we would compute the gradient of <code>L</code> with respect to <code>A</code> (denoted as ∂L/∂A), which is the same shape as <code>A</code>. To compute this, we need to know the gradient of <code>L</code> with respect to <code>C</code> (denoted as ∂L/∂C), then:</p>
<p>∂L/∂A = (∂L/∂C) * B^T (where * denotes matrix multiplication and B^T is the transpose of B)</p>
<p>Similarly, to compute the gradient of <code>L</code> with respect to <code>B</code> (denoted as ∂L/∂B):</p>
<p>∂L/∂B = A^T * (∂L/∂C)</p>
<p>The line <code>axes_to_sum_over = tuple(range(len(out_shape) - len(lhs_shape)))</code> is calculating which axes (dimensions) of the output gradient tensor (<code>out_grad</code>) need to be summed over when computing the gradient with respect to the left-hand side (<code>a</code>) input tensor.</p>
<p>This is necessary when the rank (number of dimensions) of <code>out_grad</code> is larger than the rank of <code>a</code>. This can happen, for instance, when <code>a</code> is a matrix (2D tensor) and <code>out_grad</code> is a 3D tensor (which can result from batched matrix multiplication).</p>
<p>The <code>range</code> function generates a sequence of integers from 0 up to (but not including) <code>len(out_shape) - len(lhs_shape)</code>. The <code>tuple</code> function then takes this sequence and turns it into a tuple. The result is a tuple of integers representing the axes to sum over.</p>
<p>Here is a concrete example:</p>
<p>Suppose we have a batched matrix multiplication where <code>A</code> is a matrix of shape <code>(m, n)</code>, and <code>out_grad</code> is a 3D tensor of shape <code>(b, m, n)</code>, where <code>b</code> is the batch size.</p>
<p>In this case, <code>len(out_shape) - len(a_shape)</code> equals <code>1</code>, so <code>range(len(out_shape) - len(lhs_shape))</code> generates a sequence of integers from <code>0</code> to <code>1</code> (not inclusive), which is just <code>[0]</code>.</p>
<p>So <code>axes_to_sum_over</code> will be <code>(0,)</code>, indicating that we need to sum over the first axis (the batch axis) of <code>out_grad</code> when computing the gradient with respect to <code>A</code>.</p>
<p>This summing operation effectively accumulates the individual gradients for each item in the batch into a single gradient for the <code>A</code> matrix.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Suppose we have the following shapes for `lhs` and `out_grad`</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>m, n, b <span class="op">=</span> <span class="dv">5</span>, <span class="dv">7</span>, <span class="dv">3</span></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's create some tensors with these shapes</span></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> torch.randn(m, n)          <span class="co"># lhs is a 2D tensor (matrix) of shape (m, n)</span></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>out_grad <span class="op">=</span> torch.randn(b, m, n)  <span class="co"># out_grad is a 3D tensor of shape (b, m, n)</span></span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's say `rhs` is another matrix that was involved in computing out_grad</span></span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> torch.randn(n, m)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>out_shape, A_shape, B_shape <span class="op">=</span> out_grad.shape, A.shape, B.shape</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>out_shape, A_shape, B_shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(torch.Size([3, 5, 7]), torch.Size([5, 7]), torch.Size([7, 5]))</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(out_shape), <span class="bu">len</span>(A_shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(3, 2)</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> <span class="bu">range</span>(<span class="bu">len</span>(out_shape) <span class="op">-</span> <span class="bu">len</span>(A_shape))</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>rng</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>range(0, 1)</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="bu">tuple</span>(rng)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(0,)</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>axes_to_sum_over <span class="op">=</span> <span class="bu">tuple</span>(<span class="bu">range</span>(<span class="bu">len</span>(out_shape) <span class="op">-</span> <span class="bu">len</span>(A_shape)))</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>axes_to_sum_over</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(0,)</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>torch.<span class="bu">sum</span>(out_grad <span class="op">@</span> B, axes_to_sum_over)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[-0.1309, -1.9203, -4.4179,  2.8422, -0.4453],
        [-1.5883, -8.1020, -6.7316, -1.3045,  0.6170],
        [-0.5317,  2.3444,  1.6038, -3.5786, -0.1689],
        [ 1.0831, -1.3743,  0.8485, -3.0593,  2.2023],
        [ 0.3071,  1.8321, -3.6827, -9.4409, -1.1884]])</code></pre>
</div>
</div>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/operators.py#L847" target="_blank" style="float:right; font-size:smaller">source</a></p>
<section id="matmul" class="level3">
<h3 class="anchored" data-anchor-id="matmul">matmul</h3>
<blockquote class="blockquote">
<pre><code> matmul (a:minima.autograd.Tensor, b:minima.autograd.Tensor)</code></pre>
</blockquote>
<p>Perform matrix multiplication on two tensors.</p>
<p>Args: a (Tensor): The first input tensor. b (Tensor): The second input tensor.</p>
<p>Returns: Tensor: The product of a and b.</p>
<p>Example: &gt;&gt;&gt; a = Tensor([[1, 2], [3, 4]]) &gt;&gt;&gt; b = Tensor([[5, 6], [7, 8]]) &gt;&gt;&gt; result = matmul(a, b) &gt;&gt;&gt; print(result) Tensor([[19, 22], [43, 50]])</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/operators.py#L781" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="matmul-1" class="level3">
<h3 class="anchored" data-anchor-id="matmul-1">MatMul</h3>
<blockquote class="blockquote">
<pre><code> MatMul ()</code></pre>
</blockquote>
<p>Tensor operation class that performs matrix multiplication.</p>
<p>Example: &gt;&gt;&gt; a = Tensor([[1, 2], [3, 4]]) &gt;&gt;&gt; b = Tensor([[5, 6], [7, 8]]) &gt;&gt;&gt; op = MatMul() &gt;&gt;&gt; result = op.compute(a, b) &gt;&gt;&gt; print(result) Tensor([[19, 22], [43, 50]])</p>
</section>
</section>
<section id="summation" class="level2">
<h2 class="anchored" data-anchor-id="summation">Summation</h2>
<p>The <a href="https://m0saan.github.io/minima/operators.html#summation"><code>Summation</code></a> operation, when provided with the <code>axes</code> argument, sums over these axes and thereby reduces the rank of the tensor by the number of axes summed over. The backward pass needs to take this into account, as it needs to return a gradient tensor of the same shape as the input.</p>
<p>The forward pass (<code>compute</code> method) is straightforward - it just computes the sum over the specified axes.</p>
<p>In the backward pass (<code>gradient</code> method), the goal is to compute the gradient of the sum operation. Since every element of the input tensor contributes equally to the sum, the derivative of the sum with respect to each element is 1. However, since the sum operation may reduce the dimensionality of the tensor (when <code>axes</code> is not <code>None</code>), we need to account for this when computing the gradient.</p>
<p>To do this, we first create a new shape, where the dimensions specified by <code>axes</code> are replaced by 1. We then reshape <code>out_grad</code> to this new shape. This essentially “undoes” the dimensionality reduction performed by the sum operation. Finally, we use <a href="https://m0saan.github.io/minima/operators.html#broadcast_to"><code>broadcast_to</code></a> to make the reshaped gradient tensor the same shape as the input tensor.</p>
<p>Suppose you have the following tensor in PyTorch:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 3x3 tensor</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor([[<span class="fl">1.</span>, <span class="fl">2.</span>, <span class="fl">3.</span>], [<span class="fl">4.</span>, <span class="fl">5.</span>, <span class="fl">6.</span>], [<span class="fl">7.</span>, <span class="fl">8.</span>, <span class="fl">9.</span>]], requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Sum over axis 0</span></span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> x.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>y</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([12., 15., 18.], grad_fn=&lt;SumBackward1&gt;)</code></pre>
</div>
</div>
<p><code>y</code> is now a 1-dimensional tensor of shape <code>(3,)</code>, because we’ve summed over axis 0. If we compute the gradient of <code>y</code> with respect to <code>x</code>, we’ll want the resulting gradient tensor to have the same shape as <code>x</code>, which is <code>(3,3)</code>. However, the gradient tensor we receive during backpropagation (<code>out_grad</code>) will have the same shape as <code>y</code>, which is <code>(3,)</code>.</p>
<p>So we need to “undo” the dimensionality reduction by reshaping and broadcasting <code>out_grad</code> to match the shape of <code>x</code>. Here’s how you can do it in PyTorch:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Mock out_grad tensor</span></span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>out_grad <span class="op">=</span> torch.tensor([<span class="fl">1.</span>, <span class="fl">1.</span>, <span class="fl">1.</span>])</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Reshape out_grad to have an additional dimension</span></span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>reshaped_grad <span class="op">=</span> out_grad.reshape(<span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Broadcast the reshaped_grad to match the input shape</span></span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a>broadcasted_grad <span class="op">=</span> reshaped_grad.expand_as(x)</span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(broadcasted_grad)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]])</code></pre>
</div>
</div>
<p>Now <code>broadcasted_grad</code> has the same shape as <code>x</code>, so it can be correctly used as the gradient of <code>x</code> in further computations. This manual operation simulates what the <code>gradient</code> function of the <a href="https://m0saan.github.io/minima/operators.html#summation"><code>Summation</code></a> operation is doing in your original code.</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/operators.py#L938" target="_blank" style="float:right; font-size:smaller">source</a></p>
<section id="summation-1" class="level3">
<h3 class="anchored" data-anchor-id="summation-1">summation</h3>
<blockquote class="blockquote">
<pre><code> summation (a:minima.autograd.Tensor, axes:Optional[tuple]=None)</code></pre>
</blockquote>
<p>Computes the sum of <code>a</code> along the specified axes.</p>
<p>Args: - a: The input tensor. - axes (tuple, optional): The dimensions to reduce. If <code>None</code> (default), reduces all dimensions.</p>
<p>Returns: The sum of <code>a</code> along the specified axes.</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/operators.py#L870" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="summation-2" class="level3">
<h3 class="anchored" data-anchor-id="summation-2">Summation</h3>
<blockquote class="blockquote">
<pre><code> Summation (axes:Optional[tuple]=None)</code></pre>
</blockquote>
<p>Op to compute the sum of a tensor along specified axes.</p>
<p>Example: &gt;&gt;&gt; a = Tensor([[1, 2, 3], [4, 5, 6]]) &gt;&gt;&gt; op = Summation(axes=(0,)) &gt;&gt;&gt; result = op.compute(a) &gt;&gt;&gt; print(result) Tensor([5, 7, 9])</p>
<p>Args: - axes (tuple, optional): The dimensions to reduce. If <code>None</code> (default), reduces all dimensions.</p>
<p>Methods: - compute(a: NDArray) -&gt; NDArray: Computes the sum of <code>a</code> along the specified axes. - gradient(out_grad: Tensor, node: Tensor) -&gt; Tuple[Tensor]: Computes the gradient of the sum operation.</p>
</section>
</section>
<section id="broadcast" class="level2">
<h2 class="anchored" data-anchor-id="broadcast">Broadcast</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="co"># First, we create a tensor a, and set requires_grad = True so that we can compute gradients with respect to it</span></span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.tensor([<span class="fl">1.</span>, <span class="fl">2.</span>, <span class="fl">3.</span>], requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Now, let's define a function that performs the broadcasting operation</span></span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> broadcast_to(<span class="bu">input</span>, shape):</span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">input</span>.expand(shape)</span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a><span class="co"># We broadcast a to a larger shape</span></span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a>shape <span class="op">=</span> (<span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb58-10"><a href="#cb58-10" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> broadcast_to(a, shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>b</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[1., 2., 3.],
        [1., 2., 3.],
        [1., 2., 3.]], grad_fn=&lt;ExpandBackward0&gt;)</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>b.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([3, 3])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Then, we define an output tensor as the sum of elements in b</span></span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a><span class="co"># This is a simple function that we can differentiate, and will result in a gradient for b</span></span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> b.<span class="bu">sum</span>()</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute gradients</span></span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a>out.backward()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>a.grad</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([3., 3., 3.])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the output gradient tensor</span></span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>out_grad <span class="op">=</span> torch.tensor([[<span class="fl">1.</span>, <span class="fl">2.</span>, <span class="fl">3.</span>], [<span class="fl">1.</span>, <span class="fl">2.</span>, <span class="fl">3.</span>], [<span class="fl">1.</span>, <span class="fl">2.</span>, <span class="fl">3.</span>]])</span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a>out_grad.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([3, 3])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>a_shape <span class="op">=</span> a.shape</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>a_shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([3])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>shape <span class="op">=</span> [<span class="dv">1</span>] <span class="op">*</span> (<span class="bu">len</span>((<span class="dv">3</span>,<span class="dv">3</span>)) <span class="op">-</span> <span class="bu">len</span>((<span class="dv">3</span>,<span class="dv">3</span>))) <span class="op">+</span> <span class="bu">list</span>(a_shape)</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>[3]</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="co"># The gradient for the broadcast operation is the sum of out_grad over the dimension that was broadcasted</span></span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>grad_a <span class="op">=</span> out_grad.<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(grad_a)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([3., 6., 9.])</code></pre>
</div>
</div>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/operators.py#L1009" target="_blank" style="float:right; font-size:smaller">source</a></p>
<section id="broadcast_to" class="level3">
<h3 class="anchored" data-anchor-id="broadcast_to">broadcast_to</h3>
<blockquote class="blockquote">
<pre><code> broadcast_to (a:minima.autograd.Tensor, shape:Tuple[int,...])</code></pre>
</blockquote>
<p>Broadcasts <code>a</code> to the specified shape.</p>
<p>Args: - a: The input tensor. - shape: The new shape to broadcast the input tensor to.</p>
<p>Returns: The tensor <code>a</code> broadcasted to the specified shape.</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/operators.py#L953" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="broadcastto" class="level3">
<h3 class="anchored" data-anchor-id="broadcastto">BroadcastTo</h3>
<blockquote class="blockquote">
<pre><code> BroadcastTo (shape)</code></pre>
</blockquote>
<p>Op to broadcast a tensor to a new shape.</p>
<p>Example: &gt;&gt;&gt; a = Tensor([1, 2, 3]) &gt;&gt;&gt; op = BroadcastTo((3, 3)) &gt;&gt;&gt; result = op.compute(a) &gt;&gt;&gt; print(result) Tensor([[1, 2, 3], [1, 2, 3], [1, 2, 3]])</p>
<p>Args: - shape (tuple): The new shape to broadcast the input tensor to.</p>
<p>Methods: - compute(a: NDArray) -&gt; NDArray: Broadcasts <code>a</code> to the specified shape. - gradient(out_grad: Tensor, node: Tensor) -&gt; Tuple[Tensor]: Computes the gradient of the broadcast operation.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>br <span class="op">=</span> BroadcastTo((<span class="dv">5</span>,<span class="dv">2</span>,<span class="dv">3</span>))</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> Tensor([[<span class="fl">1.</span>, <span class="fl">2.</span>, <span class="fl">3.</span>], [<span class="fl">1.</span>, <span class="fl">2.</span>, <span class="fl">3.</span>]])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>a.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(2, 3)</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a>a_br <span class="op">=</span> br.compute(a)</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>a_br.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(5, 2, 3)</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a>out_grad <span class="op">=</span> Tensor(numpy.ones_like(a_br))</span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a>out_grad.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(5, 2, 3)</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a>out_grad</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>minima.Tensor(
[[[1 1 1]
  [1 1 1]]

 [[1 1 1]
  [1 1 1]]

 [[1 1 1]
  [1 1 1]]

 [[1 1 1]
  [1 1 1]]

 [[1 1 1]
  [1 1 1]]])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a>a_shape <span class="op">=</span> a.shape</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a>a_shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(2, 3)</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a>shape <span class="op">=</span> [<span class="dv">1</span>] <span class="op">*</span> (<span class="bu">len</span>(br.shape) <span class="op">-</span> <span class="bu">len</span>(a_shape)) <span class="op">+</span> <span class="bu">list</span>(a_shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a>br.shape, shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>((5, 2, 3), [1, 2, 3])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a>sum_over <span class="op">=</span> <span class="bu">tuple</span>([idx <span class="cf">for</span> idx <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(br.shape)) <span class="cf">if</span> br.shape[idx] <span class="op">!=</span> shape[idx]])</span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a>sum_over</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(0,)</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb92"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a>reshape(summation(out_grad, sum_over), a_shape).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(2, 3)</code></pre>
</div>
</div>
</section>
</section>
<section id="logsumexp" class="level2">
<h2 class="anchored" data-anchor-id="logsumexp">LogSumExp</h2>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/operators.py#L1109" target="_blank" style="float:right; font-size:smaller">source</a></p>
<section id="logsumexp-1" class="level3">
<h3 class="anchored" data-anchor-id="logsumexp-1">logsumexp</h3>
<blockquote class="blockquote">
<pre><code> logsumexp (a, axes=None)</code></pre>
</blockquote>
<pre><code>/opt/hostedtoolcache/Python/3.9.17/x64/lib/python3.9/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Methods
  else: warn(msg)</code></pre>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/operators.py#L1024" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="logsumexp-2" class="level3">
<h3 class="anchored" data-anchor-id="logsumexp-2">LogSumExp</h3>
<blockquote class="blockquote">
<pre><code> LogSumExp (axes:Optional[tuple]=None)</code></pre>
</blockquote>
<p>A Tensor operation class for performing LogSumExp computation.</p>
</section>
</section>
<section id="export" class="level2">
<h2 class="anchored" data-anchor-id="export">Export</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb97"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nbdev<span class="op">;</span> nbdev.nbdev_export()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>