{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30e46e7a-8cc6-4dd5-8536-e4f17a480d58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumbers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Number\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optional, List\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograd\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NDArray\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograd\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Op, Tensor, Value, TensorOp\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograd\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TensorTuple, TensorTupleOp\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "from numbers import Number\n",
    "from typing import Optional, List\n",
    "from .autograd import NDArray\n",
    "from .autograd import Op, Tensor, Value, TensorOp\n",
    "from .autograd import TensorTuple, TensorTupleOp\n",
    "import numpy\n",
    "\n",
    "# NOTE: we will import numpy as the array_api\n",
    "# as the backend for our computations, this line will change in later homeworks\n",
    "import numpy as array_api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e1bd11-e64e-4403-812f-cbc0e81aa9b6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## AutoGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c3fd0804-dbc1-472e-a73d-70c2d58f8e28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List, Optional, NamedTuple, Tuple, Union\n",
    "from collections import namedtuple\n",
    "import numpy\n",
    "\n",
    "# needle version\n",
    "LAZY_MODE = False\n",
    "TENSOR_COUNTER = 0\n",
    "\n",
    "# NOTE: we will import numpy as the array_api\n",
    "# as the backend for our computations, this line will change in later homeworks\n",
    "import numpy as array_api\n",
    "NDArray = numpy.ndarray\n",
    "\n",
    "\n",
    "class Device:\n",
    "    \"\"\"Indicates the device supporting an NDArray.\"\"\"\n",
    "\n",
    "\n",
    "class CPUDevice(Device):\n",
    "    \"\"\"Represents data that sits in CPU\"\"\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"needle.cpu()\"\n",
    "\n",
    "    def __hash__(self):\n",
    "        return self.__repr__().__hash__()\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return isinstance(other, CPUDevice)\n",
    "\n",
    "    def enabled(self):\n",
    "        return True\n",
    "\n",
    "def cpu():\n",
    "    \"\"\"Return cpu device\"\"\"\n",
    "    return CPUDevice()\n",
    "\n",
    "def all_devices():\n",
    "    \"\"\"return a list of all available devices\"\"\"\n",
    "    return [cpu()]\n",
    "\n",
    "\n",
    "class Op:\n",
    "    \"\"\"Operator definition.\"\"\"\n",
    "\n",
    "    def __call__(self, *args):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def compute(self, *args: Tuple[NDArray]):\n",
    "        \"\"\"Calculate forward pass of operator.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input: np.ndarray\n",
    "            A list of input arrays to the function\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output: nd.array\n",
    "            Array output of the operation\n",
    "\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def gradient(\n",
    "        self, out_grad: \"Value\", node: \"Value\"\n",
    "    ) -> Union[\"Value\", Tuple[\"Value\"]]:\n",
    "        \"\"\"Compute partial adjoint for each input value for a given output adjoint.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        out_grad: Value\n",
    "            The adjoint wrt to the output value.\n",
    "\n",
    "        node: Value\n",
    "            The value node of forward evaluation.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        input_grads: Value or Tuple[Value]\n",
    "            A list containing partial gradient adjoints to be propagated to\n",
    "            each of the input node.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def gradient_as_tuple(self, out_grad: \"Value\", node: \"Value\") -> Tuple[\"Value\"]:\n",
    "        \"\"\" Convenience method to always return a tuple from gradient call\"\"\"\n",
    "        output = self.gradient(out_grad, node)\n",
    "        if isinstance(output, tuple):\n",
    "            return output\n",
    "        elif isinstance(output, list):\n",
    "            return tuple(output)\n",
    "        else:\n",
    "            return (output,)\n",
    "\n",
    "\n",
    "class TensorOp(Op):\n",
    "    \"\"\" Op class specialized to output tensors, will be alternate subclasses for other structures \"\"\"\n",
    "\n",
    "    def __call__(self, *args):\n",
    "        return Tensor.make_from_op(self, args)\n",
    "\n",
    "\n",
    "class TensorTupleOp(Op):\n",
    "    \"\"\"Op class specialized to output TensorTuple\"\"\"\n",
    "\n",
    "    def __call__(self, *args):\n",
    "        return TensorTuple.make_from_op(self, args)\n",
    "\n",
    "\n",
    "class Value:\n",
    "    \"\"\"A value in the computational graph.\"\"\"\n",
    "\n",
    "    # trace of computational graph\n",
    "    op: Optional[Op]\n",
    "    inputs: List[\"Value\"]\n",
    "    # The following fields are cached fields for\n",
    "    # dynamic computation\n",
    "    cached_data: NDArray\n",
    "    requires_grad: bool\n",
    "\n",
    "    def realize_cached_data(self):\n",
    "        \"\"\"Run compute to realize the cached data\"\"\"\n",
    "        # avoid recomputation\n",
    "        if self.cached_data is not None:\n",
    "            return self.cached_data\n",
    "        # note: data implicitly calls realized cached data\n",
    "        self.cached_data = self.op.compute(\n",
    "            *[x.realize_cached_data() for x in self.inputs]\n",
    "        )\n",
    "        return self.cached_data\n",
    "\n",
    "    def is_leaf(self):\n",
    "        return self.op is None\n",
    "\n",
    "    def __del__(self):\n",
    "        global TENSOR_COUNTER\n",
    "        TENSOR_COUNTER -= 1\n",
    "\n",
    "    def _init(\n",
    "        self,\n",
    "        op: Optional[Op],\n",
    "        inputs: List[\"Tensor\"],\n",
    "        *,\n",
    "        num_outputs: int = 1,\n",
    "        cached_data: List[object] = None,\n",
    "        requires_grad: Optional[bool] = None\n",
    "    ):\n",
    "        global TENSOR_COUNTER\n",
    "        TENSOR_COUNTER += 1\n",
    "        if requires_grad is None:\n",
    "            requires_grad = any(x.requires_grad for x in inputs)\n",
    "        self.op = op\n",
    "        self.inputs = inputs\n",
    "        self.num_outputs = num_outputs\n",
    "        self.cached_data = cached_data\n",
    "        self.requires_grad = requires_grad\n",
    "\n",
    "    @classmethod\n",
    "    def make_const(cls, data, *, requires_grad=False):\n",
    "        value = cls.__new__(cls)\n",
    "        value._init(\n",
    "            None,\n",
    "            [],\n",
    "            cached_data=data,\n",
    "            requires_grad=requires_grad,\n",
    "        )\n",
    "        return value\n",
    "\n",
    "    @classmethod\n",
    "    def make_from_op(cls, op: Op, inputs: List[\"Value\"]):\n",
    "        value = cls.__new__(cls)\n",
    "        value._init(op, inputs)\n",
    "\n",
    "        if not LAZY_MODE:\n",
    "            if not value.requires_grad:\n",
    "                return value.detach()\n",
    "            value.realize_cached_data()\n",
    "        return value\n",
    "\n",
    "\n",
    "### Not needed in HW1\n",
    "class TensorTuple(Value):\n",
    "    \"\"\"Represent a tuple of tensors.\n",
    "\n",
    "    To keep things simple, we do not support nested tuples.\n",
    "    \"\"\"\n",
    "\n",
    "    def __len__(self):\n",
    "        cdata = self.realize_cached_data()\n",
    "        return len(cdata)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        return needle.ops.tuple_get_item(self, index)\n",
    "\n",
    "    def tuple(self):\n",
    "        return tuple([x for x in self])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"needle.TensorTuple\" + str(self.tuple())\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "    def __add__(self, other):\n",
    "        assert isinstance(other, TensorTuple)\n",
    "        assert len(self) == len(other)\n",
    "        return needle.ops.make_tuple(*[self[i] + other[i] for i in range(len(self))])\n",
    "\n",
    "    def detach(self):\n",
    "        \"\"\"Create a new tensor that shares the data but detaches from the graph.\"\"\"\n",
    "        return Tuple.make_const(self.realize_cached_data())\n",
    "\n",
    "\n",
    "class Tensor(Value):\n",
    "    grad: \"Tensor\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        array,\n",
    "        *,\n",
    "        device: Optional[Device] = None,\n",
    "        dtype=None,\n",
    "        requires_grad=True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        if isinstance(array, Tensor):\n",
    "            if device is None:\n",
    "                device = array.device\n",
    "            if dtype is None:\n",
    "                dtype = array.dtype\n",
    "            if device == array.device and dtype == array.dtype:\n",
    "                cached_data = array.realize_cached_data()\n",
    "            else:\n",
    "                # fall back, copy through numpy conversion\n",
    "                cached_data = Tensor._array_from_numpy(\n",
    "                    array.numpy(), device=device, dtype=dtype\n",
    "                )\n",
    "        else:\n",
    "            device = device if device else cpu()\n",
    "            cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n",
    "\n",
    "        self._init(\n",
    "            None,\n",
    "            [],\n",
    "            cached_data=cached_data,\n",
    "            requires_grad=requires_grad,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _array_from_numpy(numpy_array, device, dtype):\n",
    "        if array_api is numpy:\n",
    "            return numpy.array(numpy_array, dtype=dtype)\n",
    "        return array_api.array(numpy_array, device=device, dtype=dtype)\n",
    "\n",
    "    @staticmethod\n",
    "    def make_from_op(op: Op, inputs: List[\"Value\"]):\n",
    "        tensor = Tensor.__new__(Tensor)\n",
    "        tensor._init(op, inputs)\n",
    "        if not LAZY_MODE:\n",
    "            tensor.realize_cached_data()\n",
    "        return tensor\n",
    "\n",
    "    @staticmethod\n",
    "    def make_const(data, requires_grad=False):\n",
    "        tensor = Tensor.__new__(Tensor)\n",
    "        tensor._init(\n",
    "            None,\n",
    "            [],\n",
    "            cached_data=data\n",
    "            if not isinstance(data, Tensor)\n",
    "            else data.realize_cached_data(),\n",
    "            requires_grad=requires_grad,\n",
    "        )\n",
    "        return tensor\n",
    "\n",
    "    @property\n",
    "    def data(self):\n",
    "        return self.detach()\n",
    "\n",
    "    @data.setter\n",
    "    def data(self, value):\n",
    "        assert isinstance(value, Tensor)\n",
    "        assert value.dtype == self.dtype, \"%s %s\" % (\n",
    "            value.dtype,\n",
    "            self.dtype,\n",
    "        )\n",
    "        self.cached_data = value.realize_cached_data()\n",
    "\n",
    "    def detach(self):\n",
    "        \"\"\"Create a new tensor that shares the data but detaches from the graph.\"\"\"\n",
    "        return Tensor.make_const(self.realize_cached_data())\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.realize_cached_data().shape\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.realize_cached_data().dtype\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        data = self.realize_cached_data()\n",
    "        # numpy array always sits on cpu\n",
    "        if array_api is numpy:\n",
    "            return cpu()\n",
    "        return data.device\n",
    "\n",
    "    def backward(self, out_grad=None):\n",
    "        out_grad = out_grad if out_grad else Tensor(numpy.ones(self.shape))\n",
    "        compute_gradient_of_variables(self, out_grad)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"needle.Tensor(\" + str(self.realize_cached_data()) + \")\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.realize_cached_data().__str__()\n",
    "\n",
    "    def numpy(self):\n",
    "        data = self.realize_cached_data()\n",
    "        if array_api is numpy:\n",
    "            return data\n",
    "        return data.numpy()\n",
    "\n",
    "    def __add__(self, other):\n",
    "        if isinstance(other, Tensor):\n",
    "            return needle.ops.EWiseAdd()(self, other)\n",
    "        else:\n",
    "            return needle.ops.AddScalar(other)(self)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        if isinstance(other, Tensor):\n",
    "            return needle.ops.EWiseMul()(self, other)\n",
    "        else:\n",
    "            return needle.ops.MulScalar(other)(self)\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        if isinstance(other, Tensor):\n",
    "            raise NotImplementedError()\n",
    "        else:\n",
    "            return needle.ops.PowerScalar(other)(self)\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        if isinstance(other, Tensor):\n",
    "            return needle.ops.EWiseAdd()(self, needle.ops.Negate()(other))\n",
    "        else:\n",
    "            return needle.ops.AddScalar(-other)(self)\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        if isinstance(other, Tensor):\n",
    "            return needle.ops.EWiseDiv()(self, other)\n",
    "        else:\n",
    "            return needle.ops.DivScalar(other)(self)\n",
    "\n",
    "    def __matmul__(self, other):\n",
    "        return needle.ops.MatMul()(self, other)\n",
    "\n",
    "    def matmul(self, other):\n",
    "        return needle.ops.MatMul()(self, other)\n",
    "\n",
    "    def sum(self, axes=None):\n",
    "        return needle.ops.Summation(axes)(self)\n",
    "\n",
    "    def broadcast_to(self, shape):\n",
    "        return needle.ops.BroadcastTo(shape)(self)\n",
    "\n",
    "    def reshape(self, shape):\n",
    "        return needle.ops.Reshape(shape)(self)\n",
    "\n",
    "    def __neg__(self):\n",
    "        return needle.ops.Negate()(self)\n",
    "\n",
    "    def transpose(self, axes=None):\n",
    "        return needle.ops.Transpose(axes)(self)\n",
    "\n",
    "    __radd__ = __add__\n",
    "    __rmul__ = __mul__\n",
    "    __rsub__ = __sub__\n",
    "    __rmatmul__ = __matmul__\n",
    "\n",
    "\n",
    "def compute_gradient_of_variables(output_tensor, out_grad):\n",
    "    \"\"\"Take gradient of output node with respect to each node in node_list.\n",
    "\n",
    "    Store the computed result in the grad field of each Variable.\n",
    "    \"\"\"\n",
    "    # a map from node to a list of gradient contributions from each output node\n",
    "    node_to_output_grads_list: Dict[Tensor, List[Tensor]] = {}\n",
    "    # Special note on initializing gradient of\n",
    "    # We are really taking a derivative of the scalar reduce_sum(output_node)\n",
    "    # instead of the vector output_node. But this is the common case for loss function.\n",
    "    node_to_output_grads_list[output_tensor] = [out_grad]\n",
    "\n",
    "    # Traverse graph in reverse topological order given the output_node that we are taking gradient wrt.\n",
    "    reverse_topo_order = list(reversed(find_topo_sort([output_tensor])))\n",
    "\n",
    "    ### BEGIN YOUR SOLUTION\n",
    "    \n",
    "    for node in reverse_topo_order:\n",
    "        node.gradient(out_grad, output_tensor)\n",
    "    ### END YOUR SOLUTION\n",
    "\n",
    "\n",
    "def find_topo_sort(node_list: List[Value]) -> List[Value]:\n",
    "    \"\"\"Given a list of nodes, return a topological sort list of nodes ending in them.\n",
    "\n",
    "    A simple algorithm is to do a post-order DFS traversal on the given nodes,\n",
    "    going backwards based on input edges. Since a node is added to the ordering\n",
    "    after all its predecessors are traversed due to post-order DFS, we get a topological\n",
    "    sort.\n",
    "    \"\"\"\n",
    "    topo = []\n",
    "    visited = set()\n",
    "\n",
    "    for node in node.inputs:\n",
    "        if node not in visited: topo_sort_dfs(node, visited, topo)\n",
    "    \n",
    "    topo_order.reverse()  # Reverse the list to get the correct topological order\n",
    "    return topo_order\n",
    "\n",
    "\n",
    "def topo_sort_dfs(node, visited, topo_order):\n",
    "    \"\"\"Post-order DFS\"\"\"\n",
    "    ### BEGIN YOUR SOLUTION\n",
    "    \n",
    "    visited.add(node)\n",
    "    for child in node.inputs:\n",
    "        if child not in visited:\n",
    "            topo_sort_dfs(child, visited, topo_order)\n",
    "    topo_order.append(node)\n",
    "    \n",
    "    ### END YOUR SOLUTION\n",
    "\n",
    "\n",
    "##############################\n",
    "####### Helper Methods #######\n",
    "##############################\n",
    "\n",
    "\n",
    "def sum_node_list(node_list):\n",
    "    \"\"\"Custom sum function in order to avoid create redundant nodes in Python sum implementation.\"\"\"\n",
    "    from operator import add\n",
    "    from functools import reduce\n",
    "\n",
    "    return reduce(add, node_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d496c44c-c6b5-45eb-9f2d-1f5c213bf5b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ndl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[112], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Test case 1\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m a1, b1 \u001b[38;5;241m=\u001b[39m \u001b[43mndl\u001b[49m\u001b[38;5;241m.\u001b[39mTensor(np\u001b[38;5;241m.\u001b[39masarray([[\u001b[38;5;241m0.88282157\u001b[39m]])), ndl\u001b[38;5;241m.\u001b[39mTensor(np\u001b[38;5;241m.\u001b[39masarray([[\u001b[38;5;241m0.90170084\u001b[39m]]))\n\u001b[1;32m      3\u001b[0m c1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\u001b[38;5;241m*\u001b[39ma1\u001b[38;5;241m*\u001b[39ma1 \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m4\u001b[39m\u001b[38;5;241m*\u001b[39mb1\u001b[38;5;241m*\u001b[39ma1 \u001b[38;5;241m-\u001b[39m a1\n\u001b[1;32m      5\u001b[0m soln \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m0.88282157\u001b[39m]]),\n\u001b[1;32m      6\u001b[0m                  np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m2.64846471\u001b[39m]]),\n\u001b[1;32m      7\u001b[0m                  np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m2.33812177\u001b[39m]]),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m                  np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.88282157\u001b[39m]]),\n\u001b[1;32m     13\u001b[0m                  np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m4.63946401\u001b[39m]])])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ndl' is not defined"
     ]
    }
   ],
   "source": [
    "# Test case 1\n",
    "a1, b1 = ndl.Tensor(np.asarray([[0.88282157]])), ndl.Tensor(np.asarray([[0.90170084]]))\n",
    "c1 = 3*a1*a1 + 4*b1*a1 - a1\n",
    "\n",
    "soln = np.array([np.array([[0.88282157]]),\n",
    "                 np.array([[2.64846471]]),\n",
    "                 np.array([[2.33812177]]),\n",
    "                 np.array([[0.90170084]]),\n",
    "                 np.array([[3.60680336]]),\n",
    "                 np.array([[3.1841638]]),\n",
    "                 np.array([[5.52228558]]),\n",
    "                 np.array([[-0.88282157]]),\n",
    "                 np.array([[4.63946401]])])\n",
    "\n",
    "topo_order = np.array([x.numpy() for x in ndl.autograd.find_topo_sort([c1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d734fcb-e1d8-4f1f-a08e-6c33d781b917",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(soln) == len(topo_order)\n",
    "np.testing.assert_allclose(topo_order, soln, rtol=1e-06, atol=1e-06)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4756c7c0-cb8e-454e-99c4-c0d8702d3dfc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Element Wise Addition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a04d16-43de-4de2-9df5-512a56f8be2f",
   "metadata": {},
   "source": [
    "Let's walk through the step-by-step derivative calculation for the `EWiseAdd` operation:\n",
    "\n",
    "We have the function `f(a, b) = a + b`, where `a` and `b` are tensors. Our goal is to compute the partial derivatives with respect to `a` and `b`.\n",
    "\n",
    "Let's start by calculating the derivative of `f` with respect to `a`, denoted as `df/da`:\n",
    "\n",
    "Step 1: Compute the derivative of `f` with respect to `a`.\n",
    "\n",
    "$\\frac{{\\partial f}}{{\\partial a}} = \\frac{{\\partial}}{{\\partial a}} (a + b)$\n",
    "\n",
    "Since `a` is the variable we are differentiating with respect to, the derivative of `a` with respect to itself is 1:\n",
    "\n",
    "$$\\frac{{\\partial f}}{{\\partial a}} = 1$$\n",
    "\n",
    "Therefore, $$\\frac{{\\partial f}}{{\\partial a}} = 1.$$\n",
    "\n",
    "Step 2: Compute the derivative of `f` with respect to `b`.\n",
    "\n",
    "$$\\frac{{\\partial f}}{{\\partial b}} = \\frac{{\\partial}}{{\\partial b}} (a + b)$$\n",
    "\n",
    "Again, since `b` is the variable we are differentiating with respect to, the derivative of `b` with respect to itself is 1:\n",
    "\n",
    "$$\\frac{{\\partial f}}{{\\partial b}} = 1$$\n",
    "\n",
    "Therefore, $$\\frac{{\\partial f}}{{\\partial b}} = 1$$\n",
    "\n",
    "Hence, the partial derivatives of `f(a, b) = a + b` with respect to `a` and `b` are both equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "448fccc7-03d9-4996-a2f2-62b182adaa0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EWiseAdd(TensorOp):\n",
    "    def compute(self, a: NDArray, b: NDArray):\n",
    "        return a + b\n",
    "\n",
    "    def gradient(self, out_grad: Tensor, node: Tensor):\n",
    "        return out_grad, out_grad\n",
    "\n",
    "\n",
    "def add(a, b):\n",
    "    return EWiseAdd()(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef42c9e2-8e29-4199-bffa-7967d7820eed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Scalar Addition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d90354-3519-49b9-9d07-538896f66ef4",
   "metadata": {},
   "source": [
    "Certainly! Here's the proof and explanation for the derivative of the `AddScalar` operator:\n",
    "\n",
    "Let's denote the scalar as `c` and `a` as the tensor being added by the scalar. The operation can be described as `f(a) = a + c`.\n",
    "\n",
    "The function for the backward pass (i.e., the gradient) is `df/da = 1`, which means the derivative of `f(a)` with respect to `a` is simply `1`.\n",
    "\n",
    "The LaTeX document will look as follows:\n",
    "\n",
    "We are given a function $f(a) = a + c$, where $a$ is a tensor and $c$ is a scalar. Our task is to find the derivative of this function with respect to $a$.\n",
    "\n",
    "By differentiating the function $f(a)$ with respect to $a$, we find:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{da} &= \\frac{d}{da} (a + c) \\\\\n",
    "&= 1\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, the gradient of $f(a)$ with respect to $a$ is $1$.\n",
    "\n",
    "\n",
    "We starts by defining the function `f(a) = a + c`. It then explains that when we differentiate `f(a)` with respect to `a`, we find that the derivative is `1`. This means that the gradient of `f(a)` with respect to `a` is `1`, which matches the behavior of the `AddScalar` operator as provided in the `gradient` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9afe2bcd-25b6-48df-b98a-143f5baccc6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AddScalar(TensorOp):\n",
    "    def __init__(self, scalar):\n",
    "        self.scalar = scalar\n",
    "\n",
    "    def compute(self, a: NDArray):\n",
    "        return a + self.scalar\n",
    "\n",
    "    def gradient(self, out_grad: Tensor, node: Tensor):\n",
    "        return out_grad\n",
    "\n",
    "\n",
    "def add_scalar(a, scalar):\n",
    "    return AddScalar(scalar)(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89a5335-0b34-41c1-9011-129983cb9ff8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Element Wise Mult"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad98e363-13c8-46ea-a1c3-f4c27d87d617",
   "metadata": {},
   "source": [
    "Certainly! Here's the proof and explanation for the derivative of the `EWiseMul` (element-wise multiplication) operator:\n",
    "\n",
    "Let's denote the two input tensors as `a` and `b`. The operation can be described as `f(a, b) = a * b`, where `*` represents element-wise multiplication.\n",
    "\n",
    "The function for the backward pass (i.e., the gradient) is `df/da = b` and `df/db = a`. This means that the derivative of `f(a, b)` with respect to `a` is `b`, and the derivative with respect to `b` is `a`.\n",
    "\n",
    "The LaTeX document will look as follows:\n",
    "\n",
    "\n",
    "We are given a function $f(a, b) = a \\odot b$, where $a$ and $b$ are tensors, and $\\odot$ represents element-wise multiplication. Our task is to find the derivatives of this function with respect to $a$ and $b$.\n",
    "\n",
    "By differentiating the function $f(a, b)$ with respect to $a$, we find:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{da} &= \\frac{d}{da} (a \\odot b) \\\\\n",
    "&= b\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, the gradient of $f(a, b)$ with respect to $a$ is $b$.\n",
    "\n",
    "Similarly, by differentiating the function $f(a, b)$ with respect to $b$, we find:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{db} &= \\frac{d}{db} (a \\odot b) \\\\\n",
    "&= a\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, the gradient of $f(a, b)$ with respect to $b$ is $a$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7164447-a850-4047-b89b-b94c1ca77487",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EWiseMul(TensorOp):\n",
    "    def compute(self, a: NDArray, b: NDArray):\n",
    "        return a * b\n",
    "\n",
    "    def gradient(self, out_grad: Tensor, node: Tensor):\n",
    "        lhs, rhs = node.inputs\n",
    "        return out_grad * rhs, out_grad * lhs\n",
    "\n",
    "\n",
    "def multiply(a, b):\n",
    "    return EWiseMul()(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19e45b7-0fca-485e-9771-aefef3cf6d09",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Scalar Mult"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f549c16-93ba-48b1-b87f-7507a35923c7",
   "metadata": {},
   "source": [
    "Certainly! Here's the proof and explanation for the derivative of the `MulScalar` operator:\n",
    "\n",
    "Let's denote the scalar as `c` and `a` as the tensor being multiplied by the scalar. The operation can be described as `f(a) = a * c`.\n",
    "\n",
    "The function for the backward pass (i.e., the gradient) is `df/da = c`, which means the derivative of `f(a)` with respect to `a` is `c`.\n",
    "\n",
    "The LaTeX document will look as follows:\n",
    "\n",
    "We are given a function $f(a) = a \\cdot c$, where $a$ is a tensor and $c$ is a scalar. Our task is to find the derivative of this function with respect to $a$.\n",
    "\n",
    "By differentiating the function $f(a)$ with respect to $a$, we find:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{da} &= \\frac{d}{da} (a \\cdot c) \\\\\n",
    "&= c\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, the gradient of $f(a)$ with respect to $a$ is $c$.\n",
    "\n",
    "We starts by defining the function `f(a) = a * c`. It then explains that when we differentiate `f(a)` with respect to `a`, we find that the derivative is `c`. This means that the gradient of `f(a)` with respect to `a` is `c`, which matches the behavior of the `MulScalar` operator as provided in the `gradient` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6228d02-df05-489d-a2af-e645fe38562b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MulScalar(TensorOp):\n",
    "    def __init__(self, scalar):\n",
    "        self.scalar = scalar\n",
    "\n",
    "    def compute(self, a: NDArray):\n",
    "        return a * self.scalar\n",
    "\n",
    "    def gradient(self, out_grad: Tensor, node: Tensor):\n",
    "        return (out_grad * self.scalar,)\n",
    "\n",
    "\n",
    "def mul_scalar(a, scalar):\n",
    "    return MulScalar(scalar)(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e262d60-8c17-4cd2-83e5-43cf668f0a3b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Power Scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507873e9-8c2f-49be-8e3f-b7eba5954ff6",
   "metadata": {},
   "source": [
    "Certainly! Here's the proof and explanation for the derivative of the `PowerScalar` operator:\n",
    "\n",
    "Let's denote the scalar as `n` and `a` as the tensor being raised to the power of the scalar. The operation can be described as `f(a) = a^n`.\n",
    "\n",
    "The function for the backward pass (i.e., the gradient) is `df/da = n * a^(n-1)`.\n",
    "\n",
    "The LaTeX document will look as follows:\n",
    "\n",
    "\n",
    "We are given a function $f(a) = a^n$, where $a$ is a tensor and $n$ is a scalar. Our task is to find the derivative of this function with respect to $a$.\n",
    "\n",
    "By differentiating the function $f(a)$ with respect to $a$, we find:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{da} &= \\frac{d}{da} (a^n) \\\\\n",
    "&= n \\cdot a^{n-1}\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, the gradient of $f(a)$ with respect to $a$ is $n \\cdot a^{n-1}$.\n",
    "\n",
    "We starts by defining the function `f(a) = a^n`, where `^` represents exponentiation. It then explains that when we differentiate `f(a)` with respect to `a`, we find that the derivative is `n * a^(n-1)`. This means that the gradient of `f(a)` with respect to `a` is `n * a^(n-1)`, which matches the behavior of the `PowerScalar` operator as provided in the `gradient` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "83d17d97-f9a7-4398-ba64-ba7c8b83348a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PowerScalar(TensorOp):\n",
    "    \"\"\"Op raise a tensor to an (integer) power.\"\"\"\n",
    "\n",
    "    def __init__(self, scalar: int):\n",
    "        self.scalar = scalar\n",
    "\n",
    "    def compute(self, a: NDArray) -> NDArray:\n",
    "        ### BEGIN YOUR SOLUTION\n",
    "        return array_api.power(a, self.scalar)\n",
    "        ### END YOUR SOLUTION\n",
    "\n",
    "    def gradient(self, out_grad, node):\n",
    "        ### BEGIN YOUR SOLUTION\n",
    "        return (self.scalar * power_scalar(node, self.scalar - 1) * out_grad, )\n",
    "        ### END YOUR SOLUTION\n",
    "\n",
    "\n",
    "def power_scalar(a, scalar):\n",
    "    return PowerScalar(scalar)(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eecaccd-1dae-447b-b76b-c8f8f0a339a8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Element Wise Divide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac973c7-595c-43be-a0c6-27793df1094c",
   "metadata": {},
   "source": [
    "The operation described here is an element-wise division of two tensors, `a` and `b`, where the operation can be described as `f(a, b) = a / b`. \n",
    "\n",
    "We'll compute the partial derivatives with respect to `a` and `b`:\n",
    "\n",
    "1. The partial derivative of `f(a, b)` with respect to `a` (`df/da`) is `1/b`.\n",
    "\n",
    "2. The partial derivative of `f(a, b)` with respect to `b` (`df/db`) is `-a / b^2`.\n",
    "\n",
    "These results align with the backward function you've provided.\n",
    "\n",
    "Now, let's translate this into LaTeX:\n",
    "\n",
    "\n",
    "We are given a function $f(a, b) = \\frac{a}{b}$, where $a$ and $b$ are tensors. Our task is to find the partial derivatives of this function with respect to $a$ and $b$.\n",
    "\n",
    "Let's start with $\\frac{\\partial f}{\\partial a}$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial f}{\\partial a} &= \\frac{\\partial}{\\partial a} \\left(\\frac{a}{b}\\right) \\\\\n",
    "&= \\frac{1}{b}\n",
    "\\end{align*}\n",
    "\n",
    "Now, let's compute $\\frac{\\partial f}{\\partial b}$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial f}{\\partial b} &= \\frac{\\partial}{\\partial b} \\left(\\frac{a}{b}\\right) \\\\\n",
    "&= - \\frac{a}{b^{2}}\n",
    "\\end{align*}\n",
    "\n",
    "Here is a detailed derivative:\n",
    "\n",
    "Given a function of the form $y = \\frac{u}{v}$, where both $u$ and $v$ are functions of $x$, the quotient rule of differentiation states:\n",
    "\n",
    "$$\\frac{dy}{dx} = \\frac{v \\cdot \\frac{du}{dx} - u \\cdot \\frac{dv}{dx}}{v^2}$$\n",
    "\n",
    "In our case, we're looking at the function $y = \\frac{a}{b}$, where $a$ and $b$ are tensors. We want to find the derivative with respect to $b$ (instead of $x$ in our general formula). So we have:\n",
    "\n",
    "$$\\frac{dy}{db} = \\frac{b \\cdot \\frac{da}{db} - a \\cdot \\frac{db}{db}}{b^2}$$\n",
    "\n",
    "Since $a$ does not depend on $b$, $\\frac{da}{db} = 0$, and since any variable is equal to itself, $\\frac{db}{db} = 1$. \n",
    "\n",
    "So the derivative $\\frac{dy}{db}$ simplifies to:\n",
    "\n",
    "$$\\frac{dy}{db} = \\frac{b \\cdot 0 - a \\cdot 1}{b^2}$$\n",
    "\n",
    "Therefore, the derivative of $y$ with respect to $b$ is $-\\frac{a}{b^2}$.\n",
    "\n",
    "Therefore, the gradient of $f(a, b)$ with respect to $a$ is $\\frac{1}{b}$, and the gradient of $f(a, b)$ with respect to $b$ is $- \\frac{a}{b^{2}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "64b849b5-8523-4bb8-9dd2-dbc0e8c7c8a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EWiseDiv(TensorOp):\n",
    "    \"\"\"Op to element-wise divide two nodes.\"\"\"\n",
    "\n",
    "    def compute(self, a, b):\n",
    "        ### BEGIN YOUR SOLUTION\n",
    "        return a / b\n",
    "        ### END YOUR SOLUTION\n",
    "\n",
    "    def gradient(self, out_grad, node):\n",
    "        ### BEGIN YOUR SOLUTION\n",
    "        a, b = node.inputs\n",
    "        return divide(out_grad, b), out_grad * negate(divide(a, power_scalar(b, 2)))\n",
    "        ### END YOUR SOLUTION\n",
    "\n",
    "\n",
    "def divide(a, b):\n",
    "    return EWiseDiv()(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c775097-ab17-4b05-8557-d3dfd56f7869",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Divide Scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2618d629-f6ce-4039-b6aa-e13acf3f348b",
   "metadata": {},
   "source": [
    "Let's denote the scalar as `c`, and `a` as the tensor being divided by the scalar. The operation can be described as `f(a) = a / c`.\n",
    "\n",
    "The function for the backward pass (i.e., the gradient) is `df/da = 1/c`.\n",
    "\n",
    "This is the derivative of `f(a)` with respect to `a`.\n",
    "\n",
    "The LaTeX document will look as follows:\n",
    "\n",
    "We are given a function $f(a) = \\frac{a}{c}$, where $a$ is a tensor and $c$ is a scalar. Our task is to find the derivative of this function with respect to $a$.\n",
    "\n",
    "By using the power rule of differentiation, where the derivative of $a^n$ is $n \\cdot a^{n-1}$, we can rewrite $f(a)$ as $f(a) = c^{-1}a$. \n",
    "\n",
    "Now, we can differentiate this with respect to $a$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{da} &= \\frac{d}{da} (c^{-1}a) \\\\\n",
    "&= c^{-1} \\frac{d}{da} (a) \\\\\n",
    "&= c^{-1} \\\\\n",
    "&= \\frac{1}{c}\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, the gradient of $f(a)$ with respect to $a$ is $\\frac{1}{c}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0130023a-6178-4120-a6c9-3a0f81da0cdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DivScalar(TensorOp):\n",
    "    def __init__(self, scalar):\n",
    "        self.scalar = scalar\n",
    "\n",
    "    def compute(self, a):\n",
    "        ### BEGIN YOUR SOLUTION\n",
    "        return a / self.scalar\n",
    "        ### END YOUR SOLUTION\n",
    "\n",
    "    def gradient(self, out_grad, node):\n",
    "        ### BEGIN YOUR SOLUTION\n",
    "        return out_grad * (1/self.scalar)\n",
    "        ### END YOUR SOLUTION\n",
    "\n",
    "\n",
    "def divide_scalar(a, scalar):\n",
    "    return DivScalar(scalar)(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8d63e6-346b-4c6b-b770-6fabf6f85c80",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "80d36def-d0b1-473b-bbcb-077877562b87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Transpose(TensorOp):\n",
    "    def __init__(self, axes: Optional[tuple] = None):\n",
    "        self.axes = axes\n",
    "\n",
    "    def compute(self, a):\n",
    "        # Swap the last two dimensions of the input tensor\n",
    "        if self.axes:\n",
    "            a = a.swapaxes(self.axes[0], self.axes[1])\n",
    "        else:\n",
    "            a = a.swapaxes(-2, -1)\n",
    "        return a\n",
    "\n",
    "\n",
    "    def gradient(self, out_grad, node):\n",
    "        ### BEGIN YOUR SOLUTION\n",
    "        return transpose(out_grad,axes=self.axes)\n",
    "        ### END YOUR SOLUTION\n",
    "\n",
    "\n",
    "def transpose(a, axes=None):\n",
    "    return Transpose(axes)(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a569dad-0144-47ba-86e5-6d40aef3b4ef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "44c95c81-abd5-447b-a9fc-5fee081af854",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Reshape(TensorOp):\n",
    "    def __init__(self, shape):\n",
    "        self.shape = shape\n",
    "\n",
    "    def compute(self, a):\n",
    "        ### BEGIN YOUR SOLUTION\n",
    "        self.input_shape = a.shape\n",
    "        return array_api.reshape(a, newshape=self.shape)\n",
    "        ### END YOUR SOLUTION\n",
    "\n",
    "    def gradient(self, out_grad, node):\n",
    "        ### BEGIN YOUR SOLUTION\n",
    "        # reshape gradient to match input shape\n",
    "        return reshape(out_grad, self.input_shape), \n",
    "        ### END YOUR SOLUTION\n",
    "\n",
    "\n",
    "def reshape(a, shape):\n",
    "    return Reshape(shape)(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9293dd-c605-46b6-bc54-28e92ccca957",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Broadcast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974da9d9-1024-460e-b1ef-0060aa2d2900",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here's the requested information with the appropriate formatting:\n",
    "\n",
    "#### Implementing Backward Pass for Broadcasting Operation\n",
    "\n",
    "Broadcasting is an operation that expands a tensor to a given shape by replicating its values along the new dimensions. The backward pass of broadcasting needs to \"undo\" this operation and sum up the gradients that have been duplicated due to the broadcasting operation.\n",
    "\n",
    "In the `gradient` function for the broadcasting operation, `out_grad` is the gradient tensor flowing back from further down the computational graph. This tensor has the same shape as the output of the broadcasting operation, i.e., the expanded or broadcasted shape. \n",
    "\n",
    "We need to sum up the gradients in `out_grad` along the dimensions that have been extended by the broadcasting operation, and return a gradient tensor that has the same shape as the original input tensor.\n",
    "\n",
    "To achieve this, the `gradient` function does the following:\n",
    "\n",
    "1. **Identify the singleton dimensions:** Singleton dimensions are dimensions of size 1 that were either expanded due to broadcasting or added when broadcasting to a higher rank tensor. We need to identify these dimensions so we know which axes to sum over in the backward pass.\n",
    "\n",
    "2. **Sum the gradients over singleton dimensions:** We sum the `out_grad` tensor over all the singleton dimensions. This effectively reverses the broadcasting operation, as it adds up all the gradients that were copied due to the broadcasting.\n",
    "\n",
    "3. **Reshape the gradient tensor:** We then reshape the resulting gradient tensor to ensure that it has the same shape as the input tensor. This is important because the gradient tensor must match the shape of the input tensor for it to be propagated correctly in the backward pass. \n",
    "\n",
    "The `gradient` function for the broadcasting operation can be implemented as follows:\n",
    "\n",
    "```python\n",
    "def gradient(self, out_grad, node):\n",
    "    # Shape of the input tensor\n",
    "    in_shape = node.inputs[0].shape\n",
    "\n",
    "    # Compute the difference in tensor ranks (dimensions) between the output and the input\n",
    "    rank_diff = len(self.shape) - len(in_shape)\n",
    "\n",
    "    # Indices of singleton dimensions added by broadcasting to higher-rank tensor\n",
    "    new_singleton_dims = list(range(rank_diff))\n",
    "\n",
    "    # Indices of singleton dimensions in the input tensor that were expanded by broadcasting\n",
    "    expanded_singleton_dims = [i + rank_diff for i, size in enumerate(in_shape) if size == 1]\n",
    "\n",
    "    # Combine all indices of singleton dimensions\n",
    "    singleton_dims = new_singleton_dims + expanded_singleton_dims\n",
    "\n",
    "    # Sum the out_grad tensor over all singleton dimensions to \"undo\" the broadcasting\n",
    "    grad = summation(out_grad, axes=tuple(singleton_dims))\n",
    "\n",
    "    # Reshape the resulting gradient tensor to match the shape of the input tensor\n",
    "    grad = reshape(grad, in_shape)\n",
    "\n",
    "    # Return the gradient tensor\n",
    "    return (grad,)\n",
    "```\n",
    "\n",
    "Remember to validate the correctness of this implementation by performing gradient checking as suggested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e0c9ddf1-f95c-47f8-827e-1ef24a54a5ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[3., 4.],\n",
       "         [5., 6.]]),\n",
       " torch.Size([2, 2]))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Initialize a 2x2 tensor\n",
    "A = torch.tensor([[1., 2.], [3., 4.]])\n",
    "\n",
    "# Initialize a scalar\n",
    "b = torch.tensor([2.])\n",
    "\n",
    "# Perform the broadcasting operation\n",
    "C = A + b\n",
    "C, C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "456adee2-5f2b-49c0-ad55-6be6605272c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10.])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure to enable gradient computation for b\n",
    "b = torch.tensor([2.], requires_grad=True)\n",
    "\n",
    "# Perform the broadcasting operation\n",
    "C = A + b\n",
    "\n",
    "# Now let's assume some gradient coming from the next layer during backpropagation\n",
    "grad_next = torch.tensor([[1., 2.], [3., 4.]])\n",
    "\n",
    "# Perform the backward pass\n",
    "C.backward(grad_next)\n",
    "\n",
    "# Check the gradient for b\n",
    "b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "30b02a2d-8771-4d07-92d6-8d7a8c91581e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BroadcastTo(TensorOp):\n",
    "    def __init__(self, shape):\n",
    "        self.shape = shape\n",
    "\n",
    "    def compute(self, a):\n",
    "        self.input_shape = a.shape\n",
    "        return array_api.broadcast_to(a, self.shape)\n",
    "\n",
    "    def gradient(self, out_grad, node):\n",
    "        # Get the difference in ranks between the broadcasted tensor and the input tensor\n",
    "        rank_diff = len(self.shape) - len(self.input_shape)\n",
    "\n",
    "        # Identify the new singleton dimensions that were added due to broadcasting\n",
    "        # to a higher rank tensor. These are dimensions that do not exist in the input tensor.\n",
    "        new_singleton_dims = list(range(rank_diff))\n",
    "\n",
    "        # Identify the singleton dimensions in the input tensor that were expanded by broadcasting.\n",
    "        # We count from the end of the shape (-1 refers to the last dimension, -2 the second to last, and so on).\n",
    "        # This way, we correctly handle the cases where the input tensor and the broadcasted shape\n",
    "        # differ both in rank and in size along some dimensions.\n",
    "        expanded_singleton_dims = [i + rank_diff for i, size in enumerate(self.input_shape[::-1]) if size == 1]\n",
    "\n",
    "        # Combine all indices of singleton dimensions\n",
    "        singleton_dims = new_singleton_dims + expanded_singleton_dims[::-1]\n",
    "        return (reshape(summation(out_grad,axes=tuple(singleton_dims)), self.input_shape),) # a deliberate tuple\n",
    "\n",
    "def broadcast_to(a, shape):\n",
    "    return BroadcastTo(shape)(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6147e927-8cdb-4daf-b36d-e0d70c3f9bdb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor and set requires_grad to True so that PyTorch will\n",
    "# know to compute gradients with respect to this tensor.\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# Compute a sum.\n",
    "y = x.sum()\n",
    "\n",
    "# Compute gradients.\n",
    "y.backward()\n",
    "\n",
    "# The gradient of the sum with respect to x is a tensor of ones.\n",
    "print(x.grad)  # Outputs: tensor([1., 1., 1.])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e8278167-e0cc-4add-963b-6625a69a165c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      " tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]], requires_grad=True)\n",
      "y:\n",
      " tensor([5., 7., 9.], grad_fn=<SumBackward1>)\n",
      "y.shape:\n",
      " torch.Size([3])\n",
      "loss:\n",
      " tensor(21., grad_fn=<SumBackward0>)\n",
      "Gradients:\n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a 2D tensor\n",
    "x = torch.tensor([[1., 2., 3.], [4., 5., 6.]], requires_grad=True)\n",
    "print(\"x:\\n\", x)\n",
    "\n",
    "# Sum the tensor along axis 0\n",
    "y = x.sum(axis=0)\n",
    "print(\"y:\\n\", y)\n",
    "print(\"y.shape:\\n\", y.shape)\n",
    "\n",
    "# Let's define a scalar loss as the sum of all elements in y\n",
    "loss = y.sum()\n",
    "print(\"loss:\\n\", loss)\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "# Display gradients\n",
    "print(\"Gradients:\\n\", x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b392889a-5429-4d3a-85b5-a9f85ef83c43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_shape = list(x.shape)\n",
    "new_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e53aaa97-8b5a-4531-b783-76974283195e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for axis in (0,): new_shape[axis] = 1\n",
    "new_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada258a4-3602-4775-918c-7cf42664009d",
   "metadata": {},
   "source": [
    "#### illustrate the reshaping and broadcasting of the gradient in the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2948bd0e-243b-4bc9-bed0-fa83e7b87f7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]], requires_grad=True)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a 2D tensor\n",
    "x = torch.tensor([[1., 2., 3.], [4., 5., 6.]], requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e6ab630f-615d-4e9e-a8e4-40e7fee119ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5., 7., 9.], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sum the tensor along axis 0\n",
    "y = x.sum(axis=0)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e0b8b8ff-62c0-4e8d-bf38-e35af0626c73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1.]), torch.Size([3]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute a dummy gradient for y\n",
    "grad_y = torch.ones_like(y)\n",
    "grad_y, grad_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b75ffbce-b275-4065-87a4-44084c949717",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshape grad_y to match the dimensionality of x\n",
    "new_shape = [1 if axis == 0 else size for axis, size in enumerate(x.shape)]\n",
    "new_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6be202e7-d014-4e13-8fbc-081838d5cc71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1.]]), torch.Size([1, 3]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_grad_y = grad_y.reshape(new_shape)\n",
    "reshaped_grad_y, reshaped_grad_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c284089b-d156-498b-86e5-e4e362374c38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1.],\n",
       "         [1., 1., 1.]]),\n",
       " torch.Size([2, 3]))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Broadcast the reshaped grad_y to match the shape of x\n",
    "broadcasted_grad_y = reshaped_grad_y.expand_as(x)\n",
    "broadcasted_grad_y, broadcasted_grad_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef27b570-77eb-4bd0-b49d-54a86a68d9e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Summation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9577c15-db7c-49f7-8186-a26bd8f86ac8",
   "metadata": {},
   "source": [
    "The `Summation` operation, when provided with the `axes` argument, sums over these axes and thereby reduces the rank of the tensor by the number of axes summed over. The backward pass needs to take this into account, as it needs to return a gradient tensor of the same shape as the input.\n",
    "\n",
    "Here is a simplified implementation of the correct version:\n",
    "\n",
    "The forward pass (`compute` method) is straightforward - it just computes the sum over the specified axes.\n",
    "\n",
    "In the backward pass (`gradient` method), the goal is to compute the gradient of the sum operation. Since every element of the input tensor contributes equally to the sum, the derivative of the sum with respect to each element is 1. However, since the sum operation may reduce the dimensionality of the tensor (when `axes` is not `None`), we need to account for this when computing the gradient.\n",
    "\n",
    "To do this, we first create a new shape, where the dimensions specified by `axes` are replaced by 1. We then reshape `out_grad` to this new shape. This essentially \"undoes\" the dimensionality reduction performed by the sum operation. Finally, we use `broadcast_to` to make the reshaped gradient tensor the same shape as the input tensor.\n",
    "\n",
    "This ensures that the gradient tensor is the correct shape, and that the gradient with respect to each element of the input tensor is correctly computed as 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d74fcff2-2f12-4996-a6b2-2afdb5ccbbfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Summation(TensorOp):\n",
    "    def __init__(self, axes: Optional[tuple] = None):\n",
    "        self.axes = axes\n",
    "\n",
    "    def compute(self, a):\n",
    "        # Forward pass just computes the sum over the specified axes\n",
    "        return array_api.sum(a, self.axes)\n",
    "\n",
    "    def gradient(self, out_grad, node):\n",
    "        # out_grad is the gradient of the output of this operation\n",
    "        # We need to \"undo\" the dimensionality reduction performed in the forward pass\n",
    "        # That's why we create a new shape, replacing the dimensions specified by self.axes with 1\n",
    "\n",
    "        # Initialize new shape to be the same as the input shape\n",
    "        new_shape = list(node.inputs[0].shape)\n",
    "\n",
    "        # If axes were specified, set those dimensions to 1 in the new shape\n",
    "        if self.axes:\n",
    "            for axis in self.axes: new_shape[axis] = 1\n",
    "                \n",
    "        # Reshape out_grad to the new shape\n",
    "        reshaped_grad = reshape(out_grad, new_shape)\n",
    "\n",
    "        # Broadcast the reshaped out_grad to match the input shape\n",
    "        broadcasted_grad = broadcast_to(reshaped_grad, node.inputs[0].shape)\n",
    "\n",
    "        # The gradient method needs to return a tuple, even though there's only one input\n",
    "        return (broadcasted_grad,)\n",
    "\n",
    "def summation(a, axes=None):\n",
    "    return Summation(axes)(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908a1c6c-3c64-4520-a5f3-58c07df34559",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Matrix Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1228339-e77c-4d71-b7a8-e095776dd2df",
   "metadata": {},
   "source": [
    "Matrix multiplication, often denoted by \"matmul\" in some programming languages, refers to the process of multiplying two matrices together. However, in the context of calculus, it's more common to talk about the derivative of a function. \n",
    "\n",
    "When dealing with matrices, instead of talking about derivatives, we often discuss the Jacobian, which is a matrix of partial derivatives. If you have a function that takes a matrix as input and produces a scalar output, you could compute a gradient, which would be a matrix of the same shape as the input matrix.\n",
    "\n",
    "However, in the context of deep learning and backpropagation, you might be asking about the derivative of a matrix multiplication operation with respect to its inputs. This is often needed when you're training a neural network, because you need to compute gradients to update the weights.\n",
    "\n",
    "Let's denote the matrices as `A` and `B`, where `A` is a matrix of dimension `m x n` and `B` is a matrix of dimension `n x p`, and the result of the multiplication `C = A * B` is a matrix of dimension `m x p`.\n",
    "\n",
    "If we are to compute the derivative of `C` with respect to `A` (i.e., ∂C/∂A), each element in `A` affects all elements in its corresponding row in `C`. Thus, the derivative of `C` with respect to `A` is a four-dimensional tensor. In practice, it is common to work with the gradients in a reshaped or unrolled form to perform the necessary update steps in backpropagation.\n",
    "\n",
    "Similarly, if we are to compute the derivative of `C` with respect to `B` (i.e., ∂C/∂B), each element in `B` affects all elements in its corresponding column in `C`. Again, the derivative will be a four-dimensional tensor.\n",
    "\n",
    "In actual computation, if we have a scalar-valued loss function `L`, we would compute the gradient of `L` with respect to `A` (denoted as ∂L/∂A), which is the same shape as `A`. To compute this, we need to know the gradient of `L` with respect to `C` (denoted as ∂L/∂C), then:\n",
    "\n",
    "∂L/∂A = (∂L/∂C) * B^T   (where * denotes matrix multiplication and B^T is the transpose of B)\n",
    "\n",
    "Similarly, to compute the gradient of `L` with respect to `B` (denoted as ∂L/∂B):\n",
    "\n",
    "∂L/∂B = A^T * (∂L/∂C)\n",
    "\n",
    "The details of this process can be quite involved and understanding it fully requires a good understanding of linear algebra and calculus. For more in-depth understanding, it would be beneficial to refer to a textbook or detailed resource on the subject, such as \"The Matrix Calculus You Need For Deep Learning\" by Terence Parr and Jeremy Howard.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281024ba-899f-429e-9bf2-b683de2c0d2f",
   "metadata": {},
   "source": [
    "The line `axes_to_sum_over = tuple(range(len(out_shape) - len(lhs_shape)))` is calculating which axes (dimensions) of the output gradient tensor (`out_grad`) need to be summed over when computing the gradient with respect to the left-hand side (`lhs`) input tensor.\n",
    "\n",
    "This is necessary when the rank (number of dimensions) of `out_grad` is larger than the rank of `lhs`. This can happen, for instance, when `lhs` is a matrix (2D tensor) and `out_grad` is a 3D tensor (which can result from batched matrix multiplication).\n",
    "\n",
    "The `range` function generates a sequence of integers from 0 up to (but not including) `len(out_shape) - len(lhs_shape)`. The `tuple` function then takes this sequence and turns it into a tuple. The result is a tuple of integers representing the axes to sum over.\n",
    "\n",
    "Here is a concrete example:\n",
    "\n",
    "Suppose we have a batched matrix multiplication where `lhs` is a matrix of shape `(m, n)`, and `out_grad` is a 3D tensor of shape `(b, m, n)`, where `b` is the batch size. \n",
    "\n",
    "In this case, `len(out_shape) - len(lhs_shape)` equals `1`, so `range(len(out_shape) - len(lhs_shape))` generates a sequence of integers from `0` to `1` (not inclusive), which is just `[0]`.\n",
    "\n",
    "So `axes_to_sum_over` will be `(0,)`, indicating that we need to sum over the first axis (the batch axis) of `out_grad` when computing the gradient with respect to `lhs`.\n",
    "\n",
    "This summing operation effectively accumulates the individual gradients for each item in the batch into a single gradient for the `lhs` matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fd7dbe38-2479-454e-8162-4fc7d2c329a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Suppose we have the following shapes for `lhs` and `out_grad`\n",
    "m, n, b = 5, 7, 3\n",
    "\n",
    "# Let's create some tensors with these shapes\n",
    "lhs = torch.randn(m, n)          # lhs is a 2D tensor (matrix) of shape (m, n)\n",
    "out_grad = torch.randn(b, m, n)  # out_grad is a 3D tensor of shape (b, m, n)\n",
    "\n",
    "# Let's say `rhs` is another matrix that was involved in computing out_grad\n",
    "rhs = torch.randn(n, m)\n",
    "\n",
    "# Now we want to compute the gradient of the loss with respect to `lhs`\n",
    "# First, we transpose `rhs` and perform batched matrix multiplication with `out_grad`\n",
    "# grad_product = torch.matmul(out_grad, rhs.t())\n",
    "\n",
    "# # Now we need to sum over the batch dimension\n",
    "# axes_to_sum_over = (0,)  # in PyTorch, we can also just use 0 instead of a tuple\n",
    "# grad_wrt_lhs = grad_product.sum(dim=axes_to_sum_over)\n",
    "\n",
    "# # grad_wrt_lhs is now a tensor of shape (m, n), same as `lhs`\n",
    "# print(grad_wrt_lhs.shape)  # prints: torch.Size([5, 7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5324df91-93bf-451b-b5fe-b6952758ba35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 5, 7]), torch.Size([5, 7]), torch.Size([7, 5]))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_shape, lhs_shape, rsh_shape = out_grad.shape, lhs.shape, rhs.shape\n",
    "out_shape, lhs_shape, rsh_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9b7fee3d-a548-4117-bfb3-96fc5a896784",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out_shape), len(lhs_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d2ce16be-cd69-4686-8362-69e1ac7b9d53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 1)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = range(len(out_shape) - len(lhs_shape))\n",
    "rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c0751837-8d2e-454b-a0a0-26d7d82b3255",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ce762c-16fa-4cdc-b5cb-2a00d6998032",
   "metadata": {},
   "outputs": [],
   "source": [
    "axes_to_sum_over = tuple(range(len(out_shape) - len(lhs_shape)))\n",
    "axes_to_sum_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1a9849fc-7bdd-4f67-8e55-6e3216c08d46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MatMul(TensorOp):\n",
    "    def compute(self, a, b):\n",
    "        return array_api.matmul(a, b)\n",
    "\n",
    "    def gradient(self, out_grad, node):\n",
    "        lhs, rhs = node.inputs\n",
    "        out_shape, lhs_shape, rhs_shape = out_grad.shape, lhs.shape, rhs.shape\n",
    "        \n",
    "        # compute gradient with respect to lhs\n",
    "        if len(lhs_shape) == len(out_shape):\n",
    "            grad_wrt_lhs = matmul(out_grad, transpose(rhs))\n",
    "        else:\n",
    "            axes_to_sum_over = tuple(range(len(out_shape) - len(lhs_shape)))\n",
    "            grad_wrt_lhs = summation(matmul(out_grad, transpose(rhs)), axes=axes_to_sum_over)\n",
    "        \n",
    "        # compute gradient with respect to rhs\n",
    "        if len(rhs_shape) == len(out_shape):\n",
    "            grad_wrt_rhs = matmul(transpose(lhs), out_grad)\n",
    "        else:\n",
    "            axes_to_sum_over = tuple(range(len(out_shape) - len(rhs_shape)))\n",
    "            grad_wrt_rhs = summation(matmul(transpose(lhs), out_grad), axes=axes_to_sum_over)\n",
    "        \n",
    "        return grad_wrt_lhs, grad_wrt_rhs\n",
    "\n",
    "def matmul(a, b):\n",
    "    return MatMul()(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f221494c-7d4b-4f9c-bc7f-244a587c9e2a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Negation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09826ab-2835-45c1-ab55-92eeeb1d02e4",
   "metadata": {},
   "source": [
    "Certainly! Here's the proof and explanation for the derivative of the `Negate` operator:\n",
    "\n",
    "Let's denote `a` as the tensor being negated. The operation can be described as `f(a) = -a`.\n",
    "\n",
    "The function for the backward pass (i.e., the gradient) is `df/da = -1`.\n",
    "\n",
    "The LaTeX document will look as follows:\n",
    "\n",
    "\n",
    "We are given a function $f(a) = -a$, where $a$ is a tensor. Our task is to find the derivative of this function with respect to $a$.\n",
    "\n",
    "By differentiating the function $f(a)$ with respect to $a$, we find:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{da} &= \\frac{d}{da} (-a) \\\\\n",
    "&= -1\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, the gradient of $f(a)$ with respect to $a$ is $-1$.\n",
    "\n",
    "WE starts by defining the function `f(a) = -a`, where `-` represents the negation operation. It then explains that when we differentiate `f(a)` with respect to `a`, we find that the derivative is `-1`. This means that the gradient of `f(a)` with respect to `a` is `-1`, which matches the behavior of the `Negate` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "fe81ac6f-7b2f-43b3-b5a7-d20818b40607",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Negate(TensorOp):\n",
    "    def compute(self, a):\n",
    "        ### BEGIN YOUR SOLUTION\n",
    "        return -1 * a\n",
    "        ### END YOUR SOLUTION\n",
    "\n",
    "    def gradient(self, out_grad, node):\n",
    "        ### BEGIN YOUR SOLUTION\n",
    "        return negate(out_grad), \n",
    "        ### END YOUR SOLUTION\n",
    "\n",
    "def negate(a):\n",
    "    return Negate()(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc0094e-5c2f-44a7-a766-d3190cac9b96",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c56d27-5596-4487-801d-d0d3dfac2b93",
   "metadata": {},
   "source": [
    "Certainly! Here's the proof and explanation for the derivative of the `Log` operator:\n",
    "\n",
    "Let's denote `a` as the tensor on which the logarithm is applied. The operation can be described as `f(a) = \\log(a)`, where `\\log` represents the natural logarithm.\n",
    "\n",
    "The function for the backward pass (i.e., the gradient) is `df/da = 1/a`.\n",
    "\n",
    "The LaTeX document will look as follows:\n",
    "\n",
    "\n",
    "We are given a function $f(a) = \\log(a)$, where $a$ is a tensor. Our task is to find the derivative of this function with respect to $a$.\n",
    "\n",
    "By differentiating the function $f(a)$ with respect to $a$, we find:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{da} &= \\frac{d}{da} (\\log(a)) \\\\\n",
    "&= \\frac{1}{a}\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, the gradient of $f(a)$ with respect to $a$ is $\\frac{1}{a}$.\n",
    "\n",
    "\n",
    "This document starts by defining the function `f(a) = \\log(a)`, where `\\log` represents the natural logarithm. It then explains that when we differentiate `f(a)` with respect to `a`, we find that the derivative is `1/a`. This means that the gradient of `f(a)` with respect to `a` is `1/a`, which represents the behavior of the `Log` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6fb52c72-9bb5-4d01-bd08-59cb37acfa3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Log(TensorOp):\n",
    "    def compute(self, a):\n",
    "        ### BEGIN YOUR SOLUTION\n",
    "        raise array_api.log(a)\n",
    "        ### END YOUR SOLUTION\n",
    "\n",
    "    def gradient(self, out_grad, node):\n",
    "        ### BEGIN YOUR SOLUTION\n",
    "        return (out_grad / node.inputs[0], )\n",
    "        ### END YOUR SOLUTION\n",
    "\n",
    "\n",
    "def log(a):\n",
    "    return Log()(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fb3f80-08e8-46d1-8f45-f7f4a65a5f5a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baaecd5-acfc-4037-83a2-f7d1a56a19b5",
   "metadata": {},
   "source": [
    "Certainly! Here's the proof and explanation for the derivative of the `Exp` operator:\n",
    "\n",
    "Let's denote `a` as the tensor on which the exponential function is applied. The operation can be described as `f(a) = \\exp(a)`, where `\\exp` represents the exponential function.\n",
    "\n",
    "The function for the backward pass (i.e., the gradient) is `df/da = \\exp(a)`.\n",
    "\n",
    "The LaTeX document will look as follows:\n",
    "\n",
    "\n",
    "We are given a function $f(a) = \\exp(a)$, where $a$ is a tensor. Our task is to find the derivative of this function with respect to $a$.\n",
    "\n",
    "By differentiating the function $f(a)$ with respect to $a$, we find:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{da} &= \\frac{d}{da} (\\exp(a)) \\\\\n",
    "&= \\exp(a)\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, the gradient of $f(a)$ with respect to $a$ is $\\exp(a)$.\n",
    "\n",
    "\n",
    "We starts by defining the function `f(a) = \\exp(a)`, where `\\exp` represents the exponential function. It then explains that when we differentiate `f(a)` with respect to `a`, we find that the derivative is `\\exp(a)`. This means that the gradient of `f(a)` with respect to `a` is `\\exp(a)`, which represents the behavior of the `Exp` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e3bb271-3961-409a-b450-64652a42345f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Exp(TensorOp):\n",
    "    def compute(self, a):\n",
    "        ### BEGIN YOUR SOLUTION\n",
    "        self.out = array_api.exp(a)\n",
    "        return self.out\n",
    "        ### END YOUR SOLUTION\n",
    "\n",
    "    def gradient(self, out_grad, node):\n",
    "        ### BEGIN YOUR SOLUTION\n",
    "        return out_grad * (exp(node.inputs[0])), \n",
    "        ### END YOUR SOLUTION\n",
    "\n",
    "\n",
    "def exp(a):\n",
    "    return Exp()(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef92862d-2b89-44e6-b684-955e747bc7f9",
   "metadata": {},
   "source": [
    "Certainly! Here's the proof and explanation for the derivative of the `ReLU` (Rectified Linear Unit) operator:\n",
    "\n",
    "Let's denote `a` as the tensor on which the ReLU function is applied. The ReLU function is defined as follows: \n",
    "\n",
    "\\[\n",
    "f(a) = \n",
    "\\begin{cases}\n",
    "a, & \\text{if } a \\geq 0 \\\\\n",
    "0, & \\text{if } a < 0\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "The function for the backward pass (i.e., the gradient) is `df/da = 1` if `a >= 0`, and `df/da = 0` if `a < 0`.\n",
    "\n",
    "The LaTeX document will look as follows:\n",
    "\n",
    "\n",
    "We are given a function $f(a) = \\max(0, a)$, where $a$ is a tensor. Our task is to find the derivative of this function with respect to $a$.\n",
    "\n",
    "By considering the definition of the ReLU function, we can write $f(a)$ as:\n",
    "\n",
    "$$\n",
    "f(a) = \n",
    "\\begin{cases}\n",
    "a, & \\text{if } a \\geq 0 \\\\\n",
    "0, & \\text{if } a < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Now, let's differentiate $f(a)$ with respect to $a$:\n",
    "\n",
    "$$\n",
    "\\frac{df}{da} = \n",
    "\\begin{cases}\n",
    "1, & \\text{if } a \\geq 0 \\\\\n",
    "0, & \\text{if } a < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Therefore, the gradient of $f(a)$ with respect to $a$ is $1$ if $a \\geq 0$, and $0$ if $a < 0$.\n",
    "\n",
    "\\end{document}\n",
    "```\n",
    "\n",
    "This document starts by defining the function `f(a) = \\max(0, a)`, which represents the ReLU function. It then explains that when we differentiate `f(a)` with respect to `a`, we find that the derivative is `1` if `a >= 0`, and `0` if `a < 0`. This means that the gradient of `f(a)` with respect to `a` is `1` for positive values of `a` and `0` for negative values of `a`.\n",
    "\n",
    "Please note that the `gradient` method of the `ReLU` operator is not implemented in the provided code, as indicated by `NotImplementedError()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385a5e66-8a02-4d0a-8ffb-29cd421dfcd7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a9d69c61-2298-4942-aabb-12bf3d048cf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "class ReLU(TensorOp):\n",
    "    def compute(self, a):\n",
    "        ### BEGIN YOUR SOLUTION\n",
    "        self.out = array_api.clip(a, a_min=0)\n",
    "        return self.out\n",
    "        ### END YOUR SOLUTION\n",
    "\n",
    "    def gradient(self, out_grad, node):\n",
    "        ### BEGIN YOUR SOLUTION\n",
    "        return 1 * self.out\n",
    "        ### END YOUR SOLUTION\n",
    "\n",
    "\n",
    "def relu(a):\n",
    "    return ReLU()(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
