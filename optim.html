<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="The Optim module in minima is a flexible and powerful toolbox for optimizing the parameters of your deep learning models. Built on a high-level,">

<title>minima - optim</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="styles.css">
<meta property="og:title" content="minima - optim">
<meta property="og:description" content="The Optim module in minima is a flexible and powerful toolbox for optimizing the parameters of your deep learning models. Built on a high-level,">
<meta property="og:site-name" content="minima">
<meta name="twitter:title" content="minima - optim">
<meta name="twitter:description" content="The Optim module in minima is a flexible and powerful toolbox for optimizing the parameters of your deep learning models. Built on a high-level,">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">minima</span>
    </a>
  </div>
        <div class="quarto-navbar-tools ms-auto">
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./optim.html">optim</a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome to minima</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./autograd.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">autograd</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./operators.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">operators</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./init.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">init</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./nn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">nn</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./optim.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">optim</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">data</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ndarray.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ndarray</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ndarray_backend_numpy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ndarray_backend_numpy</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./utility.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">utility</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#optimizer" id="toc-optimizer" class="nav-link active" data-scroll-target="#optimizer">Optimizer</a></li>
  <li><a href="#sgd-optimizer" id="toc-sgd-optimizer" class="nav-link" data-scroll-target="#sgd-optimizer">SGD Optimizer</a>
  <ul class="collapse">
  <li><a href="#sgd" id="toc-sgd" class="nav-link" data-scroll-target="#sgd">SGD</a></li>
  </ul></li>
  <li><a href="#adagrad-optimizer" id="toc-adagrad-optimizer" class="nav-link" data-scroll-target="#adagrad-optimizer">AdaGrad Optimizer</a>
  <ul class="collapse">
  <li><a href="#adagrad" id="toc-adagrad" class="nav-link" data-scroll-target="#adagrad">AdaGrad</a></li>
  <li><a href="#rmsprop-optimizer" id="toc-rmsprop-optimizer" class="nav-link" data-scroll-target="#rmsprop-optimizer">RMSProp Optimizer</a></li>
  <li><a href="#rmsprop" id="toc-rmsprop" class="nav-link" data-scroll-target="#rmsprop">RMSProp</a></li>
  </ul></li>
  <li><a href="#adam-optimizer" id="toc-adam-optimizer" class="nav-link" data-scroll-target="#adam-optimizer">Adam Optimizer</a>
  <ul class="collapse">
  <li><a href="#adam" id="toc-adam" class="nav-link" data-scroll-target="#adam">Adam</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/m0saan/minima/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">optim</h1>
</div>

<div>
  <div class="description">
    The <code>Optim</code> module in minima is a flexible and powerful toolbox for optimizing the parameters of your deep learning models. Built on a high-level,
  </div>
</div>


<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->
<pre><code>/opt/hostedtoolcache/Python/3.9.17/x64/lib/python3.9/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Raises
  else: warn(msg)</code></pre>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/optim.py#L14" target="_blank" style="float:right; font-size:smaller">source</a></p>
<section id="optimizer" class="level3">
<h3 class="anchored" data-anchor-id="optimizer">Optimizer</h3>
<blockquote class="blockquote">
<pre><code> Optimizer (params)</code></pre>
</blockquote>
<p>Base class for all optimizers. Not meant to be instantiated directly.</p>
<p>This class represents the abstract concept of an optimizer, and contains methods that all concrete optimizer classes must implement. It is designed to handle the parameters of a machine learning model, providing functionality to perform a step of optimization and to zero out gradients.</p>
<table class="table">
<thead>
<tr class="header">
<th></th>
<th><strong>Type</strong></th>
<th><strong>Details</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>params</td>
<td>Iterable</td>
<td>The parameters of the model to be optimized.</td>
</tr>
</tbody>
</table>
</section>
<section id="sgd-optimizer" class="level2">
<h2 class="anchored" data-anchor-id="sgd-optimizer">SGD Optimizer</h2>
<p>This is a PyTorch-style implementation of the classic optimizer Stochastic Gradient Descent (SGD).</p>
<p>SGD update is,</p>
<p><span class="math display">\[
\theta_{t} = \theta_{t-1} - \alpha \cdot g_{t}
\]</span></p>
<p>where <span class="math inline">\(\alpha\)</span> is the learning rate, and <span class="math inline">\(g_{t}\)</span> is the gradient at time step <span class="math inline">\(t\)</span>. <span class="math inline">\(θ_{t}\)</span> represents the model parameters at time step <span class="math inline">\(t\)</span>.</p>
<p>The learning rate <span class="math inline">\(\alpha\)</span> is a scalar hyperparameter that controls the size of the update at each iteration.</p>
<p>An optional momentum term can be added to the update rule:</p>
<p><span class="math display">\[
\begin{align*}
v_{t} &amp; \leftarrow \mu v_{t-1} + (1-\mu) \cdot g_t \\
\theta_{t} &amp; \leftarrow \theta_{t-1} - \alpha \cdot v_t
\end{align*}
\]</span></p>
<p>where <span class="math inline">\(v_{t}\)</span> is the momentum term at time step <span class="math inline">\(t\)</span>, and <span class="math inline">\(\mu\)</span> is the momentum factor. The momentum term increases for dimensions whose gradients point in the same<br>
direction and reduces updates for dimensions whose gradients change direction, thereby adding a form of preconditioning.</p>
<p>A weight decay term can also be included, which adds a regularization effect:</p>
<p><span class="math display">\[
\theta_{t} = (1 - \alpha \cdot \lambda) \cdot \theta_{t-1} - \alpha \cdot g_t
\]</span></p>
<p>where <span class="math inline">\(\lambda\)</span> is the weight decay factor. This results in the model weights shrinking at each time step, which can prevent overfitting by keeping the model complexity in check.</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/optim.py#L63" target="_blank" style="float:right; font-size:smaller">source</a></p>
<section id="sgd" class="level3">
<h3 class="anchored" data-anchor-id="sgd">SGD</h3>
<blockquote class="blockquote">
<pre><code> SGD (params, lr=0.01, momentum=0.0, wd=0.0)</code></pre>
</blockquote>
<p>Implements stochastic gradient descent (optionally with momentum).</p>
<p>This is a basic optimizer that’s suitable for many machine learning models, and is often used as a baseline for comparing other optimizers’ performance.</p>
<table class="table">
<thead>
<tr class="header">
<th></th>
<th><strong>Type</strong></th>
<th><strong>Default</strong></th>
<th><strong>Details</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>params</td>
<td>Iterable</td>
<td></td>
<td>The parameters of the model to be optimized.</td>
</tr>
<tr class="even">
<td>lr</td>
<td>float</td>
<td>0.01</td>
<td>The learning rate.</td>
</tr>
<tr class="odd">
<td>momentum</td>
<td>float</td>
<td>0.0</td>
<td>The momentum factor.</td>
</tr>
<tr class="even">
<td>wd</td>
<td>float</td>
<td>0.0</td>
<td>The weight decay (L2 regularization).</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="adagrad-optimizer" class="level2">
<h2 class="anchored" data-anchor-id="adagrad-optimizer">AdaGrad Optimizer</h2>
<blockquote class="blockquote">
<p><strong>Intuitive explanation:</strong></p>
</blockquote>
<p>Imagine you’re trying to navigate your way across a complex terrain - like a big mountain with lots of hills, valleys and flat areas.<br>
Your goal is to find the lowest valley. This is much like the problem a neural network faces when it’s trying to find the optimal values for its weights - the lowest point in its loss function.</p>
<p>You start at a random point on this terrain, which is like initializing your model with random weights. Now, you need to figure out which direction to go in to get to the lowest point.<br>
You can’t see the whole terrain at once, but you can look around your current location and see which way is downhill. This is like calculating the gradient of the loss function with respect to the weights.</p>
<p>In a basic gradient descent algorithm, you would just go in the direction of the steepest slope with a fixed step size. But this approach can lead to problems.<br>
What if you’re on a steep slope and you take too big of a step? You might overshoot the valley you’re trying to get to. Or, what if you’re on a flat part of the<br>
terrain and you take too small of a step? You might get stuck and not make much progress.</p>
<p>This is where AdaGrad comes in. AdaGrad is like a smart hiker that adjusts its step size based on the terrain it’s currently on.<br>
If it’s on a steep slope, it takes smaller steps to avoid overshooting the valley. If it’s on a flat area, it takes bigger steps to make faster progress.</p>
<p>It does this by keeping track of the sum of the squares of the gradients that it has seen so far (kinda like a memory), and uses this to scale down the step size.<br>
This means that parameters with larger gradients will have their learning rate decreased more, while parameters with smaller gradients will have their learning rate</p>
<p>The neat thing about AdaGrad is that it adjusts the learning rate for each parameter individually, based on what it’s learned about the landscape around that parameter.<br>
This can be especially useful when dealing with sparse data, where only a few parameters might be updated frequently.</p>
<blockquote class="blockquote">
<p><strong>Detailed explanation</strong></p>
</blockquote>
<p>Building on the foundational concepts of Stochastic Gradient Descent (SGD), we have AdaGrad, an algorithm that introduces an innovative twist to the optimization process.<br>
Unlike traditional SGD that utilizes a single learning rate <span class="math inline">\(\alpha\)</span> across all parameters, AdaGrad institutes a per-parameter learning rate. The learning rate for AdaGrad is computed as:</p>
<p><span class="math display">\[
\theta_{t} = \theta_{t-1} - \frac{\alpha}{\sqrt{G_t + \epsilon}} \cdot g_{t}
\]</span></p>
<p>where <span class="math inline">\(\theta_{t}\)</span> represents the model parameters at time step <span class="math inline">\(t\)</span>, <span class="math inline">\(\alpha\)</span> is the initial learning rate, <span class="math inline">\(g_{t}\)</span> is the gradient at time step <span class="math inline">\(t\)</span>, <span class="math inline">\(G_{t}\)</span> is a diagonal matrix<br>
where each diagonal element <span class="math inline">\(i, i\)</span> is the sum of the squares of the gradients w.r.t. <span class="math inline">\(\theta_i\)</span> up to time step <span class="math inline">\(t\)</span>, and <span class="math inline">\(\epsilon\)</span> is a smoothing term to avoid division by zero (usually on the order of <span class="math inline">\(1e-7\)</span>).</p>
<p>In AdaGrad, each parameter <span class="math inline">\(\theta_i\)</span> gets its own learning rate, which is inversely proportional to the square root of the sum of the squares of past gradients.<br>
This is the <code>cache</code> in the implementation, which holds a history of squared gradients. The greater the sum of the past gradients for a particular parameter, the smaller the learning rate for that parameter.</p>
<p>This feature allows AdaGrad to normalize the updates made during training, preventing any single weight from rising too high compared to the others.<br>
This is particularly beneficial when dealing with sparse data, as the less frequently updated parameters are allowed larger updates when they do get updated, thereby effectively utilizing more neurons for training.</p>
<p>However, it’s important to note that AdaGrad has a tendency to decrease the learning rate quite aggressively due to the constant accumulation of the square of gradients in <span class="math inline">\(G_{t}\)</span>.<br>
This can sometimes lead to premature and excessive decay of the learning rate during training, causing the model to stop learning before reaching the optimal point.<br>
This monotonic decrease in the learning rate is one reason AdaGrad is not as widely used, except in some specific applications.</p>
<p>To summarize, AdaGrad adds a valuable tool to our optimization toolkit by providing an adaptive learning rate for each individual parameter.<br>
It elegantly solves the problem of learning rate selection and normalization of parameter updates, and while it has some limitations, it’s a<br>
powerful concept that has paved the way for further innovations in optimization algorithms.</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/optim.py#L130" target="_blank" style="float:right; font-size:smaller">source</a></p>
<section id="adagrad" class="level3">
<h3 class="anchored" data-anchor-id="adagrad">AdaGrad</h3>
<blockquote class="blockquote">
<pre><code> AdaGrad (params, lr=0.001, wd=0.0, eps=1e-07)</code></pre>
</blockquote>
<p>Implements AdaGrad optimization algorithm.</p>
<p>AdaGrad is an optimizer with parameter-wise learning rates, which adapts the learning rate based on how frequently a parameter gets updated during training. It’s particularly useful for sparse data.</p>
<table class="table">
<thead>
<tr class="header">
<th></th>
<th><strong>Type</strong></th>
<th><strong>Default</strong></th>
<th><strong>Details</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>params</td>
<td>Iterable</td>
<td></td>
<td>The parameters of the model to be optimized.</td>
</tr>
<tr class="even">
<td>lr</td>
<td>float</td>
<td>0.001</td>
<td>The initial learning rate.</td>
</tr>
<tr class="odd">
<td>wd</td>
<td>float</td>
<td>0.0</td>
<td>The weight decay (L2 regularization).</td>
</tr>
<tr class="even">
<td>eps</td>
<td>float</td>
<td>1e-07</td>
<td>A small constant for numerical stability.</td>
</tr>
</tbody>
</table>
</section>
<section id="rmsprop-optimizer" class="level3">
<h3 class="anchored" data-anchor-id="rmsprop-optimizer">RMSProp Optimizer</h3>
<p>RMSProp, short for Root Mean Square Propagation, which is an optimization algorithm that introduces an adaptive learning rate for each parameter in a model.</p>
<p>RMSProp introduces an adaptive learning rate for each parameter to tackle different landscapes of the loss function. It does this by maintaining a moving (or ‘running’) average<br>
of the squared gradients, effectively measuring the scale of recent gradients. This running average, also known as the cache, is calculated as follows:</p>
<p><span class="math display">\[
cache_{t} = \rho \cdot cache_{t-1} + (1-\rho) \cdot (g_{t})^2
\]</span></p>
<p>where <span class="math inline">\(\rho\)</span> is the decay rate that determines how much of the history of squared gradients we retain. This cache term holds a form of “memory” of the magnitude of recent gradients, and its contents “move” with the data over time.</p>
<p>Then, the parameter update rule becomes:</p>
<p><span class="math display">\[
\theta_{t} = \theta_{t-1} - \frac{\alpha}{\sqrt{cache_{t} + \epsilon}} \cdot g_{t}
\]</span></p>
<p>where <span class="math inline">\(\epsilon\)</span> is a small constant for numerical stability, often around <span class="math inline">\(1e-8\)</span>. This normalization by the square root of the cache ensures smooth changes in the learning rate and<br>
helps retain the global direction of parameter updates. This adaptivity makes the learning rate changes more resilient to fluctuations in the gradient.</p>
<p>RMSProp introduces a new hyperparameter, <span class="math inline">\(\rho\)</span>, the cache memory decay rate. Given the momentum-like properties of RMSProp, even small gradient updates can have substantial effects<br>
due to the adaptive learning rate updates. As such, the default learning rate often used with RMSProp is smaller, around <span class="math inline">\(0.001\)</span>, to ensure stability.</p>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/optim.py#L194" target="_blank" style="float:right; font-size:smaller">source</a></p>
</section>
<section id="rmsprop" class="level3">
<h3 class="anchored" data-anchor-id="rmsprop">RMSProp</h3>
<blockquote class="blockquote">
<pre><code> RMSProp (params, lr=0.001, wd=0.0, eps=1e-07, rho=0.9)</code></pre>
</blockquote>
<p>Implements RMSProp optimization algorithm.</p>
<p>RMSProp is an optimizer with parameter-wise adaptive learning rates, which adapt the learning rate for each parameter individually, making it suitable for dealing with sparse or multi-scale data.</p>
<table class="table">
<colgroup>
<col style="width: 6%">
<col style="width: 25%">
<col style="width: 34%">
<col style="width: 34%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th><strong>Type</strong></th>
<th><strong>Default</strong></th>
<th><strong>Details</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>params</td>
<td>Iterable</td>
<td></td>
<td>The parameters of the model to be optimized.</td>
</tr>
<tr class="even">
<td>lr</td>
<td>float</td>
<td>0.001</td>
<td>The initial learning rate.</td>
</tr>
<tr class="odd">
<td>wd</td>
<td>float</td>
<td>0.0</td>
<td>The weight decay (L2 regularization).</td>
</tr>
<tr class="even">
<td>eps</td>
<td>float</td>
<td>1e-07</td>
<td>A small constant for numerical stability.</td>
</tr>
<tr class="odd">
<td>rho</td>
<td>float</td>
<td>0.9</td>
<td>The decay rate for the moving average of squared gradients.</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="adam-optimizer" class="level2">
<h2 class="anchored" data-anchor-id="adam-optimizer">Adam Optimizer</h2>
<p>This is a PyTorch-like implementation of popular optimizer <em>Adam</em> from paper <a href="https://papers.labml.ai/paper/1412.6980">Adam: A Method for Stochastic Optimization</a>.</p>
<p><em>Adam</em> update is, <span class="math display">\[
\begin{align}
m_t &amp;\leftarrow \beta_1 m_{t-1} + (1 - \beta_1) \cdot g_t \\
v_t &amp;\leftarrow \beta_2 v_{t-1} + (1 - \beta_2) \cdot g_t^2 \\
\hat{m}_t &amp;\leftarrow \frac{m_t}{1-\beta_1^t} \\
\hat{v}_t &amp;\leftarrow \frac{v_t}{1-\beta_2^t} \\
\theta_t &amp;\leftarrow \theta_{t-1} - \alpha \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
\end{align}
\]</span> where <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta_1\)</span>, <span class="math inline">\(\beta_2\)</span> and <span class="math inline">\(\epsilon\)</span> are scalar hyper parameters. <span class="math inline">\(m_t\)</span> and <span class="math inline">\(v_t\)</span> are first and second order moments. <span class="math inline">\(\hat{m}_t\)</span> and <span class="math inline">\(\hat{v}_t\)</span> are biased corrected moments. <span class="math inline">\(\epsilon\)</span> is used as a fix for division by zero error, but also acts as a form of a hyper-parameter that acts against variance in gradients.</p>
<p>Effective step taken assuming <span class="math inline">\(\epsilon = 0\)</span> is, <span class="math display">\[\Delta t = \alpha \cdot \frac{\hat{m}_t}{\hat{v}_t}\]</span> This is bounded by, <span class="math display">\[\vert \Delta t \vert \le \alpha \cdot \frac{1 - \beta_1}{\sqrt{1-\beta_2}}\]</span> when <span class="math inline">\(1-\beta_1 \gt \sqrt{1-\beta_2}\)</span> and <span class="math display">\[\vert \Delta t\vert  \le \alpha\]</span> otherwise. And in most common scenarios, <span class="math display">\[\vert \Delta t \vert \approx \alpha\]</span></p>
<pre><code>/opt/hostedtoolcache/Python/3.9.17/x64/lib/python3.9/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Attributes
  else: warn(msg)</code></pre>
<hr>
<p><a href="https://github.com/m0saan/minima/blob/main/minima/optim.py#L261" target="_blank" style="float:right; font-size:smaller">source</a></p>
<section id="adam" class="level3">
<h3 class="anchored" data-anchor-id="adam">Adam</h3>
<blockquote class="blockquote">
<pre><code> Adam (params, lr=1e-05, beta1=0.9, beta2=0.999, eps=1e-08,
       weight_decay=0.0)</code></pre>
</blockquote>
<p>Implements the Adam optimization algorithm.</p>
<p>Adam is an adaptive learning rate optimization algorithm that has been designed specifically for training deep neural networks. It leverages the power of adaptive learning rates methods to find individual learning rates for each parameter.</p>
<table class="table">
<thead>
<tr class="header">
<th></th>
<th><strong>Type</strong></th>
<th><strong>Default</strong></th>
<th><strong>Details</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>params</td>
<td>Iterable</td>
<td></td>
<td><code>params</code> is the list of parameters</td>
</tr>
<tr class="even">
<td>lr</td>
<td>float</td>
<td>1e-05</td>
<td><code>lr</code> is the learning rate <span class="math inline">\(\alpha\)</span></td>
</tr>
<tr class="odd">
<td>beta1</td>
<td>float</td>
<td>0.9</td>
<td>The exponential decay rate for the first moment estimates. Default is 0.9.</td>
</tr>
<tr class="even">
<td>beta2</td>
<td>float</td>
<td>0.999</td>
<td>The exponential decay rate for the second moment estimates. Default is 0.999.</td>
</tr>
<tr class="odd">
<td>eps</td>
<td>float</td>
<td>1e-08</td>
<td><code>eps</code> is <span class="math inline">\(\hat{\epsilon}\)</span> or <span class="math inline">\(\epsilon\)</span> based on <code>optimized_update</code></td>
</tr>
<tr class="even">
<td>weight_decay</td>
<td>float</td>
<td>0.0</td>
<td>is an instance of class <code>WeightDecay</code> defined in <a href="index.html"><code>__init__.py</code></a></td>
</tr>
</tbody>
</table>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>