[
  {
    "objectID": "ndarray.html",
    "href": "ndarray.html",
    "title": "ndarray",
    "section": "",
    "text": "source\n\ndefault_device\n\n default_device ()\n\n\nsource\n\n\ncpu_numpy\n\n cpu_numpy ()\n\nReturn numpy device\n\nsource\n\n\nBackendDevice\n\n BackendDevice (name, mod)\n\nA backend device, wraps the implementation module.\n\nsource\n\n\nNDArray\n\n NDArray (value:Union[ForwardRef('NDArray'),numpy.ndarray,Sequence],\n          device:Optional[__main__.BackendDevice]=None)\n\nNDArray represents a n-dimensional array with operations that can be performed on multiple devices. This class is an abstraction over numpy and other backend devices, providing a unified interface to interact with arrays.\nUse cases of this class include numerical operations, scientific computing, and machine learning.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nvalue\ntyping.Union[ForwardRef(‘NDArray’), numpy.ndarray, typing.Sequence]\n\nThe value on which to create the NDArray from\n\n\ndevice\ntyping.Optional[main.BackendDevice]\nNone\nThe device on which the array computations are performed.\n\n\nReturns\nNone\n\n\n\n\n\nThe concept of “strides” is crucial to understanding how multi-dimensional arrays are stored and accessed in memory. The term “stride” refers to the number of elements (or steps) you need to move in memory to go from one element to the next along a particular axis of an array.\nIn the context of PyTorch and many other libraries that work with multi-dimensional arrays, the strides attribute of a tensor gives you the number of elements you need to skip in memory to move one step along each dimension of the tensor.\nThe strides of a tensor are defined as a tuple of integers, where each integer represents the step size for the corresponding dimension.\nLet’s look at your example:\nx = torch.arange(24).reshape(2,3,4)\nprint(x.stride()) # outputs: (12, 4, 1)\nHere, the tensor x has a shape of (2,3,4), and the stride is (12,4,1).\n\nThe first element of the stride tuple, 12, tells you that you need to step over 12 elements in memory to get from one element to the next along the first axis (axis=0, the one that has size 2). This makes sense, because there are 12 elements in each “block” of this dimension (3*4 = 12).\nThe second element, 4, says that you need to step over 4 elements in memory to move from one element to the next along the second axis (axis=1, the one that has size 3). This is because there are 4 elements in each “row” of this dimension.\nThe last element, 1, shows that you only need to step over 1 element in memory to move from one element to the next along the last axis (axis=2, the one that has size 4). This is because elements along this axis are contiguous in memory.\n\nIn conclusion, the concept of strides is critical for efficient storage and computations on multi-dimensional arrays, as it allows libraries like PyTorch to perform complex operations without needing to actually rearrange or copy any data in memory.\nLet’s start with a simple 1D case. Imagine we have an array of size 10 and we want to select every second element. Instead of physically copying every second element to a new array, we could just create a new “view” of the array with a stride of 2. This means that to move to the next element in our sliced array, we jump over 2 elements in the original data.\nFor multi-dimensional arrays, the principle is the same but each dimension has its own stride. When you slice a tensor, you’re essentially creating a new tensor (a view) that starts from a different offset and possibly uses different strides.\nConsider a 2D case: if you slice the first dimension (e.g., array[1:, :]), you’re changing the offset to start from the second element along that dimension. Essentially, you’re jumping over a number of elements equal to the stride of that dimension.\nIf you slice the second dimension (e.g., array[:, ::2] to select every second column), you’re not changing the offset, but you’re doubling the stride for the second dimension. This tells the tensor to skip one element in memory for every step in that dimension, giving you every second column.\nIn summary, slicing doesn’t involve any data copying. Instead, it changes the starting point (offset) and how you move along each dimension (stride) of the tensor. This makes slicing operations very efficient, even on large tensors.\n\nimport nbdev; nbdev.nbdev_export()"
  },
  {
    "objectID": "autograd.html",
    "href": "autograd.html",
    "title": "autograd",
    "section": "",
    "text": "In calculus, the derivative of a function at a certain point is a measure of how the function changes at that point. It is defined as the limit of the ratio of the change in the function value (f(x)) to the change in the x value (Δx) as Δx approaches zero. This can be written as:\n\\[f'(x) = \\lim_{{Δx \\to 0}} \\frac{{f(x + Δx) - f(x)}}{{Δx}}\\]\nThis equation represents the slope of the tangent line to the function at a specific point x, which can also be interpreted as the instantaneous rate of change of the function at that point.\nIf you have a function y = f(x) = x^n, where n is a constant, the power rule of differentiation tells us that the derivative of f(x) with respect to x is:\n\\[f'(x) = n * x^{n-1}\\]\nIn the context of the function d = a*b + c which we’re going to use below, since a is the variable and b and c are constants, the derivative of d with respect to a is just b. This can be written in LaTeX as:\n\\[ \\frac{{dd}}{{da}} = b \\]\nwe begin by assigning values to three variables a, b, and c. We then create a fourth variable, d, which is equal to the product of a and b, added to c. When you execute this cell, it should display the value of d.\n\na = 4\nb = -2\nc = 11\nd = a*b + c\nd\n\n3\n\n\nwe define a function f_a(a,b,c), which helps us estimate the slope of the function at the point a. The function first calculates d1 using the given inputs, a, b, and c. Then it increments a by a small value h and recalculates the value d2. The function then prints the original d1, the new d2, and the estimated slope which is (d2 - d1) / h.\n\ndef f_a(a,b,c):\n    h = 0.01\n    d1 = a*b + c\n    a += h\n    d2 = a*b + c\n    \n    print(f'd1: {d1}')\n    print(f'd2: {d2}')\n    print(f'slope: {(d2 - d1) / h}')\n    \nf_a(a,b,c)\n\nd1: 3\nd2: 2.9800000000000004\nslope: -1.9999999999999574\n\n\nthat states that the derivative of d with respect to a, denoted as (db/da), is analytically equal to b. This is because in the expression d = a*b + c, the coefficient of a is b, so by the power rule of differentiation, the derivative is b. In this case, b equals -2.\nNow if we do this with b\n\ndef f_b(a,b,c):\n    h = 0.01\n    d1 = a*b + c\n    b += h\n    d2 = a*b + c\n    \n    print(f'd1: {d1}')\n    print(f'd2: {d2}')\n    print(f'slope: {(d2 - d1) / h}')\n    \nf_b(a,b,c)\n\nd1: 3\nd2: 3.04\nslope: 4.0000000000000036\n\n\nHere’s what happens in the function: 1. It begins by defining a small change h which is set to 0.01. 2. Then, the function calculates d1, which is the result of a*b + c with the original inputs a, b, and c. 3. It increments b by the small value h. 4. Next, the function calculates a new d2, which is the result of a*b + c after the increment to b. 5. Finally, the function prints out the original d1, the new d2, and the estimated slope calculated as (d2 - d1) / h.\nWhen you call f_b(a,b,c), the function performs all these operations using the values of a, b, and c from the previous context.\nThe output will give you an approximate value of the derivative of d with respect to b (noted as dd/db in mathematical notation), assuming that the function d(a, b, c) = a*b + c is relatively smooth and continuous near the point b."
  },
  {
    "objectID": "autograd.html#derivatives",
    "href": "autograd.html#derivatives",
    "title": "autograd",
    "section": "",
    "text": "In calculus, the derivative of a function at a certain point is a measure of how the function changes at that point. It is defined as the limit of the ratio of the change in the function value (f(x)) to the change in the x value (Δx) as Δx approaches zero. This can be written as:\n\\[f'(x) = \\lim_{{Δx \\to 0}} \\frac{{f(x + Δx) - f(x)}}{{Δx}}\\]\nThis equation represents the slope of the tangent line to the function at a specific point x, which can also be interpreted as the instantaneous rate of change of the function at that point.\nIf you have a function y = f(x) = x^n, where n is a constant, the power rule of differentiation tells us that the derivative of f(x) with respect to x is:\n\\[f'(x) = n * x^{n-1}\\]\nIn the context of the function d = a*b + c which we’re going to use below, since a is the variable and b and c are constants, the derivative of d with respect to a is just b. This can be written in LaTeX as:\n\\[ \\frac{{dd}}{{da}} = b \\]\nwe begin by assigning values to three variables a, b, and c. We then create a fourth variable, d, which is equal to the product of a and b, added to c. When you execute this cell, it should display the value of d.\n\na = 4\nb = -2\nc = 11\nd = a*b + c\nd\n\n3\n\n\nwe define a function f_a(a,b,c), which helps us estimate the slope of the function at the point a. The function first calculates d1 using the given inputs, a, b, and c. Then it increments a by a small value h and recalculates the value d2. The function then prints the original d1, the new d2, and the estimated slope which is (d2 - d1) / h.\n\ndef f_a(a,b,c):\n    h = 0.01\n    d1 = a*b + c\n    a += h\n    d2 = a*b + c\n    \n    print(f'd1: {d1}')\n    print(f'd2: {d2}')\n    print(f'slope: {(d2 - d1) / h}')\n    \nf_a(a,b,c)\n\nd1: 3\nd2: 2.9800000000000004\nslope: -1.9999999999999574\n\n\nthat states that the derivative of d with respect to a, denoted as (db/da), is analytically equal to b. This is because in the expression d = a*b + c, the coefficient of a is b, so by the power rule of differentiation, the derivative is b. In this case, b equals -2.\nNow if we do this with b\n\ndef f_b(a,b,c):\n    h = 0.01\n    d1 = a*b + c\n    b += h\n    d2 = a*b + c\n    \n    print(f'd1: {d1}')\n    print(f'd2: {d2}')\n    print(f'slope: {(d2 - d1) / h}')\n    \nf_b(a,b,c)\n\nd1: 3\nd2: 3.04\nslope: 4.0000000000000036\n\n\nHere’s what happens in the function: 1. It begins by defining a small change h which is set to 0.01. 2. Then, the function calculates d1, which is the result of a*b + c with the original inputs a, b, and c. 3. It increments b by the small value h. 4. Next, the function calculates a new d2, which is the result of a*b + c after the increment to b. 5. Finally, the function prints out the original d1, the new d2, and the estimated slope calculated as (d2 - d1) / h.\nWhen you call f_b(a,b,c), the function performs all these operations using the values of a, b, and c from the previous context.\nThe output will give you an approximate value of the derivative of d with respect to b (noted as dd/db in mathematical notation), assuming that the function d(a, b, c) = a*b + c is relatively smooth and continuous near the point b."
  },
  {
    "objectID": "autograd.html#derivatives-in-the-context-of-neural-nets-autograd",
    "href": "autograd.html#derivatives-in-the-context-of-neural-nets-autograd",
    "title": "autograd",
    "section": "Derivatives in the context of neural nets (Autograd)",
    "text": "Derivatives in the context of neural nets (Autograd)\nAutomatic differentiation, or auto grad as it’s often referred to in the context of deep learning, is a powerful tool that greatly simplifies the process of working with derivatives. It does this by automatically computing the derivatives (or gradients) of functions, thus relieving the need to manually calculate these derivatives as we have done above.\nThe use of auto grad is fundamental to the training process of deep learning models. Deep learning models, such as neural networks, are essentially complex mathematical functions with many parameters (weights and biases). Training these models involves adjusting these parameters to minimize a loss function, which quantifies how well the model is performing on a given task. The most common method for doing this is gradient descent, which uses the gradients of the loss function with respect to the parameters to update the parameters in a way that decreases the loss.\nHowever, the manual calculation of these gradients, especially for complex models, is not only tedious but also prone to errors. Here’s where auto grad comes in. By using automatic differentiation, we can compute these gradients automatically and accurately, no matter how complex the model is.\nIn a deep learning framework, when we define our model and loss function, the framework uses auto grad to build a computational graph under the hood. This graph captures all the computations that are done in the forward pass (i.e., when we pass our inputs through the model to get the output). Then, when we need to compute the gradients during the backward pass, the framework uses this computational graph and the chain rule from calculus to compute the gradients automatically. This process is often referred to as backpropagation.\nThe main advantage of using auto grad in deep learning is that it allows us to focus on designing our models and defining our loss functions without worrying about the details of computing the gradients. This simplifies our code, reduces the chance of errors, and allows for greater flexibility in designing complex models. In fact, with auto grad, we can easily experiment with new types of models and loss functions, as we can rely on the framework to correctly compute the gradients no matter how complex our design is.\nLet’s start building a mini autograd engine\nIn the context of deep learning and automatic differentiation, the Value class is designed to encapsulate a scalar value and its relationships within a computational graph. This abstraction is essential for constructing mathematical expressions from basic operations and for performing the forward pass, which evaluates the expression.\nThe Value class is initialized with data and optional parameters specifying its children (or dependencies) and the operation that produced it. Each instance of the Value class can have a gradient, which is initialized as zero and can be updated during backpropagation.\nTwo fundamental operations are implemented for instances of the Value class: addition (add) and multiplication (mul). These methods allow two Value instances (or a Value and a scalar) to be added or multiplied, respectively. The results of these operations are also Value instances, maintaining the relationships in the computational graph.\nThis ability to build out mathematical expressions using only addition and multiplication allows for the construction of a broad variety of functions. For example, given multiple inputs (a, b, c, f), we can formulate a mathematical expression that generates a single output (l). After the forward pass, the output value is calculated and can be visualized, as demonstrated in the example where the forward pass output is -8.\nIn summary, the Value class is a fundamental building block for creating and navigating a computational graph in the context of automatic differentiation, making it an invaluable tool in any deep learning framework.\nThe code provided builds upon the previously discussed Value class, which acts as a node within a computational graph in the context of automatic differentiation. It demonstrates how to define scalar values a, b, c, and f and use them to build a computational graph. The graph computes the expression L = (a * b + c) * f, represented in nodes labeled ‘e’, ‘d’, and ‘L’.\nThe focus of this explanation is the process of backpropagation and the computation of gradients for every node in the graph, which is crucial for training neural networks. In a neural network setting, the loss function L would typically be calculated with respect to the network’s weights. Here, these weights are abstractly represented by the scalar variables a, b, c, and f.\nThe fundamental idea behind backpropagation is to compute the derivative of the output value L with respect to every node in the graph. These derivatives represent the impact each node has on the final output. They are stored in the grad attribute of the Value class, which is initialized to zero, signifying that there is initially no effect on the output.\nIn this context, a gradient of zero means changing the value of a node has no effect on the final output, or loss function. After performing backpropagation, the grad attribute will store the actual derivative of L with respect to that node. This is essential information when training a neural network because it dictates how to adjust the weights (in this example, a, b, c, and f) to minimize the loss function L.\nThe function draw_dot(L) is presumably used to visualize this computational graph, including both the data and the grad of each node. This visualization aids in understanding the forward and backward passes of computation within the graph.\nIn conclusion, this code snippet creates a simple computational graph using the Value class, computes a mathematical expression, and prepares for backpropagation. The next steps would involve the actual calculation of the gradients, enabling the iterative optimization of weights based on their influence on the final output."
  },
  {
    "objectID": "autograd.html#manual-gradient",
    "href": "autograd.html#manual-gradient",
    "title": "autograd",
    "section": "Manual gradient",
    "text": "Manual gradient\n\nbase case (L grad)\n\ndef lol():\n    h = 0.001\n    \n    a = Value(2.0, label='a')\n    b = Value(-3.0, label='b')\n    c = Value(10.0, label='c')\n    e = a*b; e.label='e'\n    d = e + c; d.label='d'\n    f = Value(-2.0, label='f')\n    L = d*f; L.label='L'\n    \n    L1 = L.data\n    \n    a = Value(2.0, label='a')\n    b = Value(-3.0, label='b')\n    c = Value(10.0, label='c')\n    e = a*b; e.label='e'\n    d = e + c; d.label='d'\n    f = Value(-2.0, label='f')\n    L = d*f; L.label='L' \n    \n    L2 = L.data + h\n    \n    print(f'grad: {(L2 - L1) / h}')\n\nlol()\n\ngrad: 1.000000000000334\n\n\nsure enough it’s 1\n\nL.grad = 1\n\n\nf\nHere is a generic version of lol\n\ndef lol(label):\n    def foo(v, label):\n        if v.label == label: v.data += h\n        \n    h = 0.001\n    \n    a = Value(2.0, label='a')\n    b = Value(-3.0, label='b')\n    c = Value(10.0, label='c')\n    e = a*b; e.label='e'\n    d = e + c; d.label='d'\n    f = Value(-2.0, label='f')\n    L = d*f; L.label='L'\n    \n    L1 = L.data\n    \n    a = Value(2.0, label='a'); foo(a, label)\n    b = Value(-3.0, label='b'); foo(b, label)\n    c = Value(10.0, label='c'); foo(c, label)\n    e = a*b; e.label='e'; foo(e, label)\n    d = e + c; d.label='d'; foo(d, label)\n    f = Value(-2.0, label='f'); foo(f, label)\n    L = d*f; L.label='L'; foo(L, label) \n    \n    L2 = L.data\n    \n    print(f'grad: {(L2 - L1) / h}')\n\nlol('f')\n\ngrad: 3.9999999999995595\n\n\n\nf.grad = 4\n\n\nlol('d')\n\ngrad: -2.000000000000668\n\n\n\nd.grad = -2\n\nLet’s draw what we have up to this point\n\n# draw_dot(L)\n\nSure, here’s the step by step derivation for each of the variables:\n\nWith respect to a:\n\nGiven that L = (a*b + c) * f, we will apply the product rule for differentiation.\nThe derivative of a*b with respect to a is b, and the derivative of c with respect to a is 0. Therefore:\n\\[\n\\frac{dL}{da} = f \\cdot \\frac{d(a*b + c)}{da} = f \\cdot (b + 0) = b \\cdot f\n\\]\n\nWith respect to b:\n\nThe derivative of a*b with respect to b is a, and the derivative of c with respect to b is 0. Therefore:\n\\[\n\\frac{dL}{db} = f \\cdot \\frac{d(a*b + c)}{db} = f \\cdot (a + 0) = a \\cdot f\n\\]\n\nWith respect to c:\n\nThe derivative of a*b with respect to c is 0, and the derivative of c with respect to c is 1. Therefore:\n\\[\n\\frac{dL}{dc} = f \\cdot \\frac{d(a*b + c)}{dc} = f \\cdot (0 + 1) = f\n\\]\n\nWith respect to f:\n\nThe derivative of (a*b + c) with respect to f is 0, and f is just f, therefore:\n\\[\n\\frac{dL}{df} = (a*b + c) \\cdot \\frac{df}{df} = a*b + c\n\\]\n\nWith respect to e (where e = a*b):\n\nThe derivative of e + c with respect to e is 1. Therefore:\n\\[\n\\frac{dL}{de} = f \\cdot \\frac{d(e + c)}{de} = f \\cdot 1 = f\n\\]\n\nWith respect to d (where d = e + c):\n\nThe derivative of d with respect to d is 1. Therefore:\n\\[\n\\frac{dL}{dd} = f \\cdot \\frac{df}{df} = f\n\\]\n\nlol('e')\n\ngrad: -2.000000000000668\n\n\n\ne.grad = -2 # 1 * d.grad\n\n\nlol('c')\n\ngrad: -1.9999999999988916\n\n\n\nc.grad = -2 # 1 * d.grad\n\n\n# draw_dot(L)\n\n\nlol('a')\n\ngrad: 6.000000000000227\n\n\n\na.grad = 6  # b * e.grad\n\n\nlol('b')\n\ngrad: -3.9999999999995595\n\n\n\nb.grad = -4 # a * e.grad\n\n\n# draw_dot(L)\n\n\na = Value(2.0, label='a')\nb = Value(-3.0, label='b')\nc = Value(10.0, label='c')\ne = a*b; e.label='e'\nd = e + c; d.label='d'\nf = Value(-2.0, label='f')\nL = d*f; L.label='L' \n\n# draw_dot(L)\n\n\nL.grad = 1\n\n\nL._backward()\n\n\n# draw_dot(L)\n\n\nd._backward()\n\n\n# draw_dot(L)\n\n\nc._backward()\n\nWe expect that nothing will happen\n\n# draw_dot(L)\n\n\ne._backward()\n\n\n# draw_dot(L)\n\nsure enough, exactly as we did before\nWe can do thid process automatically using topo sort algorithms, which’s will give us the correct order on which to call _backward on\n\na = Value(2.0, label='a')\nb = Value(-3.0, label='b')\nc = Value(10.0, label='c')\ne = a*b; e.label='e'\nd = e + c; d.label='d'\nf = Value(-2.0, label='f')\nL = d*f; L.label='L' \n\n# draw_dot(L)\n\n\n# topological order all of the children in the graph\ntopo = []\nvisited = set()\ndef build_topo(v):\n    if v not in visited:\n        visited.add(v)\n        for child in v.children:\n            build_topo(child)\n        topo.append(v)\n\nbuild_topo(L)\n\n\ntopo\n\n[Value(data=10.0, grad=0),\n Value(data=2.0, grad=0),\n Value(data=-3.0, grad=0),\n Value(data=-6.0, grad=0),\n Value(data=4.0, grad=0),\n Value(data=-2.0, grad=0),\n Value(data=-8.0, grad=0)]\n\n\n\n# go one variable at a time and apply the chain rule to get its gradient\nL.grad = 1\nfor v in reversed(topo):\n    v._backward()\n\n\n# draw_dot(L)\n\nSo let’s now update the Value class with this logic\n\nL.backward()\n\n\n# draw_dot(L)\n\n\nsource\n\n\n\nValue\n\n Value (data, children=(), op='', label='')\n\nA class representing a scalar value and its gradient in a computational graph.\nAttributes: - data (float): the scalar value associated with this node - grad (float): the gradient of the output of the computational graph w.r.t. this node’s value - label (str): a label for this node, used for debugging and visualization purposes - _op (str): a string representation of the operation that produced this node in the computational graph - _prev (set of Value objects): the set of nodes that contributed to the computation of this node - _backward (function): a function that computes the gradients of this node w.r.t. its inputs\nMethods: - init(self, data, children=(), op=’‘, label=’’): Initializes a Value object with the given data, children, op, and label - repr(self): Returns a string representation of this Value object - add(self, other): Implements the addition operation between two Value objects - mul(self, other): Implements the multiplication operation between two Value objects - item(self): Returns the scalar value associated with this Value object - tanh(self): Applies the hyperbolic tangent function to this Value object and returns a new Value object\n\nsource\n\n\nall_devices\n\n all_devices ()\n\nreturn a list of all available devices\n\nsource\n\n\ncpu\n\n cpu ()\n\nReturn cpu device\n\nsource\n\n\nCPUDevice\n\n CPUDevice ()\n\nRepresents data that sits in CPU\n\nsource\n\n\nDevice\n\n Device ()\n\nIndicates the device supporting an NDArray.\n\nsource\n\n\nOperator\n\n Operator ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nTensorOp\n\n TensorOp ()\n\nOp class specialized to output tensors, will be alternate subclasses for other structures\n\nsource\n\n\nValue\n\n Value ()\n\nRepresents a node within a computational graph.\nThis class encapsulates a single value and its relationships in the graph, making it easy to track and manage the value’s dependencies, the operation that produced it, and whether it requires a gradient for backpropagation. It’s central to the functioning of automatic differentiation within deep learning frameworks.\nAttributes: op (Operator) _prev (Set[‘Value’]) cached_data (NDArray) requires_grad (bool)\n\nsource\n\n\nTensor\n\n Tensor (array, device:Optional[__main__.Device]=None, dtype=None,\n         requires_grad=True, **kwargs)\n\nA Tensor represents a multidimensional array of values in a computational graph.\nAttributes: - data: The actual data of the tensor. It is computed lazily. - children: Other tensors that this tensor depends on for computing its value. - requires_grad: Whether this tensor needs to compute gradients.\nMethods: - compute_cached_data: Computes and returns the actual data for this tensor. - shape: Returns the shape of this tensor. - dtype: Returns the data type of this tensor.\nExample: &gt;&gt;&gt; t1 = Tensor([[1.0, 2.0], [3.0, 4.0]]) &gt;&gt;&gt; print(t1.shape) (2, 2) &gt;&gt;&gt; print(t1.dtype) float64\n\nt1 = mi.Tensor([9, 9, 2, 2, 3, 9, 9, 9, 5, 4,])\nt1\n\nminima.Tensor([9 9 2 2 3 9 9 9 5 4])\n\n\n\nt2 = mi.Tensor([9, 9, 2, 2, 3, 9, 9, 9, 9, 4,])\nt2\n\nminima.Tensor([9 9 2 2 3 9 9 9 9 4])\n\n\n\nTensor.accuracy(t1, t2)\n\nminima.Tensor(0.9)"
  },
  {
    "objectID": "autograd.html#export",
    "href": "autograd.html#export",
    "title": "autograd",
    "section": "Export",
    "text": "Export\n\nimport nbdev; nbdev.nbdev_export()"
  },
  {
    "objectID": "init.html",
    "href": "init.html",
    "title": "init",
    "section": "",
    "text": "rand: This function generates a tensor filled with random numbers drawn from a uniform distribution between low and high (defaulting to 0 and 1). It does this by creating an array of random values on the specified device (defaulting to CPU), then scales and shifts these values to the correct range. The result is wrapped in a mi.Tensor object, which supports automatic differentiation if requires_grad is True.\nsource"
  },
  {
    "objectID": "init.html#export",
    "href": "init.html#export",
    "title": "init",
    "section": "Export",
    "text": "Export\n\nimport nbdev; nbdev.nbdev_export()"
  },
  {
    "objectID": "optim.html",
    "href": "optim.html",
    "title": "optim",
    "section": "",
    "text": "/opt/hostedtoolcache/Python/3.9.17/x64/lib/python3.9/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Raises\n  else: warn(msg)\nsource"
  },
  {
    "objectID": "optim.html#sgd-optimizer",
    "href": "optim.html#sgd-optimizer",
    "title": "optim",
    "section": "SGD Optimizer",
    "text": "SGD Optimizer\nThis is a PyTorch-style implementation of the classic optimizer Stochastic Gradient Descent (SGD).\nSGD update is,\n\\[\n\\theta_{t} = \\theta_{t-1} - \\alpha \\cdot g_{t}\n\\]\nwhere \\(\\alpha\\) is the learning rate, and \\(g_{t}\\) is the gradient at time step \\(t\\). \\(θ_{t}\\) represents the model parameters at time step \\(t\\).\nThe learning rate \\(\\alpha\\) is a scalar hyperparameter that controls the size of the update at each iteration.\nAn optional momentum term can be added to the update rule:\n\\[\n\\begin{align*}\nv_{t} & \\leftarrow \\mu v_{t-1} + (1-\\mu) \\cdot g_t \\\\\n\\theta_{t} & \\leftarrow \\theta_{t-1} - \\alpha \\cdot v_t\n\\end{align*}\n\\]\nwhere \\(v_{t}\\) is the momentum term at time step \\(t\\), and \\(\\mu\\) is the momentum factor. The momentum term increases for dimensions whose gradients point in the same\ndirection and reduces updates for dimensions whose gradients change direction, thereby adding a form of preconditioning.\nA weight decay term can also be included, which adds a regularization effect:\n\\[\n\\theta_{t} = (1 - \\alpha \\cdot \\lambda) \\cdot \\theta_{t-1} - \\alpha \\cdot g_t\n\\]\nwhere \\(\\lambda\\) is the weight decay factor. This results in the model weights shrinking at each time step, which can prevent overfitting by keeping the model complexity in check.\n\nsource\n\nSGD\n\n SGD (params, lr=0.01, momentum=0.0, wd=0.0)\n\nImplements stochastic gradient descent (optionally with momentum).\nThis is a basic optimizer that’s suitable for many machine learning models, and is often used as a baseline for comparing other optimizers’ performance.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nparams\nIterable\n\nThe parameters of the model to be optimized.\n\n\nlr\nfloat\n0.01\nThe learning rate.\n\n\nmomentum\nfloat\n0.0\nThe momentum factor.\n\n\nwd\nfloat\n0.0\nThe weight decay (L2 regularization)."
  },
  {
    "objectID": "optim.html#adagrad-optimizer",
    "href": "optim.html#adagrad-optimizer",
    "title": "optim",
    "section": "AdaGrad Optimizer",
    "text": "AdaGrad Optimizer\n\nIntuitive explanation:\n\nImagine you’re trying to navigate your way across a complex terrain - like a big mountain with lots of hills, valleys and flat areas.\nYour goal is to find the lowest valley. This is much like the problem a neural network faces when it’s trying to find the optimal values for its weights - the lowest point in its loss function.\nYou start at a random point on this terrain, which is like initializing your model with random weights. Now, you need to figure out which direction to go in to get to the lowest point.\nYou can’t see the whole terrain at once, but you can look around your current location and see which way is downhill. This is like calculating the gradient of the loss function with respect to the weights.\nIn a basic gradient descent algorithm, you would just go in the direction of the steepest slope with a fixed step size. But this approach can lead to problems.\nWhat if you’re on a steep slope and you take too big of a step? You might overshoot the valley you’re trying to get to. Or, what if you’re on a flat part of the\nterrain and you take too small of a step? You might get stuck and not make much progress.\nThis is where AdaGrad comes in. AdaGrad is like a smart hiker that adjusts its step size based on the terrain it’s currently on.\nIf it’s on a steep slope, it takes smaller steps to avoid overshooting the valley. If it’s on a flat area, it takes bigger steps to make faster progress.\nIt does this by keeping track of the sum of the squares of the gradients that it has seen so far (kinda like a memory), and uses this to scale down the step size.\nThis means that parameters with larger gradients will have their learning rate decreased more, while parameters with smaller gradients will have their learning rate\nThe neat thing about AdaGrad is that it adjusts the learning rate for each parameter individually, based on what it’s learned about the landscape around that parameter.\nThis can be especially useful when dealing with sparse data, where only a few parameters might be updated frequently.\n\nDetailed explanation\n\nBuilding on the foundational concepts of Stochastic Gradient Descent (SGD), we have AdaGrad, an algorithm that introduces an innovative twist to the optimization process.\nUnlike traditional SGD that utilizes a single learning rate \\(\\alpha\\) across all parameters, AdaGrad institutes a per-parameter learning rate. The learning rate for AdaGrad is computed as:\n\\[\n\\theta_{t} = \\theta_{t-1} - \\frac{\\alpha}{\\sqrt{G_t + \\epsilon}} \\cdot g_{t}\n\\]\nwhere \\(\\theta_{t}\\) represents the model parameters at time step \\(t\\), \\(\\alpha\\) is the initial learning rate, \\(g_{t}\\) is the gradient at time step \\(t\\), \\(G_{t}\\) is a diagonal matrix\nwhere each diagonal element \\(i, i\\) is the sum of the squares of the gradients w.r.t. \\(\\theta_i\\) up to time step \\(t\\), and \\(\\epsilon\\) is a smoothing term to avoid division by zero (usually on the order of \\(1e-7\\)).\nIn AdaGrad, each parameter \\(\\theta_i\\) gets its own learning rate, which is inversely proportional to the square root of the sum of the squares of past gradients.\nThis is the cache in the implementation, which holds a history of squared gradients. The greater the sum of the past gradients for a particular parameter, the smaller the learning rate for that parameter.\nThis feature allows AdaGrad to normalize the updates made during training, preventing any single weight from rising too high compared to the others.\nThis is particularly beneficial when dealing with sparse data, as the less frequently updated parameters are allowed larger updates when they do get updated, thereby effectively utilizing more neurons for training.\nHowever, it’s important to note that AdaGrad has a tendency to decrease the learning rate quite aggressively due to the constant accumulation of the square of gradients in \\(G_{t}\\).\nThis can sometimes lead to premature and excessive decay of the learning rate during training, causing the model to stop learning before reaching the optimal point.\nThis monotonic decrease in the learning rate is one reason AdaGrad is not as widely used, except in some specific applications.\nTo summarize, AdaGrad adds a valuable tool to our optimization toolkit by providing an adaptive learning rate for each individual parameter.\nIt elegantly solves the problem of learning rate selection and normalization of parameter updates, and while it has some limitations, it’s a\npowerful concept that has paved the way for further innovations in optimization algorithms.\n\nsource\n\nAdaGrad\n\n AdaGrad (params, lr=0.001, wd=0.0, eps=1e-07)\n\nImplements AdaGrad optimization algorithm.\nAdaGrad is an optimizer with parameter-wise learning rates, which adapts the learning rate based on how frequently a parameter gets updated during training. It’s particularly useful for sparse data.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nparams\nIterable\n\nThe parameters of the model to be optimized.\n\n\nlr\nfloat\n0.001\nThe initial learning rate.\n\n\nwd\nfloat\n0.0\nThe weight decay (L2 regularization).\n\n\neps\nfloat\n1e-07\nA small constant for numerical stability.\n\n\n\n\n\nRMSProp Optimizer\nRMSProp, short for Root Mean Square Propagation, which is an optimization algorithm that introduces an adaptive learning rate for each parameter in a model.\nRMSProp introduces an adaptive learning rate for each parameter to tackle different landscapes of the loss function. It does this by maintaining a moving (or ‘running’) average\nof the squared gradients, effectively measuring the scale of recent gradients. This running average, also known as the cache, is calculated as follows:\n\\[\ncache_{t} = \\rho \\cdot cache_{t-1} + (1-\\rho) \\cdot (g_{t})^2\n\\]\nwhere \\(\\rho\\) is the decay rate that determines how much of the history of squared gradients we retain. This cache term holds a form of “memory” of the magnitude of recent gradients, and its contents “move” with the data over time.\nThen, the parameter update rule becomes:\n\\[\n\\theta_{t} = \\theta_{t-1} - \\frac{\\alpha}{\\sqrt{cache_{t} + \\epsilon}} \\cdot g_{t}\n\\]\nwhere \\(\\epsilon\\) is a small constant for numerical stability, often around \\(1e-8\\). This normalization by the square root of the cache ensures smooth changes in the learning rate and\nhelps retain the global direction of parameter updates. This adaptivity makes the learning rate changes more resilient to fluctuations in the gradient.\nRMSProp introduces a new hyperparameter, \\(\\rho\\), the cache memory decay rate. Given the momentum-like properties of RMSProp, even small gradient updates can have substantial effects\ndue to the adaptive learning rate updates. As such, the default learning rate often used with RMSProp is smaller, around \\(0.001\\), to ensure stability.\n\nsource\n\n\nRMSProp\n\n RMSProp (params, lr=0.001, wd=0.0, eps=1e-07, rho=0.9)\n\nImplements RMSProp optimization algorithm.\nRMSProp is an optimizer with parameter-wise adaptive learning rates, which adapt the learning rate for each parameter individually, making it suitable for dealing with sparse or multi-scale data.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nparams\nIterable\n\nThe parameters of the model to be optimized.\n\n\nlr\nfloat\n0.001\nThe initial learning rate.\n\n\nwd\nfloat\n0.0\nThe weight decay (L2 regularization).\n\n\neps\nfloat\n1e-07\nA small constant for numerical stability.\n\n\nrho\nfloat\n0.9\nThe decay rate for the moving average of squared gradients."
  },
  {
    "objectID": "optim.html#adam-optimizer",
    "href": "optim.html#adam-optimizer",
    "title": "optim",
    "section": "Adam Optimizer",
    "text": "Adam Optimizer\nThis is a PyTorch-like implementation of popular optimizer Adam from paper Adam: A Method for Stochastic Optimization.\nAdam update is, \\[\n\\begin{align}\nm_t &\\leftarrow \\beta_1 m_{t-1} + (1 - \\beta_1) \\cdot g_t \\\\\nv_t &\\leftarrow \\beta_2 v_{t-1} + (1 - \\beta_2) \\cdot g_t^2 \\\\\n\\hat{m}_t &\\leftarrow \\frac{m_t}{1-\\beta_1^t} \\\\\n\\hat{v}_t &\\leftarrow \\frac{v_t}{1-\\beta_2^t} \\\\\n\\theta_t &\\leftarrow \\theta_{t-1} - \\alpha \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n\\end{align}\n\\] where \\(\\alpha\\), \\(\\beta_1\\), \\(\\beta_2\\) and \\(\\epsilon\\) are scalar hyper parameters. \\(m_t\\) and \\(v_t\\) are first and second order moments. \\(\\hat{m}_t\\) and \\(\\hat{v}_t\\) are biased corrected moments. \\(\\epsilon\\) is used as a fix for division by zero error, but also acts as a form of a hyper-parameter that acts against variance in gradients.\nEffective step taken assuming \\(\\epsilon = 0\\) is, \\[\\Delta t = \\alpha \\cdot \\frac{\\hat{m}_t}{\\hat{v}_t}\\] This is bounded by, \\[\\vert \\Delta t \\vert \\le \\alpha \\cdot \\frac{1 - \\beta_1}{\\sqrt{1-\\beta_2}}\\] when \\(1-\\beta_1 \\gt \\sqrt{1-\\beta_2}\\) and \\[\\vert \\Delta t\\vert  \\le \\alpha\\] otherwise. And in most common scenarios, \\[\\vert \\Delta t \\vert \\approx \\alpha\\]\n/opt/hostedtoolcache/Python/3.9.17/x64/lib/python3.9/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Attributes\n  else: warn(msg)\n\nsource\n\nAdam\n\n Adam (params, lr=1e-05, beta1=0.9, beta2=0.999, eps=1e-08,\n       weight_decay=0.0)\n\nImplements the Adam optimization algorithm.\nAdam is an adaptive learning rate optimization algorithm that has been designed specifically for training deep neural networks. It leverages the power of adaptive learning rates methods to find individual learning rates for each parameter.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nparams\nIterable\n\nparams is the list of parameters\n\n\nlr\nfloat\n1e-05\nlr is the learning rate \\(\\alpha\\)\n\n\nbeta1\nfloat\n0.9\nThe exponential decay rate for the first moment estimates. Default is 0.9.\n\n\nbeta2\nfloat\n0.999\nThe exponential decay rate for the second moment estimates. Default is 0.999.\n\n\neps\nfloat\n1e-08\neps is \\(\\hat{\\epsilon}\\) or \\(\\epsilon\\) based on optimized_update\n\n\nweight_decay\nfloat\n0.0\nis an instance of class WeightDecay defined in __init__.py"
  },
  {
    "objectID": "index.html#installing",
    "href": "index.html#installing",
    "title": "Welcome to minima",
    "section": "Installing",
    "text": "Installing\nYou can install minima on your own machines with conda\nIf you’re using miniconda (recommended) then run:\nconda install minima\n…or if you’re using Anaconda then run:\nconda install minima anaconda\nTo install with pip, use: pip install minima.\nIf you plan to develop Minima yourself, or want to be on the cutting edge, you can use an editable install.\ngit clone https://github.com/m0saan/minima\npip install ."
  },
  {
    "objectID": "index.html#features",
    "href": "index.html#features",
    "title": "Welcome to minima",
    "section": "Features",
    "text": "Features\n\nEasy to install and use\nSimple and intuitive API for defining and training neural networks\nBuilt-in support for common layers and activation functions\nSupports both CPU and GPU acceleration\nCompatible with NumPy arrays for easy data manipulation"
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "Welcome to minima",
    "section": "Usage",
    "text": "Usage\nHere’s a simple example of how to define and train a neural network using Minima:\nimport minima as mi\n\n# Define the neural network architecture\nmodel = mi.nn.Sequential(\n    mi.nn.Linear(784, 128),\n    mi.nn.ReLU(),\n    mi.nn.Linear(128, 10),\n    mi.nn.Softmax()\n)\n\n# Load the dataset\nx_train, y_train, x_test, y_test = load_data()\n\n# Train the model\nloss_fn = mi.nn.CrossEntropyLoss()\noptimizer = mi.optim.SGD(model.parameters(), lr=0.01)\nfor epoch in range(10):\n    for x_batch, y_batch in mi.nn.minibatch(x_train, y_train, batch_size=32):\n        y_pred = model(x_batch)\n        loss = loss_fn(y_pred, y_batch)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n# Evaluate the model\ny_pred = model(x_test)\naccuracy = compute_accuracy(y_pred, y_test)\nprint(f\"Accuracy: {accuracy:.2f}\")\nThis example defines a simple neural network with two linear layers and two activation functions, trains it on a dataset using stochastic gradient descent, and evaluates its accuracy on a test set."
  },
  {
    "objectID": "index.html#documentation",
    "href": "index.html#documentation",
    "title": "Welcome to minima",
    "section": "Documentation",
    "text": "Documentation\nFor more information on how to use minima, please refer to the documentation, which can be found in the website above."
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "Welcome to minima",
    "section": "Contributing",
    "text": "Contributing\ncoming soon!"
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Welcome to minima",
    "section": "License",
    "text": "License\nminima is released under the Apache License 2.0. See LICENSE for more information."
  },
  {
    "objectID": "ndarray_backend_numpy.html",
    "href": "ndarray_backend_numpy.html",
    "title": "ndarray_backend_numpy",
    "section": "",
    "text": "source"
  },
  {
    "objectID": "ndarray_backend_numpy.html#export",
    "href": "ndarray_backend_numpy.html#export",
    "title": "ndarray_backend_numpy",
    "section": "Export",
    "text": "Export\n\nimport nbdev; nbdev.nbdev_export()"
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "data",
    "section": "",
    "text": "source"
  },
  {
    "objectID": "data.html#export",
    "href": "data.html#export",
    "title": "data",
    "section": "Export",
    "text": "Export\n\nimport nbdev; nbdev.nbdev_export()"
  },
  {
    "objectID": "operators.html",
    "href": "operators.html",
    "title": "operators",
    "section": "",
    "text": "During backpropagation in a neural network, we compute gradients starting from the output layer and propagate them back towards the input layer. The key idea here is that each layer receives the gradient of the loss with respect to its output (let’s call this out_grad), and it needs to compute and pass back the gradient of the loss with respect to its input (let’s call this in_grad). This is needed so that the parameters of each layer can be updated correctly during gradient descent.\nThe out_grad parameter refers to the gradient of the loss function with respect to the output of the node. Multiplying this with the local gradient gives the gradient of the loss with respect to the input to the node, according to the chain rule of calculus, which is the basis for backpropagation in neural networks.\nThe chain rule is a fundamental concept in calculus that provides a method to compute the derivative of composite functions. In simple terms, the chain rule states that the derivative of a composite function is the derivative of the outer function multiplied by the derivative of the inner function.\nGiven a composite function that is the composition of two functions, say, \\(f(g(x))\\), the chain rule can be stated as follows:\n\\[\\frac{df}{dx} = \\frac{df}{dg} \\cdot \\frac{dg}{dx}\\]\nWhere:\n\n\\(\\frac{df}{dx}\\) is the derivative of the composite function \\(f(g(x))\\) with respect to \\(x\\),\n\\(\\frac{df}{dg}\\) is the derivative of the outer function \\(f\\) with respect to its argument \\(g(x)\\), and\n\\(\\frac{dg}{dx}\\) is the derivative of the inner function \\(g(x)\\) with respect to \\(x\\).\n\nThe chain rule can be extended to the case where we have more than two composite functions."
  },
  {
    "objectID": "operators.html#note-about-the-out_grad-parameter",
    "href": "operators.html#note-about-the-out_grad-parameter",
    "title": "operators",
    "section": "",
    "text": "During backpropagation in a neural network, we compute gradients starting from the output layer and propagate them back towards the input layer. The key idea here is that each layer receives the gradient of the loss with respect to its output (let’s call this out_grad), and it needs to compute and pass back the gradient of the loss with respect to its input (let’s call this in_grad). This is needed so that the parameters of each layer can be updated correctly during gradient descent.\nThe out_grad parameter refers to the gradient of the loss function with respect to the output of the node. Multiplying this with the local gradient gives the gradient of the loss with respect to the input to the node, according to the chain rule of calculus, which is the basis for backpropagation in neural networks.\nThe chain rule is a fundamental concept in calculus that provides a method to compute the derivative of composite functions. In simple terms, the chain rule states that the derivative of a composite function is the derivative of the outer function multiplied by the derivative of the inner function.\nGiven a composite function that is the composition of two functions, say, \\(f(g(x))\\), the chain rule can be stated as follows:\n\\[\\frac{df}{dx} = \\frac{df}{dg} \\cdot \\frac{dg}{dx}\\]\nWhere:\n\n\\(\\frac{df}{dx}\\) is the derivative of the composite function \\(f(g(x))\\) with respect to \\(x\\),\n\\(\\frac{df}{dg}\\) is the derivative of the outer function \\(f\\) with respect to its argument \\(g(x)\\), and\n\\(\\frac{dg}{dx}\\) is the derivative of the inner function \\(g(x)\\) with respect to \\(x\\).\n\nThe chain rule can be extended to the case where we have more than two composite functions."
  },
  {
    "objectID": "operators.html#element-wise-addition",
    "href": "operators.html#element-wise-addition",
    "title": "operators",
    "section": "Element Wise Addition",
    "text": "Element Wise Addition\nLet’s walk through the step-by-step derivative calculation for the EWiseAdd operation:\nWe have the function f(a, b) = a + b, where a and b are tensors. Our goal is to compute the partial derivatives with respect to a and b.\nLet’s start by calculating the derivative of f with respect to a, denoted as df/da:\nStep 1: Compute the derivative of f with respect to a.\n\\(\\frac{{\\partial f}}{{\\partial a}} = \\frac{{\\partial}}{{\\partial a}} (a + b)\\)\nSince a is the variable we are differentiating with respect to, the derivative of a with respect to itself is 1:\n\\[\\frac{{\\partial f}}{{\\partial a}} = 1\\]\nTherefore, \\[\\frac{{\\partial f}}{{\\partial a}} = 1.\\]\nStep 2: Compute the derivative of f with respect to b.\n\\[\\frac{{\\partial f}}{{\\partial b}} = \\frac{{\\partial}}{{\\partial b}} (a + b)\\]\nAgain, since b is the variable we are differentiating with respect to, the derivative of b with respect to itself is 1:\n\\[\\frac{{\\partial f}}{{\\partial b}} = 1\\]\nTherefore, \\[\\frac{{\\partial f}}{{\\partial b}} = 1\\]\nHence, the partial derivatives of f(a, b) = a + b with respect to a and b are both equal to 1.\n\nsource\n\nadd\n\n add (a:minima.autograd.Tensor, b:minima.autograd.Tensor)\n\nAdds two tensors element-wise.\nArgs: - a: The first tensor. - b: The second tensor.\nReturns: The element-wise sum of a and b.\n\nsource\n\n\nEWiseAdd\n\n EWiseAdd ()\n\nPerforms element-wise addition of two tensors.\nExample: &gt;&gt;&gt; a = Tensor([1, 2, 3]) &gt;&gt;&gt; b = Tensor([4, 5, 6]) &gt;&gt;&gt; op = EWiseAdd() &gt;&gt;&gt; result = op.compute(a, b) &gt;&gt;&gt; print(result) Tensor([5, 7, 9])\nCreate two 1-D tensors\n\na = Tensor([1, 2, 3])\nb = Tensor([4, 5, 6])\n\nCreate an EWiseAdd operation instance\n\nop = EWiseAdd()\n\nCompute the element-wise sum of a and b\n\nresult = op.compute(a, b)\nresult\n\nminima.Tensor(\n[5 7 9])\n\n\nAlternatively, you can use the add function directly\n\nresult = add(a, b)\nresult\n\nminima.Tensor(\n[5 7 9])\n\n\nor\n\nop(a,b)\n\nminima.Tensor(\n[5 7 9])\n\n\nFor 2-D tensors, we can compute the element-wise sum of a and b in the same way\n\na = Tensor([[1, 2, 3], [4, 5, 6]])\nb = Tensor([[7, 8, 9], [10, 11, 12]])\n\nresult = op.compute(a, b)\nresult\n\nminima.Tensor(\n[[ 8 10 12]\n [14 16 18]])"
  },
  {
    "objectID": "operators.html#scalar-addition",
    "href": "operators.html#scalar-addition",
    "title": "operators",
    "section": "Scalar Addition",
    "text": "Scalar Addition\nExplanation for the derivative of the AddScalar operator:\nLet’s denote the scalar as c and a as the tensor being added by the scalar. The operation can be described as f(a) = a + c.\nThe function for the backward pass (i.e., the gradient) is df/da = 1, which means the derivative of f(a) with respect to a is simply 1.\nWe are given a function \\(f(a) = a + c\\), where \\(a\\) is a tensor and \\(c\\) is a scalar. Our task is to find the derivative of this function with respect to \\(a\\).\nBy differentiating the function \\(f(a)\\) with respect to \\(a\\), we find:\n\\[\\begin{align*}\n\\frac{df}{da} &= \\frac{d}{da} (a + c) \\\\\n&= 1\n\\end{align*}\\]\nTherefore, the gradient of \\(f(a)\\) with respect to \\(a\\) is \\(1\\).\nWe starts by defining the function f(a) = a + c. It then explains that when we differentiate f(a) with respect to a, we find that the derivative is 1. This means that the gradient of f(a) with respect to a is 1, which matches the behavior of the AddScalar operator as provided in the gradient method.\n\nsource\n\nadd_scalar\n\n add_scalar (a:minima.autograd.Tensor, scalar:Union[int,float])\n\nAdds a scalar to a tensor.\nArgs: - a: The tensor. - scalar: The scalar to add.\nReturns: The sum of a and the scalar.\n\nsource\n\n\nAddScalar\n\n AddScalar (scalar:Union[int,float])\n\nPerforms addition of a tensor and a scalar.\nExample: &gt;&gt;&gt; a = Tensor([1, 2, 3]) &gt;&gt;&gt; op = AddScalar(5) &gt;&gt;&gt; result = op.compute(a) &gt;&gt;&gt; print(result) Tensor([6, 7, 8])"
  },
  {
    "objectID": "operators.html#element-wise-multiplication",
    "href": "operators.html#element-wise-multiplication",
    "title": "operators",
    "section": "Element Wise Multiplication",
    "text": "Element Wise Multiplication\nExplanation for the derivative of the EWiseMul (element-wise multiplication) operator:\nLet’s denote the two input tensors as a and b. The operation can be described as f(a, b) = a * b, where * represents element-wise multiplication.\nThe function for the backward pass (i.e., the gradient) is df/da = b and df/db = a. This means that the derivative of f(a, b) with respect to a is b, and the derivative with respect to b is a.\nWe are given a function \\(f(a, b) = a \\odot b\\), where \\(a\\) and \\(b\\) are tensors, and \\(\\odot\\) represents element-wise multiplication. Our task is to find the derivatives of this function with respect to \\(a\\) and \\(b\\).\nBy differentiating the function \\(f(a, b)\\) with respect to \\(a\\), we find:\n\\[\\begin{align*}\n\\frac{df}{da} &= \\frac{d}{da} (a \\odot b) \\\\\n&= b\n\\end{align*}\\]\nTherefore, the gradient of \\(f(a, b)\\) with respect to \\(a\\) is \\(b\\).\nSimilarly, by differentiating the function \\(f(a, b)\\) with respect to \\(b\\), we find:\n\\[\\begin{align*}\n\\frac{df}{db} &= \\frac{d}{db} (a \\odot b) \\\\\n&= a\n\\end{align*}\\]\nTherefore, the gradient of \\(f(a, b)\\) with respect to \\(b\\) is \\(a\\).\n\nsource\n\nmultiply\n\n multiply (a:minima.autograd.Tensor, b:minima.autograd.Tensor)\n\nMultiplies two tensors element-wise.\nArgs: - a: The first tensor. - b: The second tensor.\nReturns: The element-wise product of a and b.\n\nsource\n\n\nEWiseMul\n\n EWiseMul ()\n\nPerforms element-wise multiplication of two tensors.\nExample: &gt;&gt;&gt; a = Tensor([1, 2, 3]) &gt;&gt;&gt; b = Tensor([4, 5, 6]) &gt;&gt;&gt; op = EWiseMul() &gt;&gt;&gt; result = op.compute(a, b) &gt;&gt;&gt; print(result) Tensor([4, 10, 18])"
  },
  {
    "objectID": "operators.html#scalar-multiplication",
    "href": "operators.html#scalar-multiplication",
    "title": "operators",
    "section": "Scalar Multiplication",
    "text": "Scalar Multiplication\nLet’s denote the scalar as c and a as the tensor being multiplied by the scalar. The operation can be described as f(a) = a * c.\nThe function for the backward pass (i.e., the gradient) is df/da = c, which means the derivative of f(a) with respect to a is c.\nThe LaTeX document will look as follows:\nWe are given a function \\(f(a) = a \\cdot c\\), where \\(a\\) is a tensor and \\(c\\) is a scalar. Our task is to find the derivative of this function with respect to \\(a\\).\nBy differentiating the function \\(f(a)\\) with respect to \\(a\\), we find:\n\\[\\begin{align*}\n\\frac{df}{da} &= \\frac{d}{da} (a \\cdot c) \\\\\n&= c\n\\end{align*}\\]\nTherefore, the gradient of \\(f(a)\\) with respect to \\(a\\) is \\(c\\).\nWe starts by defining the function f(a) = a * c. It then explains that when we differentiate f(a) with respect to a, we find that the derivative is c. This means that the gradient of f(a) with respect to a is c, which matches the behavior of the MulScalar operator as provided in the gradient method.\n\nsource\n\nmul_scalar\n\n mul_scalar (a:minima.autograd.Tensor, scalar:Union[int,float])\n\nMultiplies a tensor by a scalar.\nArgs: - a: The tensor. - scalar: The scalar to multiply.\nReturns: The product of a and the scalar.\n\nsource\n\n\nMulScalar\n\n MulScalar (scalar:Union[int,float])\n\nPerforms multiplication of a tensor and a scalar.\nExample: &gt;&gt;&gt; a = Tensor([1, 2, 3]) &gt;&gt;&gt; op = MulScalar(5) &gt;&gt;&gt; result = op.compute(a) &gt;&gt;&gt; print(result) Tensor([5, 10, 15])"
  },
  {
    "objectID": "operators.html#element-wise-divide",
    "href": "operators.html#element-wise-divide",
    "title": "operators",
    "section": "Element Wise Divide",
    "text": "Element Wise Divide\nThe operation described here is an element-wise division of two tensors, a and b, where the operation can be described as f(a, b) = a / b.\nWe’ll compute the partial derivatives with respect to a and b:\n\nThe partial derivative of f(a, b) with respect to a (df/da) is 1/b.\nThe partial derivative of f(a, b) with respect to b (df/db) is -a / b^2.\n\nWe are given a function \\(f(a, b) = \\frac{a}{b}\\), where \\(a\\) and \\(b\\) are tensors. Our task is to find the partial derivatives of this function with respect to \\(a\\) and \\(b\\).\nLet’s start with \\(\\frac{\\partial f}{\\partial a}\\):\n\\[\\begin{align*}\n\\frac{\\partial f}{\\partial a} &= \\frac{\\partial}{\\partial a} \\left(\\frac{a}{b}\\right) \\\\\n&= \\frac{1}{b}\n\\end{align*}\\]\nNow, let’s compute \\(\\frac{\\partial f}{\\partial b}\\):\n\\[\\begin{align*}\n\\frac{\\partial f}{\\partial b} &= \\frac{\\partial}{\\partial b} \\left(\\frac{a}{b}\\right) \\\\\n&= - \\frac{a}{b^{2}}\n\\end{align*}\\]\nHere is a detailed derivative:\nGiven a function of the form \\(y = \\frac{u}{v}\\), where both \\(u\\) and \\(v\\) are functions of \\(x\\), the quotient rule of differentiation states:\n\\[\\frac{dy}{dx} = \\frac{v \\cdot \\frac{du}{dx} - u \\cdot \\frac{dv}{dx}}{v^2}\\]\nIn our case, we’re looking at the function \\(y = \\frac{a}{b}\\), where \\(a\\) and \\(b\\) are tensors. We want to find the derivative with respect to \\(b\\) (instead of \\(x\\) in our general formula). So we have:\n\\[\\frac{dy}{db} = \\frac{b \\cdot \\frac{da}{db} - a \\cdot \\frac{db}{db}}{b^2}\\]\nSince \\(a\\) does not depend on \\(b\\), \\(\\frac{da}{db} = 0\\), and since any variable is equal to itself, \\(\\frac{db}{db} = 1\\).\nSo the derivative \\(\\frac{dy}{db}\\) simplifies to:\n\\[\\frac{dy}{db} = \\frac{b \\cdot 0 - a \\cdot 1}{b^2}\\]\nTherefore, the derivative of \\(y\\) with respect to \\(b\\) is \\(-\\frac{a}{b^2}\\).\nTherefore, the gradient of \\(f(a, b)\\) with respect to \\(a\\) is \\(\\frac{1}{b}\\), and the gradient of \\(f(a, b)\\) with respect to \\(b\\) is \\(- \\frac{a}{b^{2}}\\).\n\nsource\n\ndivide\n\n divide (a:minima.autograd.Tensor, b:minima.autograd.Tensor)\n\nDivides two tensors element-wise.\nArgs: a (Tensor): The dividend tensor. b (Tensor): The divisor tensor.\nReturns: Tensor: The resulting tensor after element-wise division.\nExample: &gt;&gt;&gt; import numpy as np &gt;&gt;&gt; a = Tensor(np.array([1, 2, 3])) &gt;&gt;&gt; b = Tensor(np.array([4, 5, 6])) &gt;&gt;&gt; result = divide(a, b) &gt;&gt;&gt; print(result) Tensor([0.25, 0.4, 0.5])\n\nsource\n\n\nEWiseDiv\n\n EWiseDiv ()\n\nThe EWiseDiv operation divides two tensors element-wise.\nExample: &gt;&gt;&gt; import numpy as np &gt;&gt;&gt; a = Tensor(np.array([1, 2, 3])) &gt;&gt;&gt; b = Tensor(np.array([4, 5, 6])) &gt;&gt;&gt; div = EWiseDiv() &gt;&gt;&gt; result = div.compute(a.data, b.data) &gt;&gt;&gt; print(result) array([0.25, 0.4, 0.5])"
  },
  {
    "objectID": "operators.html#scalar-division",
    "href": "operators.html#scalar-division",
    "title": "operators",
    "section": "Scalar Division",
    "text": "Scalar Division\nLet’s denote the scalar as c, and a as the tensor being divided by the scalar. The operation can be described as f(a) = a / c.\nThe function for the backward pass (i.e., the gradient) is df/da = 1/c.\nThis is the derivative of f(a) with respect to a.\nWe are given a function \\(f(a) = \\frac{a}{c}\\), where \\(a\\) is a tensor and \\(c\\) is a scalar. Our task is to find the derivative of this function with respect to \\(a\\).\nBy using the power rule of differentiation, where the derivative of \\(a^n\\) is \\(n \\cdot a^{n-1}\\), we can rewrite \\(f(a)\\) as \\(f(a) = c^{-1}a\\).\nNow, we can differentiate this with respect to \\(a\\):\n\\[\\begin{align*}\n\\frac{df}{da} &= \\frac{d}{da} (c^{-1}a) \\\\\n&= c^{-1} \\frac{d}{da} (a) \\\\\n&= c^{-1} \\\\\n&= \\frac{1}{c}\n\\end{align*}\\]\nTherefore, the gradient of \\(f(a)\\) with respect to \\(a\\) is \\(\\frac{1}{c}\\).\n\nsource\n\ndivide_scalar\n\n divide_scalar (a:minima.autograd.Tensor, scalar:Union[int,float])\n\nDivides a tensor by a scalar.\nArgs: a (Tensor): The tensor to divide. scalar (int, float): The scalar to divide the tensor by.\nReturns: Tensor: The resulting tensor after division.\nExample: &gt;&gt;&gt; import numpy as np &gt;&gt;&gt; a = Tensor(np.array([1, 2, 3])) &gt;&gt;&gt; scalar = 2 &gt;&gt;&gt; result = divide_scalar(a, scalar) &gt;&gt;&gt; print(result) Tensor([0.5, 1.0, 1.5])\n\nsource\n\n\nDivScalar\n\n DivScalar (scalar:Union[int,float])\n\nThe DivScalar operation divides a tensor by a scalar.\nExample: &gt;&gt;&gt; import numpy as np &gt;&gt;&gt; a = Tensor(np.array([1, 2, 3])) &gt;&gt;&gt; scalar = 2 &gt;&gt;&gt; div_scalar = DivScalar(scalar) &gt;&gt;&gt; result = div_scalar.compute(a.data) &gt;&gt;&gt; print(result) array([0.5, 1.0, 1.5])"
  },
  {
    "objectID": "operators.html#negation",
    "href": "operators.html#negation",
    "title": "operators",
    "section": "Negation",
    "text": "Negation\nLet’s denote a as the tensor being negated. The operation can be described as f(a) = -a.\nThe function for the backward pass (i.e., the gradient) is df/da = -1.\nWe are given a function \\(f(a) = -a\\), where \\(a\\) is a tensor. Our task is to find the derivative of this function with respect to \\(a\\).\nBy differentiating the function \\(f(a)\\) with respect to \\(a\\), we find:\n\\[\\begin{align*}\n\\frac{df}{da} &= \\frac{d}{da} (-a) \\\\\n&= -1\n\\end{align*}\\]\nTherefore, the gradient of \\(f(a)\\) with respect to \\(a\\) is \\(-1\\).\n\nsource\n\nnegate\n\n negate (a:minima.autograd.Tensor)\n\nNegates the given tensor.\nArgs: - a: The tensor to negate.\nReturns: The negation of a.\nExample: &gt;&gt;&gt; a = Tensor([1, -2, 3]) &gt;&gt;&gt; result = negate(a) &gt;&gt;&gt; print(result) Tensor([-1, 2, -3])\n\nsource\n\n\nNegate\n\n Negate ()\n\nNegates the given tensor.\nExample: &gt;&gt;&gt; a = Tensor([1, -2, 3]) &gt;&gt;&gt; op = Negate() &gt;&gt;&gt; result = op.compute(a) &gt;&gt;&gt; print(result) Tensor([-1, 2, -3])"
  },
  {
    "objectID": "operators.html#exp",
    "href": "operators.html#exp",
    "title": "operators",
    "section": "Exp",
    "text": "Exp\nExplanation for the derivative of the Exp operator:\nLet’s denote a as the tensor on which the exponential function is applied. The operation can be described as f(a) = exp(a), where exp represents the exponential function.\nThe function for the backward pass (i.e., the gradient) is df/da = exp(a).\nWe are given a function \\(f(a) = \\exp(a)\\), where \\(a\\) is a tensor. Our task is to find the derivative of this function with respect to \\(a\\).\nBy differentiating the function \\(f(a)\\) with respect to \\(a\\), we find:\n\\[\\begin{align*}\n\\frac{df}{da} &= \\frac{d}{da} (\\exp(a)) \\\\\n&= \\exp(a)\n\\end{align*}\\]\nTherefore, the gradient of \\(f(a)\\) with respect to \\(a\\) is \\(\\exp(a)\\).\n\nsource\n\nexp\n\n exp (a:minima.autograd.Tensor)\n\nCalculates the exponential of the given tensor.\nArgs: - a: The tensor.\nReturns: The exponential of a.\nExample: &gt;&gt;&gt; a = Tensor([1, 2, 3]) &gt;&gt;&gt; result = exp(a) &gt;&gt;&gt; print(result) Tensor([2.71828183, 7.3890561, 20.08553692])\n\nsource\n\n\nExp\n\n Exp ()\n\nCalculates the exponential of the given tensor.\nExample: &gt;&gt;&gt; a = Tensor([1, 2, 3]) &gt;&gt;&gt; op = Exp() &gt;&gt;&gt; result = op.compute(a) &gt;&gt;&gt; print(result) Tensor([2.71828183, 7.3890561, 20.08553692])"
  },
  {
    "objectID": "operators.html#relu",
    "href": "operators.html#relu",
    "title": "operators",
    "section": "ReLU",
    "text": "ReLU\nThe derivative of the ReLU (Rectified Linear Unit) operator:\nLet’s denote a as the tensor on which the ReLU function is applied. The ReLU function is defined as follows:\n\\[\nf(a) =\n\\begin{cases}\na, & \\text{if } a \\geq 0 \\\\\n0, & \\text{if } a &lt; 0\n\\end{cases}\n\\]\nThe function for the backward pass (i.e., the gradient) is df/da = 1 if a &gt;= 0, and df/da = 0 if a &lt; 0.\nWe are given a function \\(f(a) = \\max(0, a)\\), where \\(a\\) is a tensor. Our task is to find the derivative of this function with respect to \\(a\\).\nBy considering the definition of the ReLU function, we can write \\(f(a)\\) as:\n\\[\nf(a) =\n\\begin{cases}\na, & \\text{if } a \\geq 0 \\\\\n0, & \\text{if } a &lt; 0\n\\end{cases}\n\\]\nNow, let’s differentiate \\(f(a)\\) with respect to \\(a\\):\n\\[\n\\frac{df}{da} =\n\\begin{cases}\n1, & \\text{if } a \\geq 0 \\\\\n0, & \\text{if } a &lt; 0\n\\end{cases}\n\\]\nTherefore, the gradient of \\(f(a)\\) with respect to \\(a\\) is \\(1\\) if \\(a \\geq 0\\), and \\(0\\) if \\(a &lt; 0\\).\n\nsource\n\nrelu\n\n relu (a:minima.autograd.Tensor)\n\nApplies the ReLU (Rectified Linear Unit) activation function to the given tensor.\nArgs: - a: The tensor.\nReturns: The result of applying ReLU to a.\nExample: &gt;&gt;&gt; a = Tensor([1, -2, 3]) &gt;&gt;&gt; result = relu(a) &gt;&gt;&gt; print(result) Tensor([1, 0, 3])\n\nsource\n\n\nReLU\n\n ReLU ()\n\nApplies the ReLU (Rectified Linear Unit) activation function to the given tensor.\nExample: &gt;&gt;&gt; a = Tensor([1, -2, 3]) &gt;&gt;&gt; op = ReLU() &gt;&gt;&gt; result = op.compute(a) &gt;&gt;&gt; print(result) Tensor([1, 0, 3])"
  },
  {
    "objectID": "operators.html#power-scalar",
    "href": "operators.html#power-scalar",
    "title": "operators",
    "section": "Power Scalar",
    "text": "Power Scalar\nThe derivative of the PowerScalar operator:\nLet’s denote the scalar as n and a as the tensor being raised to the power of the scalar. The operation can be described as f(a) = a^n.\nThe function for the backward pass (i.e., the gradient) is df/da = n * a^(n-1).\nWe are given a function \\(f(a) = a^n\\), where \\(a\\) is a tensor and \\(n\\) is a scalar. Our task is to find the derivative of this function with respect to \\(a\\).\nBy differentiating the function \\(f(a)\\) with respect to \\(a\\), we find:\n\\[\\begin{align*}\n\\frac{df}{da} &= \\frac{d}{da} (a^n) \\\\\n&= n \\cdot a^{n-1}\n\\end{align*}\\]\nTherefore, the gradient of \\(f(a)\\) with respect to \\(a\\) is \\(n \\cdot a^{n-1}\\).\n\nsource\n\npower_scalar\n\n power_scalar (a:minima.autograd.Tensor, scalar:int)\n\nRaises a tensor to a power.\nArgs: a (Tensor): The input tensor. scalar (int): The power to raise the tensor to.\nReturns: Tensor: The resulting tensor after the power operation.\nExample: &gt;&gt;&gt; import numpy as np &gt;&gt;&gt; tensor = Tensor(np.array([1, 2, 3])) &gt;&gt;&gt; result = power_scalar(tensor, 2) &gt;&gt;&gt; print(result) Tensor([1, 4, 9])\n\nsource\n\n\nPowerScalar\n\n PowerScalar (scalar:int)\n\nThe PowerScalar operation raises a tensor to an (integer) power.\nAttributes: scalar (int): The power to raise the tensor to.\nExample: &gt;&gt;&gt; import numpy as np &gt;&gt;&gt; tensor = Tensor(np.array([1, 2, 3])) &gt;&gt;&gt; pow_scalar = PowerScalar(2) &gt;&gt;&gt; result = pow_scalar.compute(tensor.data) &gt;&gt;&gt; print(result) array([1, 4, 9])"
  },
  {
    "objectID": "operators.html#log",
    "href": "operators.html#log",
    "title": "operators",
    "section": "Log",
    "text": "Log\nExplanation for the derivative of the Log operator:\nLet’s denote a as the tensor on which the logarithm is applied. The operation can be described as f(a) = log(a), where log represents the natural logarithm.\nThe function for the backward pass (i.e., the gradient) is df/da = 1/a.\nWe are given a function \\(f(a) = \\log(a)\\), where \\(a\\) is a tensor. Our task is to find the derivative of this function with respect to \\(a\\).\nBy differentiating the function \\(f(a)\\) with respect to \\(a\\), we find:\n\\[\\begin{align*}\n\\frac{df}{da} &= \\frac{d}{da} (\\log(a)) \\\\\n&= \\frac{1}{a}\n\\end{align*}\\]\nWe started by defining the function f(a) = log(a), where log represents the natural logarithm. It then explains that when we differentiate f(a) with respect to a, we find that the derivative is 1/a. This means that the gradient of f(a) with respect to a is 1/a, which represents the behavior of the Log operator.\n\nclass Log(TensorOp):\n    \"\"\"\n    The Log operation applies the natural logarithm element-wise on the tensor.\n\n    Example:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; a = Tensor(np.array([1.0, 2.0, 3.0]))\n        &gt;&gt;&gt; log_op = Log()\n        &gt;&gt;&gt; result = log_op.compute(a.data)\n        &gt;&gt;&gt; print(result)\n        array([0., 0.69314718, 1.09861229])\n    \"\"\"\n\n    def compute(self, a: NDArray) -&gt; NDArray:\n        \"\"\"\n        Applies the natural logarithm to the tensor.\n\n        Args:\n            a (NDArray): The input tensor.\n\n        Returns:\n            NDArray: The resulting tensor after applying the natural logarithm.\n        \"\"\"\n        return ARRAY_API.log(a)\n\n    def gradient(self, out_grad: Tensor, node: Tensor) -&gt; Tuple[Tensor, ...]:\n        \"\"\"\n        Computes the gradient of the log operation.\n\n        Args:\n            out_grad (Tensor): The gradient of the output tensor.\n            node (Tensor): The node in the computational graph where the operation was performed.\n\n        Returns:\n            Tuple[Tensor, ...]: The gradient with respect to the input tensor.\n        \"\"\"\n        a = node.children[0]\n        return (out_grad / a, )\n\ndef log(a: Tensor) -&gt; Tensor:\n    \"\"\"\n    Applies the natural logarithm to the tensor.\n\n    Args:\n        a (Tensor): The input tensor.\n\n    Returns:\n        Tensor: The resulting tensor after applying the natural logarithm.\n\n    Example:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; a = Tensor(np.array([1.0, 2.0, 3.0]))\n        &gt;&gt;&gt; result = log(a)\n        &gt;&gt;&gt; print(result)\n        Tensor([0., 0.69314718, 1.09861229])\n    \"\"\"\n    return Log()(a)"
  },
  {
    "objectID": "operators.html#transpose",
    "href": "operators.html#transpose",
    "title": "operators",
    "section": "Transpose",
    "text": "Transpose\nThis operation described here is the derivative of a transposition operation. Let’s define our transposition operation as a function f such that f(a) = a^T where a is a tensor, and a^T is the transpose of tensor a.\nThe goal here is to compute the derivative of this operation with respect to a. It’s important to note that transposition operation doesn’t change the values of the tensor’s elements, but it just rearranges their positions. This implies that the gradient (derivative) of a transposed tensor is simply the transposed gradient of the original tensor.\nLet’s denote the gradient of the transposed tensor as g, which can be mathematically represented as g = df/da, where df/da is the derivative of f(a) with respect to a.\nGiven this understanding, we can make an important conclusion:\n\nThe derivative of f(a) with respect to a is df/da = g^T, meaning that the derivative of the transposed tensor is simply the transposed gradient of the original tensor.\n\nThis concept can be written in mathematical terms using LaTeX as follows:\nWe have a function \\(f(a) = a^T\\), where \\(a\\) is a tensor and \\(a^T\\) is its transpose. We want to find the derivative of this function with respect to \\(a\\), that is, compute \\(\\frac{df}{da}\\).\n\\[\\begin{align*}\n\\frac{df}{da} &= \\frac{d}{da} (a^T) \\\\\n&= (g)^T\n\\end{align*}\\]\nIn the equation above, \\(g\\) is the gradient of the transposed tensor. This equation indicates that the derivative of the transpose of a tensor is the transpose of the gradient of the original tensor.\nNow, if we consider a Python class Transpose that implements this transposition operation, we would have a gradient method in the class that computes the derivative of the transpose operation. This method would apply the transpose function to out_grad, which represents the gradient of the output tensor, thereby giving us the transposed gradient of the original tensor. In the code, transpose(out_grad, axes=self.axes) performs the transposition of out_grad along the same axes that were used in the forward pass. Thus, the gradient of the transposition operation with respect to the input tensor a is computed as the transpose of the output gradient out_grad.\n\nsource\n\ntranspose\n\n transpose (a:minima.autograd.Tensor, axes:Optional[tuple]=None)\n\nPerform the transpose operation on the input tensor along the specified axes. If no axes are specified, it swaps the last two dimensions of the input tensor.\nArgs: a (Tensor): The input tensor. axes (Optional[tuple]): The pair of axes that should be swapped. If not provided, the last two axes are swapped.\nReturns: Tensor: The transposed tensor.\nExample: &gt;&gt;&gt; a = Tensor(np.arange(1, 7).reshape(2, 3)) &gt;&gt;&gt; result = transpose(a) &gt;&gt;&gt; print(result) Tensor([[1, 4], [2, 5], [3, 6]])\n\nsource\n\n\nTranspose\n\n Transpose (axes:Optional[tuple]=None)\n\nTensor operation class that performs transposition of a tensor along specified axes.\nIf no axes are specified, it swaps the last two dimensions of the input tensor.\nExample: &gt;&gt;&gt; a = Tensor(np.arange(1, 7).reshape(2, 3)) &gt;&gt;&gt; op = Transpose() &gt;&gt;&gt; result = op.compute(a.data) &gt;&gt;&gt; print(result) array([[1, 4], [2, 5], [3, 6]])"
  },
  {
    "objectID": "operators.html#reshape",
    "href": "operators.html#reshape",
    "title": "operators",
    "section": "Reshape",
    "text": "Reshape\nThe operation described here is a reshaping of a tensor a, where the operation can be described as f(a) = reshape(a, new_shape).\nWe’ll compute the derivative of this operation.\nThe reshaping operation doesn’t change the values of the tensor elements but only rearranges them. This means that the gradient of a reshaped tensor is just the reshaped gradient of the original tensor.\nLet’s denote the gradient of the reshaped tensor as g = df/da, where f(a) = reshape(a, new_shape).\nGiven this, we can derive the following:\n\nThe derivative of f(a) with respect to a is df/da = reshape(g, original_shape).\n\nThis conclusion can be illustrated as follows in Latex:\nWe are given a function \\(f(a) = reshape(a, new\\_shape)\\), where \\(a\\) is a tensor and reshape(a, new_shape) is the reshaped tensor. Our task is to find the derivative of this function with respect to \\(a\\).\nLet’s compute \\(\\frac{df}{da}\\):\n\\[\\begin{align*}\n\\frac{df}{da} &= \\frac{d}{da} (reshape(a, new\\_shape)) \\\\\n&= reshape(g, original\\_shape)\n\\end{align*}\\]\nHere, \\(g\\) is the gradient of the reshaped tensor. The derivative of a reshaped tensor is the reshaped derivative of the original tensor. The reshaped derivative has the same shape as the original tensor.\nNow, let’s apply this to the Reshape class.\nThe gradient method in the Reshape class computes the gradient of the reshape operation. The gradient of the reshaped tensor is just the reshaped gradient of the original tensor. This is implemented by applying the reshape function to out_grad, which is the gradient of the output tensor, and then returning this reshaped gradient. The shape used for the reshaping is the shape of the original tensor, which is obtained from node.children[0].shape.\nTherefore, the gradient of the reshape operation with respect to the input tensor a is the reshaping of the output gradient out_grad to the shape of the original tensor.\n\nsource\n\nreshape\n\n reshape (a:minima.autograd.Tensor, shape:Tuple[int,...])\n\nReshape the input tensor to the specified shape.\nArgs: a (Tensor): The input tensor. shape (Tuple[int, …]): The desired shape of the output tensor.\nReturns: Tensor: The reshaped tensor.\nExample: &gt;&gt;&gt; a = Tensor([1, 2, 3, 4, 5, 6]) &gt;&gt;&gt; result = reshape(a, (2, 3)) &gt;&gt;&gt; print(result) Tensor([[1, 2, 3], [4, 5, 6]])\n\nsource\n\n\nReshape\n\n Reshape (shape:Tuple[int,...])\n\nTensor operation class that reshapes a tensor.\nExample: &gt;&gt;&gt; a = Tensor([1, 2, 3, 4, 5, 6]) &gt;&gt;&gt; op = Reshape((2, 3)) &gt;&gt;&gt; result = op.compute(a) &gt;&gt;&gt; print(result) Tensor([[1, 2, 3], [4, 5, 6]])"
  },
  {
    "objectID": "operators.html#matrix-multiplication",
    "href": "operators.html#matrix-multiplication",
    "title": "operators",
    "section": "Matrix Multiplication",
    "text": "Matrix Multiplication\nMatrix multiplication, often denoted by “matmul” in some programming languages, refers to the process of multiplying two matrices together. However, in the context of calculus, it’s more common to talk about the derivative of a function.\nWhen dealing with matrices, instead of talking about derivatives, we often discuss the Jacobian, which is a matrix of partial derivatives. If you have a function that takes a matrix as input and produces a scalar output, you could compute a gradient, which would be a matrix of the same shape as the input matrix.\nHowever, in the context of deep learning and backpropagation, you might be asking about the derivative of a matrix multiplication operation with respect to its inputs. This is often needed when you’re training a neural network, because you need to compute gradients to update the weights.\nLet’s denote the matrices as A and B, where A is a matrix of dimension m x n and B is a matrix of dimension n x p, and the result of the multiplication C = A * B is a matrix of dimension m x p.\nIf we are to compute the derivative of C with respect to A (i.e., ∂C/∂A), each element in A affects all elements in its corresponding row in C.\nSimilarly, if we are to compute the derivative of C with respect to B (i.e., ∂C/∂B), each element in B affects all elements in its corresponding column in C.\nIn actual computation, if we have a scalar-valued loss function L, we would compute the gradient of L with respect to A (denoted as ∂L/∂A), which is the same shape as A. To compute this, we need to know the gradient of L with respect to C (denoted as ∂L/∂C), then:\n∂L/∂A = (∂L/∂C) * B^T (where * denotes matrix multiplication and B^T is the transpose of B)\nSimilarly, to compute the gradient of L with respect to B (denoted as ∂L/∂B):\n∂L/∂B = A^T * (∂L/∂C)\nThe line axes_to_sum_over = tuple(range(len(out_shape) - len(lhs_shape))) is calculating which axes (dimensions) of the output gradient tensor (out_grad) need to be summed over when computing the gradient with respect to the left-hand side (a) input tensor.\nThis is necessary when the rank (number of dimensions) of out_grad is larger than the rank of a. This can happen, for instance, when a is a matrix (2D tensor) and out_grad is a 3D tensor (which can result from batched matrix multiplication).\nThe range function generates a sequence of integers from 0 up to (but not including) len(out_shape) - len(lhs_shape). The tuple function then takes this sequence and turns it into a tuple. The result is a tuple of integers representing the axes to sum over.\nHere is a concrete example:\nSuppose we have a batched matrix multiplication where A is a matrix of shape (m, n), and out_grad is a 3D tensor of shape (b, m, n), where b is the batch size.\nIn this case, len(out_shape) - len(a_shape) equals 1, so range(len(out_shape) - len(lhs_shape)) generates a sequence of integers from 0 to 1 (not inclusive), which is just [0].\nSo axes_to_sum_over will be (0,), indicating that we need to sum over the first axis (the batch axis) of out_grad when computing the gradient with respect to A.\nThis summing operation effectively accumulates the individual gradients for each item in the batch into a single gradient for the A matrix.\n\n# Suppose we have the following shapes for `lhs` and `out_grad`\nm, n, b = 5, 7, 3\n\n# Let's create some tensors with these shapes\nA = torch.randn(m, n)          # lhs is a 2D tensor (matrix) of shape (m, n)\nout_grad = torch.randn(b, m, n)  # out_grad is a 3D tensor of shape (b, m, n)\n\n# Let's say `rhs` is another matrix that was involved in computing out_grad\nB = torch.randn(n, m)\n\n\nout_shape, A_shape, B_shape = out_grad.shape, A.shape, B.shape\nout_shape, A_shape, B_shape\n\n(torch.Size([3, 5, 7]), torch.Size([5, 7]), torch.Size([7, 5]))\n\n\n\nlen(out_shape), len(A_shape)\n\n(3, 2)\n\n\n\nrng = range(len(out_shape) - len(A_shape))\nrng\n\nrange(0, 1)\n\n\n\ntuple(rng)\n\n(0,)\n\n\n\naxes_to_sum_over = tuple(range(len(out_shape) - len(A_shape)))\naxes_to_sum_over\n\n(0,)\n\n\n\ntorch.sum(out_grad @ B, axes_to_sum_over)\n\ntensor([[-0.1309, -1.9203, -4.4179,  2.8422, -0.4453],\n        [-1.5883, -8.1020, -6.7316, -1.3045,  0.6170],\n        [-0.5317,  2.3444,  1.6038, -3.5786, -0.1689],\n        [ 1.0831, -1.3743,  0.8485, -3.0593,  2.2023],\n        [ 0.3071,  1.8321, -3.6827, -9.4409, -1.1884]])\n\n\n\nsource\n\nmatmul\n\n matmul (a:minima.autograd.Tensor, b:minima.autograd.Tensor)\n\nPerform matrix multiplication on two tensors.\nArgs: a (Tensor): The first input tensor. b (Tensor): The second input tensor.\nReturns: Tensor: The product of a and b.\nExample: &gt;&gt;&gt; a = Tensor([[1, 2], [3, 4]]) &gt;&gt;&gt; b = Tensor([[5, 6], [7, 8]]) &gt;&gt;&gt; result = matmul(a, b) &gt;&gt;&gt; print(result) Tensor([[19, 22], [43, 50]])\n\nsource\n\n\nMatMul\n\n MatMul ()\n\nTensor operation class that performs matrix multiplication.\nExample: &gt;&gt;&gt; a = Tensor([[1, 2], [3, 4]]) &gt;&gt;&gt; b = Tensor([[5, 6], [7, 8]]) &gt;&gt;&gt; op = MatMul() &gt;&gt;&gt; result = op.compute(a, b) &gt;&gt;&gt; print(result) Tensor([[19, 22], [43, 50]])"
  },
  {
    "objectID": "operators.html#summation",
    "href": "operators.html#summation",
    "title": "operators",
    "section": "Summation",
    "text": "Summation\nThe Summation operation, when provided with the axes argument, sums over these axes and thereby reduces the rank of the tensor by the number of axes summed over. The backward pass needs to take this into account, as it needs to return a gradient tensor of the same shape as the input.\nThe forward pass (compute method) is straightforward - it just computes the sum over the specified axes.\nIn the backward pass (gradient method), the goal is to compute the gradient of the sum operation. Since every element of the input tensor contributes equally to the sum, the derivative of the sum with respect to each element is 1. However, since the sum operation may reduce the dimensionality of the tensor (when axes is not None), we need to account for this when computing the gradient.\nTo do this, we first create a new shape, where the dimensions specified by axes are replaced by 1. We then reshape out_grad to this new shape. This essentially “undoes” the dimensionality reduction performed by the sum operation. Finally, we use broadcast_to to make the reshaped gradient tensor the same shape as the input tensor.\nSuppose you have the following tensor in PyTorch:\n\n# 3x3 tensor\nx = torch.tensor([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]], requires_grad=True)\n\n# Sum over axis 0\ny = x.sum(axis=0)\n\n\ny\n\ntensor([12., 15., 18.], grad_fn=&lt;SumBackward1&gt;)\n\n\ny is now a 1-dimensional tensor of shape (3,), because we’ve summed over axis 0. If we compute the gradient of y with respect to x, we’ll want the resulting gradient tensor to have the same shape as x, which is (3,3). However, the gradient tensor we receive during backpropagation (out_grad) will have the same shape as y, which is (3,).\nSo we need to “undo” the dimensionality reduction by reshaping and broadcasting out_grad to match the shape of x. Here’s how you can do it in PyTorch:\n\n# Mock out_grad tensor\nout_grad = torch.tensor([1., 1., 1.])\n\n# Reshape out_grad to have an additional dimension\nreshaped_grad = out_grad.reshape(3, 1)\n\n# Broadcast the reshaped_grad to match the input shape\nbroadcasted_grad = reshaped_grad.expand_as(x)\n\nprint(broadcasted_grad)\n\ntensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]])\n\n\nNow broadcasted_grad has the same shape as x, so it can be correctly used as the gradient of x in further computations. This manual operation simulates what the gradient function of the Summation operation is doing in your original code.\n\nsource\n\nsummation\n\n summation (a:minima.autograd.Tensor, axes:Optional[tuple]=None)\n\nComputes the sum of a along the specified axes.\nArgs: - a: The input tensor. - axes (tuple, optional): The dimensions to reduce. If None (default), reduces all dimensions.\nReturns: The sum of a along the specified axes.\n\nsource\n\n\nSummation\n\n Summation (axes:Optional[tuple]=None)\n\nOp to compute the sum of a tensor along specified axes.\nExample: &gt;&gt;&gt; a = Tensor([[1, 2, 3], [4, 5, 6]]) &gt;&gt;&gt; op = Summation(axes=(0,)) &gt;&gt;&gt; result = op.compute(a) &gt;&gt;&gt; print(result) Tensor([5, 7, 9])\nArgs: - axes (tuple, optional): The dimensions to reduce. If None (default), reduces all dimensions.\nMethods: - compute(a: NDArray) -&gt; NDArray: Computes the sum of a along the specified axes. - gradient(out_grad: Tensor, node: Tensor) -&gt; Tuple[Tensor]: Computes the gradient of the sum operation."
  },
  {
    "objectID": "operators.html#broadcast",
    "href": "operators.html#broadcast",
    "title": "operators",
    "section": "Broadcast",
    "text": "Broadcast\n\n# First, we create a tensor a, and set requires_grad = True so that we can compute gradients with respect to it\na = torch.tensor([1., 2., 3.], requires_grad=True)\n\n# Now, let's define a function that performs the broadcasting operation\ndef broadcast_to(input, shape):\n    return input.expand(shape)\n\n# We broadcast a to a larger shape\nshape = (3, 3)\nb = broadcast_to(a, shape)\n\n\nb\n\ntensor([[1., 2., 3.],\n        [1., 2., 3.],\n        [1., 2., 3.]], grad_fn=&lt;ExpandBackward0&gt;)\n\n\n\nb.shape\n\ntorch.Size([3, 3])\n\n\n\n# Then, we define an output tensor as the sum of elements in b\n# This is a simple function that we can differentiate, and will result in a gradient for b\nout = b.sum()\n\n# Compute gradients\nout.backward()\n\n\na.grad\n\ntensor([3., 3., 3.])\n\n\n\n# Define the output gradient tensor\nout_grad = torch.tensor([[1., 2., 3.], [1., 2., 3.], [1., 2., 3.]])\nout_grad.shape\n\ntorch.Size([3, 3])\n\n\n\na_shape = a.shape\na_shape\n\ntorch.Size([3])\n\n\n\nshape = [1] * (len((3,3)) - len((3,3))) + list(a_shape)\nshape\n\n[3]\n\n\n\n# The gradient for the broadcast operation is the sum of out_grad over the dimension that was broadcasted\ngrad_a = out_grad.sum(dim=0)\n\nprint(grad_a)\n\ntensor([3., 6., 9.])\n\n\n\nsource\n\nbroadcast_to\n\n broadcast_to (a:minima.autograd.Tensor, shape:Tuple[int,...])\n\nBroadcasts a to the specified shape.\nArgs: - a: The input tensor. - shape: The new shape to broadcast the input tensor to.\nReturns: The tensor a broadcasted to the specified shape.\n\nsource\n\n\nBroadcastTo\n\n BroadcastTo (shape)\n\nOp to broadcast a tensor to a new shape.\nExample: &gt;&gt;&gt; a = Tensor([1, 2, 3]) &gt;&gt;&gt; op = BroadcastTo((3, 3)) &gt;&gt;&gt; result = op.compute(a) &gt;&gt;&gt; print(result) Tensor([[1, 2, 3], [1, 2, 3], [1, 2, 3]])\nArgs: - shape (tuple): The new shape to broadcast the input tensor to.\nMethods: - compute(a: NDArray) -&gt; NDArray: Broadcasts a to the specified shape. - gradient(out_grad: Tensor, node: Tensor) -&gt; Tuple[Tensor]: Computes the gradient of the broadcast operation.\n\nbr = BroadcastTo((5,2,3))\na = Tensor([[1., 2., 3.], [1., 2., 3.]])\n\n\na.shape\n\n(2, 3)\n\n\n\na_br = br.compute(a)\na_br.shape\n\n(5, 2, 3)\n\n\n\nout_grad = Tensor(numpy.ones_like(a_br))\nout_grad.shape\n\n(5, 2, 3)\n\n\n\nout_grad\n\nminima.Tensor(\n[[[1 1 1]\n  [1 1 1]]\n\n [[1 1 1]\n  [1 1 1]]\n\n [[1 1 1]\n  [1 1 1]]\n\n [[1 1 1]\n  [1 1 1]]\n\n [[1 1 1]\n  [1 1 1]]])\n\n\n\na_shape = a.shape\na_shape\n\n(2, 3)\n\n\n\nshape = [1] * (len(br.shape) - len(a_shape)) + list(a_shape)\n\n\nbr.shape, shape\n\n((5, 2, 3), [1, 2, 3])\n\n\n\nsum_over = tuple([idx for idx in range(len(br.shape)) if br.shape[idx] != shape[idx]])\nsum_over\n\n(0,)\n\n\n\nreshape(summation(out_grad, sum_over), a_shape).shape\n\n(2, 3)"
  },
  {
    "objectID": "operators.html#logsumexp",
    "href": "operators.html#logsumexp",
    "title": "operators",
    "section": "LogSumExp",
    "text": "LogSumExp\n\nsource\n\nlogsumexp\n\n logsumexp (a, axes=None)\n\n/opt/hostedtoolcache/Python/3.9.17/x64/lib/python3.9/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Methods\n  else: warn(msg)\n\nsource\n\n\nLogSumExp\n\n LogSumExp (axes:Optional[tuple]=None)\n\nA Tensor operation class for performing LogSumExp computation."
  },
  {
    "objectID": "operators.html#export",
    "href": "operators.html#export",
    "title": "operators",
    "section": "Export",
    "text": "Export\n\nimport nbdev; nbdev.nbdev_export()"
  },
  {
    "objectID": "utility.html",
    "href": "utility.html",
    "title": "utility",
    "section": "",
    "text": "source"
  },
  {
    "objectID": "utility.html#export",
    "href": "utility.html#export",
    "title": "utility",
    "section": "Export",
    "text": "Export\n\nimport nbdev; nbdev.nbdev_export()"
  },
  {
    "objectID": "nn.html",
    "href": "nn.html",
    "title": "nn",
    "section": "",
    "text": "source"
  },
  {
    "objectID": "nn.html#layer-normalization",
    "href": "nn.html#layer-normalization",
    "title": "nn",
    "section": "Layer normalization",
    "text": "Layer normalization\nLayer normalization \\(\\text{LN}\\) normalizes the input \\(X\\) as follows:\nWhen input \\(X \\in \\mathbb{R}^{B \\times C}\\) is a batch of embeddings, where \\(B\\) is the batch size and \\(C\\) is the number of features. \\(\\gamma \\in \\mathbb{R}^{C}\\) and \\(\\beta \\in \\mathbb{R}^{C}\\). \\[\\text{LN}(X) = \\gamma\n\\frac{X - \\underset{C}{\\mathbb{E}}[X]}{\\sqrt{\\underset{C}{Var}[X] + \\epsilon}}\n+ \\beta\\]\nWhen input \\(X \\in \\mathbb{R}^{L \\times B \\times C}\\) is a batch of a sequence of embeddings, where \\(B\\) is the batch size, \\(C\\) is the number of channels, \\(L\\) is the length of the sequence. \\(\\gamma \\in \\mathbb{R}^{C}\\) and \\(\\beta \\in \\mathbb{R}^{C}\\). \\[\\text{LN}(X) = \\gamma\n\\frac{X - \\underset{C}{\\mathbb{E}}[X]}{\\sqrt{\\underset{C}{Var}[X] + \\epsilon}}\n+ \\beta\\]\nWhen input \\(X \\in \\mathbb{R}^{B \\times C \\times H \\times W}\\) is a batch of image representations, where \\(B\\) is the batch size, \\(C\\) is the number of channels, \\(H\\) is the height and \\(W\\) is the width. This is not a widely used scenario. \\(\\gamma \\in \\mathbb{R}^{C \\times H \\times W}\\) and \\(\\beta \\in \\mathbb{R}^{C \\times H \\times W}\\). \\[\\text{LN}(X) = \\gamma\n\\frac{X - \\underset{C, H, W}{\\mathbb{E}}[X]}{\\sqrt{\\underset{C, H, W}{Var}[X] + \\epsilon}}\n+ \\beta\\]\n\nX = mi.Tensor(init.rand(5, 10))\nX\n\nminima.Tensor(\n[[0.424498 0.53646  0.553287 0.97566  0.097845 0.951467 0.582025 0.259501 0.248237 0.651587]\n [0.762057 0.323642 0.67996  0.59344  0.998091 0.910109 0.327038 0.583429 0.118361 0.912831]\n [0.503303 0.953529 0.739612 0.116599 0.830379 0.703346 0.34279  0.722535 0.687944 0.927093]\n [0.874576 0.247239 0.280458 0.342372 0.324733 0.963528 0.838563 0.806436 0.37879  0.214825]\n [0.659279 0.326977 0.308499 0.127876 0.815616 0.123615 0.956371 0.24132  0.681585 0.669349]])\n\n\n\nbs, fs = X.shape\nbs, fs\n\n(5, 10)\n\n\n\nmean = X.sum(axes=(1,)) / fs\nmean\n\nminima.Tensor(\n[0.528057 0.620896 0.652713 0.527152 0.491049])\n\n\n\nmean = mean.reshape((bs, 1))\nmean\n\nminima.Tensor(\n[[0.528057]\n [0.620896]\n [0.652713]\n [0.527152]\n [0.491049]])\n\n\n\nmean = mean.broadcast_to(X.shape)\nmean\n\nminima.Tensor(\n[[0.528057 0.528057 0.528057 0.528057 0.528057 0.528057 0.528057 0.528057 0.528057 0.528057]\n [0.620896 0.620896 0.620896 0.620896 0.620896 0.620896 0.620896 0.620896 0.620896 0.620896]\n [0.652713 0.652713 0.652713 0.652713 0.652713 0.652713 0.652713 0.652713 0.652713 0.652713]\n [0.527152 0.527152 0.527152 0.527152 0.527152 0.527152 0.527152 0.527152 0.527152 0.527152]\n [0.491049 0.491049 0.491049 0.491049 0.491049 0.491049 0.491049 0.491049 0.491049 0.491049]])\n\n\n\nx_centred = X - mean\nx_centred\n\nminima.Tensor(\n[[-0.103559  0.008403  0.02523   0.447604 -0.430211  0.42341   0.053968 -0.268556 -0.27982   0.12353 ]\n [ 0.141161 -0.297254  0.059064 -0.027456  0.377195  0.289213 -0.293858 -0.037467 -0.502535  0.291935]\n [-0.14941   0.300816  0.086899 -0.536114  0.177666  0.050634 -0.309923  0.069822  0.035231  0.27438 ]\n [ 0.347424 -0.279913 -0.246694 -0.18478  -0.202419  0.436376  0.311411  0.279284 -0.148362 -0.312327]\n [ 0.16823  -0.164072 -0.18255  -0.363173  0.324568 -0.367433  0.465322 -0.249728  0.190536  0.1783  ]])\n\n\n\nsource\n\nLayerNorm1d\n\n LayerNorm1d (dim:int, eps=1e-05, device=None, dtype='float32')\n\n1D Layer normalization module in Minima.\nApplies layer normalization over a 1D input. The mean and standard deviation are computed over the last dimension.\nAttributes: - dim (int): The dimension of the input feature space. - eps (float): A small constant for numerical stability. - weight (Parameter): The learnable weights of the module of size ‘dim’, initialized with ones. - bias (Parameter): The learnable bias of the module of size ‘dim’, initialized with zeros.\nMethods: - forward(x: Tensor) -&gt; Tensor: Applies layer normalization to the input tensor.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndim\nint\n\nThe dimension of the input feature space.\n\n\neps\nfloat\n1e-05\nA small constant for numerical stability. Default is 1e-5.\n\n\ndevice\nNoneType\nNone\nThe desired device of returned tensor. If None, uses the current device for the default tensor type. Default is None.\n\n\ndtype\nstr\nfloat32\nThe desired data type of returned tensor. If None, uses the default data type. Default is “float32”."
  },
  {
    "objectID": "nn.html#batch-norm",
    "href": "nn.html#batch-norm",
    "title": "nn",
    "section": "Batch Norm",
    "text": "Batch Norm\nThis is an implementation of Batch Normalization from paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.\n\nInternal Covariate Shift\nThe paper defines Internal Covariate Shift as the change in the distribution of network activations due to the change in network parameters during training. For example, let’s say there are two layers \\(l_1\\) and \\(l_2\\). During the beginning of the training \\(l_1\\) outputs (inputs to \\(l_2\\)) could be in distribution \\(\\mathcal{N}(0.5, 1)\\). Then, after some training steps, it could move to \\(\\mathcal{N}(0.6, 1.5)\\). This is internal covariate shift.\nInternal covariate shift will adversely affect training speed because the later layers (\\(l_2\\) in the above example) have to adapt to this shifted distribution.\nBy stabilizing the distribution, batch normalization minimizes the internal covariate shift."
  },
  {
    "objectID": "nn.html#normalization",
    "href": "nn.html#normalization",
    "title": "nn",
    "section": "Normalization",
    "text": "Normalization\nIt is known that whitening improves training speed and convergence. Whitening is linearly transforming inputs to have zero mean, unit variance, and be uncorrelated.\n\nNormalizing outside gradient computation doesn’t work\nNormalizing outside the gradient computation using pre-computed (detached) means and variances doesn’t work. For instance. (ignoring variance), let \\[\\hat{x} = x - \\mathbb{E}[x]\\] where \\(x = u + b\\) and \\(b\\) is a trained bias and \\(\\mathbb{E}[x]\\) is an outside gradient computation (pre-computed constant).\nNote that \\(\\hat{x}\\) has no effect on \\(b\\). Therefore, \\(b\\) will increase or decrease based \\(\\frac{\\partial{\\mathcal{L}}}{\\partial x}\\), and keep on growing indefinitely in each training update. The paper notes that similar explosions happen with variances.\n\n\nBatch Normalization\nWhitening is computationally expensive because you need to de-correlate and the gradients must flow through the full whitening calculation.\nThe paper introduces a simplified version which they call Batch Normalization. First simplification is that it normalizes each feature independently to have zero mean and unit variance: \\[\\hat{x}^{(k)} = \\frac{x^{(k)} - \\mathbb{E}[x^{(k)}]}{\\sqrt{Var[x^{(k)}]}}\\] where \\(x = (x^{(1)} ... x^{(d)})\\) is the \\(d\\)-dimensional input.\nThe second simplification is to use estimates of mean \\(\\mathbb{E}[x^{(k)}]\\) and variance \\(Var[x^{(k)}]\\) from the mini-batch for normalization; instead of calculating the mean and variance across the whole dataset.\nNormalizing each feature to zero mean and unit variance could affect what the layer can represent. As an example paper illustrates that, if the inputs to a sigmoid are normalized most of it will be within \\([-1, 1]\\) range where the sigmoid is linear. To overcome this each feature is scaled and shifted by two trained parameters \\(\\gamma^{(k)}\\) and \\(\\beta^{(k)}\\). \\[y^{(k)} =\\gamma^{(k)} \\hat{x}^{(k)} + \\beta^{(k)}\\] where \\(y^{(k)}\\) is the output of the batch normalization layer.\nNote that when applying batch normalization after a linear transform like \\(Wu + b\\) the bias parameter \\(b\\) gets cancelled due to normalization. So you can and should omit bias parameter in linear transforms right before the batch normalization.\nBatch normalization also makes the back propagation invariant to the scale of the weights and empirically it improves generalization, so it has regularization effects too."
  },
  {
    "objectID": "nn.html#inference",
    "href": "nn.html#inference",
    "title": "nn",
    "section": "Inference",
    "text": "Inference\nWe need to know \\(\\mathbb{E}[x^{(k)}]\\) and \\(Var[x^{(k)}]\\) in order to perform the normalization. So during inference, you either need to go through the whole (or part of) dataset and find the mean and variance, or you can use an estimate calculated during training. The usual practice is to calculate an exponential moving average of mean and variance during the training phase and use that for inference.\nBatch normalization layer \\(\\text{BN}\\) normalizes the input \\(X\\) as follows:\nWhen input \\(X \\in \\mathbb{R}^{B \\times C \\times H \\times W}\\) is a batch of image representations, where \\(B\\) is the batch size, \\(C\\) is the number of channels, \\(H\\) is the height and \\(W\\) is the width. \\(\\gamma \\in \\mathbb{R}^{C}\\) and \\(\\beta \\in \\mathbb{R}^{C}\\). \\[\\text{BN}(X) = \\gamma\n\\frac{X - \\underset{B, H, W}{\\mathbb{E}}[X]}{\\sqrt{\\underset{B, H, W}{Var}[X] + \\epsilon}}\n+ \\beta\\]\nWhen input \\(X \\in \\mathbb{R}^{B \\times C}\\) is a batch of embeddings, where \\(B\\) is the batch size and \\(C\\) is the number of features. \\(\\gamma \\in \\mathbb{R}^{C}\\) and \\(\\beta \\in \\mathbb{R}^{C}\\). \\[\\text{BN}(X) = \\gamma\n\\frac{X - \\underset{B}{\\mathbb{E}}[X]}{\\sqrt{\\underset{B}{Var}[X] + \\epsilon}}\n+ \\beta\\]\nWhen input \\(X \\in \\mathbb{R}^{B \\times C \\times L}\\) is a batch of a sequence embeddings, where \\(B\\) is the batch size, \\(C\\) is the number of features, and \\(L\\) is the length of the sequence. \\(\\gamma \\in \\mathbb{R}^{C}\\) and \\(\\beta \\in \\mathbb{R}^{C}\\). \\[\\text{BN}(X) = \\gamma\n\\frac{X - \\underset{B, L}{\\mathbb{E}}[X]}{\\sqrt{\\underset{B, L}{Var}[X] + \\epsilon}}\n+ \\beta\\]\n\nsource\n\nBatchNorm1d\n\n BatchNorm1d (dim:int, eps=1e-05, momentum=0.1, device=None,\n              dtype='float32')\n\n1D Batch normalization module in Minima.\nThis module applies Batch Normalization over a 1D input as described in the paper “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift” by Ioffe and Szegedy.\nAttributes: - dim (int): The dimension of the input feature space. - eps (float): A small constant added to the denominator for numerical stability. - momentum (float): The value used for the running_mean and running_var computation. - weight (Parameter): The learnable scale factor of the module of size ‘dim’, initialized with ones. - bias (Parameter): The learnable offset of the module of size ‘dim’, initialized with zeros. - running_mean (Tensor): The running mean. Represents the mean of the features over batches. Initialized with zeros. - running_std (Tensor): The running standard deviation. Represents the standard deviation of the features over batches. Initialized with ones.\nMethods: - update_stats(x: Tensor) -&gt; Tuple[Tensor, Tensor]: Calculates the mean and standard deviation of the input tensor. - forward(x: Tensor) -&gt; Tensor: Applies batch normalization to the input tensor.\nExample:\nbatch_norm = BatchNorm1d(dim=512)\noutput = batch_norm(input_tensor)  # Apply batch normalization\n/opt/hostedtoolcache/Python/3.9.17/x64/lib/python3.9/site-packages/fastcore/docscrape.py:225: UserWarning: potentially wrong underline length... \nParameters: \n---------- in \nDropout Layer for a Neural Network.\n...\n  else: warn(msg)\n\nsource\n\n\nDropout\n\n Dropout (p=0.5)\n\nDropout Layer for a Neural Network.\nThis class represents a dropout layer in a neural network, which is a simple and effective regularization technique. During training, it randomly zeroes out some of the elements of the input tensor with probability p using samples from a Bernoulli distribution."
  },
  {
    "objectID": "nn.html#parameters",
    "href": "nn.html#parameters",
    "title": "nn",
    "section": "Parameters:",
    "text": "Parameters:\np: float, optional, default = 0.5 Probability of an element to be zeroed. Default: 0.5.\n/opt/hostedtoolcache/Python/3.9.17/x64/lib/python3.9/site-packages/fastcore/docscrape.py:225: UserWarning: potentially wrong underline length... \nParameters: \n---------- in \nResidual Layer for a Neural Network.\n...\n  else: warn(msg)\n\nsource\n\nResidual\n\n Residual (fn:__main__.Module)\n\nResidual Layer for a Neural Network.\nThis class represents a residual layer in a neural network, which is a technique that helps to overcome the problem of vanishing and exploding gradients in deep neural networks. It achieves this by allowing gradients to pass through layers directly (via an identity shortcut connection) without any modification."
  },
  {
    "objectID": "nn.html#parameters-1",
    "href": "nn.html#parameters-1",
    "title": "nn",
    "section": "Parameters:",
    "text": "Parameters:\nfn: Module The function to be applied to the input tensor.\n\nsource\n\nIdentity\n\n Identity ()\n\nBase class for all neural network modules in Minima.\nYour models should also subclass this class. Subclasses should define a forward method.\nAttributes: - training (bool): Module is initialized in training mode by default. Use eval() to switch it to evaluation mode.\nMethods: - parameters(): Returns a list of all Parameter instances in the module. - _children(): Returns a list of all child Module instances. - eval(): Switches the module and all its children to evaluation mode. - train(): Switches the module and all its children back to training mode. - __call__(): The call method, which simply calls the forward method, must be defined by all subclasses."
  },
  {
    "objectID": "nn.html#export",
    "href": "nn.html#export",
    "title": "nn",
    "section": "Export",
    "text": "Export\n\nimport nbdev; nbdev.nbdev_export()"
  }
]